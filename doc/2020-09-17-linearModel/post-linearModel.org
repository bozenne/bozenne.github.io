#+TITLE: Diagnostics and partial residuals in a linear model
#+Author: Brice Ozenne

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
options(width = 100)
path <- "~/Documents/GitHub/bozenne.github.io/doc/2020-09-17-linearModel/"
setwd(path)
#+END_SRC

#+RESULTS:


* Summary
:PROPERTIES:
:UNNUMBERED: t
:END:

In the document we provide a brief introduction of R (sections
[[#sec:Rsoftware]], [[#sec:Data]], [[#sec:descriptive]]) and to the linear model
(sections [[#sec:lm]]). We show how some of the underlying hypotheses can
be checked and what to do when there is evidence that one or several
assumptions are not met. We also introduce the notion of partial
residuals (sections [[#sec:pRes]]) and explain how to compute and display
them.

\clearpage

* Software
:PROPERTIES:
:CUSTOM_ID: sec:Rsoftware
:END:
The \Rlogo{} software can be downloaded at
https://cloud.r-project.org/. R studio provide a convenient user
interface that can be downloaded at
https://www.rstudio.com/products/rstudio/.  In this document the
\Rlogo{} code will be display in boxes:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
1+1 ## comment about the code
#+END_SRC

#+RESULTS:
: [1] 2

while the \Rlogo{} output will be displayed in dark red below the box. 

\bigskip

When starting a fresh \Rlogo{} session, only the core functionalities
of \Rlogo{} are available. Additional functionalities called packages
can be downloaded from the CRAN using the command =install.packages=:
#+BEGIN_SRC R :exports code :results silent :session *R* :eval never
install.packages(pkgs = c("lava","car","nlme","ggfortify", "exact2x2",
                          "devtools","reshape2","Publish","officer"))
#+END_SRC
Two of the packages we need are not available on CRAN but only on
Github, this is why we also run [fn::if you do not manage to install
it skip that part, you should still be able to run most of the code
used in this document]:
#+BEGIN_SRC R :exports both :results output :session *R* :eval never
devtools::install_github("bozenne/butils")
devtools::install_github("kkholst/gof")
#+END_SRC
After having installed the packages, one needs to load them using the
command =library= to use them in the current \Rlogo{} session:
#+BEGIN_SRC R  :results silent   :exports both  :session *R* :cache no
library(lava) ## simulate data, latent variable models
library(car) ## miscellaneous sfunction
library(nlme) ## mixed models
library(ggfortify)  ## graphical display
library(butils)  ## miscellaneous function
library(reshape2) ## wide to long format
library(gof)  ## diagnostic tests
library(Publish)  ## table 1
library(officer)  ## export to word
library(exact2x2)  ## compare proportions
#+END_SRC

\clearpage

* Data
:PROPERTIES:
:CUSTOM_ID: sec:Data
:END:

** Generation
#+LaTeX: \lstset{style=code-tiny}
We will use the dataset generated by the following commands:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10)
m.lvm <- lvm(Y1[100:sigma21]~beta*AgeC+BMI2+Gene,
             Y2[110:sigma22]~beta*AgeC+BMI2+Gene,
             Y3[120:sigma23]~beta*AgeC+0.5*BMI2+Gene)
categorical(m.lvm, K = 3) <- ~Gene
distribution(m.lvm, ~Age) <- uniform.lvm(20,50)
distribution(m.lvm, ~BMI) <- gaussian.lvm(mean = 24, sd = 3)
transform(m.lvm, AgeC~Age) <- function(x, ...){x-35}
transform(m.lvm, Id~Age) <- function(x, ...){1:NROW(x)}
transform(m.lvm, BMI2~BMI) <- function(x, ...){(x-24) + (x-24)^2}
latent(m.lvm) <- ~AgeC+BMI2

p1 <- c(beta = 1, sigma21 = 1, sigma22 = 2, sigma23 = 3)
p2 <- c(beta = -1, sigma21 = 4, sigma22 = 4, sigma23 = 4)
d <- rbind(cbind(lava::sim(n = 1e2, m.lvm, latent=FALSE, p = p1), Gender = "Male"),
           cbind(lava::sim(n = 1e2, m.lvm, latent=FALSE, p = p2), Gender = "Female")
           )

d$Gender <- as.factor(d$Gender)
d$Gene <- factor(d$Gene, labels = c("A","B","C"))
d$Y1 <- round(d$Y1,1)
d$Y2 <- round(d$Y2,1)
d$Age <- round(d$Age,1)
d$BMI <- round(d$BMI,1)
d <- d[,c("Id","Age","Gender","BMI","Gene","Y1","Y2","Y3")]
head(d)
#+END_SRC

#+RESULTS:
:   Id  Age Gender  BMI Gene    Y1    Y2       Y3
: 1  1 44.2   Male 23.8    A 109.1 120.8 131.7429
: 2  2 41.3   Male 27.5    B 123.2 133.9 136.3850
: 3  3 27.4   Male 30.6    A 140.6 154.0 136.1408
: 4  4 35.3   Male 25.2    C 104.4 116.2 125.0175
: 5  5 37.1   Male 21.8    A 105.0 113.2 123.6257
: 6  6 29.9   Male 18.1    C 125.9 136.2 131.7966
#+LaTeX: \lstset{style=code-small}

We will then export the data in a .csv format using 
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
write.csv(d, "data.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:

\bigskip

_Note:_ in "real life" studies, this step does not exist. Instead an
experiment is performed where some data are collected.


  \clearpage


** Data management
:PROPERTIES:
:CUSTOM_ID: sec:dataManagement
:END:

*** Working directory

The working directory is where R, by default, look for files
to import and export data or figures. The current working directory
can be accessed using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
path <- getwd()
path
#+END_SRC

#+RESULTS:
: [1] "/home/brice/Documents/GitHub/bozenne.github.io/doc/2020-09-17-linearModel"

It can be changed using the function =setwd()=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
path2 <- "~"
setwd(path2)
#+END_SRC

#+RESULTS:

We can check that the working directory has indeed changed calling
again =getwd()=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
getwd()
#+END_SRC

#+RESULTS:
: [1] "/home/brice"

We move back to the original working directory doing:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
setwd(path)
#+END_SRC

#+RESULTS:

*** Importing the data

It is a good idea to start by checking that the working directory
contains the data we want to import. For instance the file =data.csv=
is storing the data, we can use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
file.exists("data.csv")
#+END_SRC

#+RESULTS:
: [1] TRUE

We can also list all files in the current directory with a =.csv= extension using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
list.files(pattern = ".csv") 
#+END_SRC

#+RESULTS:
: [1] "data.csv"

\clearpage

We can also display the first lines of the file using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
readLines("data.csv")[1:3]
#+END_SRC

#+RESULTS:
: [1] "\"Id\",\"Age\",\"Gender\",\"BMI\",\"Gene\",\"Y1\",\"Y2\",\"Y3\""
: [2] "1,44.2,\"Male\",23.8,\"A\",109.1,120.8,131.742898261202"        
: [3] "2,41.3,\"Male\",27.5,\"B\",123.2,133.9,136.385038040154"

We can see that the columns are separated with =,= and that the =.=
indicates the decimal values. Moreover the words such as the columns
names or the subject identities are surrounded by =\"= (e.g. =\"Id\"=
stand for Id). Finally in this example there is no missing values but
if there was it is important to know how they are encoded.

\bigskip

 The command to import the data depends on the type of file. Here for
a =.csv= file we use =read.csv=. Luckily the default arguments =sep=,
=dec=, =quote= are correctly specified:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
args(read.csv)
#+END_SRC

#+RESULTS:
: function (file, header = TRUE, sep = ",", quote = "\"", dec = ".", 
:     fill = TRUE, comment.char = "", ...) 
: NULL

The argument =header= set to =TRUE= indicates that the first line of
the dataset contains the column names (and not the actual data). The
=...= indicates there are additional arguments that are not shown here
(see the documentation using =help(read.csv)=). For instance, in
presence of missing values, one would need to specify the argument
=na.string=. Here it is sufficient to do:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfW <- read.csv("data.csv")
#+END_SRC

#+RESULTS:

Other functions exists to import other types of data,
e.g. =read.table= for =.txt= files, =read.xlsx= from the xlsx package
for =.xlsx= file, or =read.spss= from the foreign package for spss
data files. One should always inspect if R has correctly imported the
data, e.g. using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
str(dfW)
#+END_SRC

#+RESULTS:
: 'data.frame':	200 obs. of  8 variables:
:  $ Id    : int  1 2 3 4 5 6 7 8 9 10 ...
:  $ Age   : num  44.2 41.3 27.4 35.3 37.1 29.9 33.1 26 43.6 43.5 ...
:  $ Gender: Factor w/ 2 levels "Female","Male": 2 2 2 2 2 2 2 2 2 2 ...
:  $ BMI   : num  23.8 27.5 30.6 25.2 21.8 18.1 18.1 21.2 27.6 22.1 ...
:  $ Gene  : Factor w/ 3 levels "A","B","C": 1 2 1 3 1 3 3 1 3 3 ...
:  $ Y1    : num  109 123 141 104 105 ...
:  $ Y2    : num  121 134 154 116 113 ...
:  $ Y3    : num  132 136 136 125 124 ...

In this example, the two columns contain character strings (=Factor=
is a type of character strings in R) and the rest contains numerical
values.

*** Data processing

Often the raw data needs to be transformed before being analyzed:
- A typical example is when one need to deal with the variable:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gender <- c(1,0,1,0,1) ## what is 1? what is 0?
#+END_SRC

#+RESULTS:
This is already better:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
female <- c(1,0,1,0,1) ## we can guess that 1: female and 0: male
#+END_SRC

#+RESULTS:

but it is a good practice in such situation to rename the actual
values into something understandable:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
factor(gender, levels = 0:1, labels = c("Female","Male")) 
#+END_SRC

#+RESULTS:
: [1] Male   Female Male   Female Male  
: Levels: Female Male

- With repeated measurements per individual, one often needs to
  reshape his dataset from the wide format (one line per individual)
  to the long format (one line per measurement). This can be done
  using the =melt= method. The opposite operation can be performed
  using =dcast=.

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
str(dfW)
#+END_SRC

#+RESULTS:
: 'data.frame':	200 obs. of  8 variables:
:  $ Id    : int  1 2 3 4 5 6 7 8 9 10 ...
:  $ Age   : num  44.2 41.3 27.4 35.3 37.1 29.9 33.1 26 43.6 43.5 ...
:  $ Gender: Factor w/ 2 levels "Female","Male": 2 2 2 2 2 2 2 2 2 2 ...
:  $ BMI   : num  23.8 27.5 30.6 25.2 21.8 18.1 18.1 21.2 27.6 22.1 ...
:  $ Gene  : Factor w/ 3 levels "A","B","C": 1 2 1 3 1 3 3 1 3 3 ...
:  $ Y1    : num  109 123 141 104 105 ...
:  $ Y2    : num  121 134 154 116 113 ...
:  $ Y3    : num  132 136 136 125 124 ...

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfL <- reshape2::melt(dfW, id.vars = c("Id","Gender","Age","BMI","Gene"),
                      measure.vars = c("Y1","Y2","Y3"),
                      value.name = "score",variable.name = "outcome")
head(dfL)
#+END_SRC

#+RESULTS:
:   Id Gender  Age  BMI Gene outcome score
: 1  1   Male 44.2 23.8    A      Y1 109.1
: 2  2   Male 41.3 27.5    B      Y1 123.2
: 3  3   Male 27.4 30.6    A      Y1 140.6
: 4  4   Male 35.3 25.2    C      Y1 104.4
: 5  5   Male 37.1 21.8    A      Y1 105.0
: 6  6   Male 29.9 18.1    C      Y1 125.9


- It is often a good idea to restrict the dataset to the relevant
  variables (e.g. remove genetic data if they are not of interest). It
  is easier to work with and to display in the next steps. This can
  for instance be done by defining the variables of interest:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
keep.var <- c("Id","BMI","Y1")
#+END_SRC

#+RESULTS:
We can check that the variables defined in =keep.var= are in =df=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
keep.var %in% names(dfW)
#+END_SRC

#+RESULTS:
: [1] TRUE TRUE TRUE

and then subset the initial dataset:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfW.red <- dfW[,keep.var]
head(dfW.red)
#+END_SRC

#+RESULTS:
:   Id  BMI    Y1
: 1  1 23.8 109.1
: 2  2 27.5 123.2
: 3  3 30.6 140.6
: 4  4 25.2 104.4
: 5  5 21.8 105.0
: 6  6 18.1 125.9

- Often after having imported the data we want to change its column
  names. First we need to know the current column names. The =names=
  function can be used to output all the column names:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
names(dfW)
#+END_SRC

#+RESULTS:
: [1] "Id"     "Age"    "Gender" "BMI"    "Gene"   "Y1"     "Y2"     "Y3"

Alternatively the =grep= function will output any column name
containing a given string of characters:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
grep(pattern = "Y", x = names(dfW), value = TRUE)
#+END_SRC

#+RESULTS:
: [1] "Y1" "Y2" "Y3"

Then, we can rename columns one at a time using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
names(dfW)[names(dfW) == "Y1"] <- "baseline_score"
names(dfW)[names(dfW) == "Y2"] <- "followup_score"
names(dfW)[names(dfW) == "Y3"] <- "final_score"
head(dfW)
#+END_SRC

#+RESULTS:
:   Id  Age Gender  BMI Gene baseline_score followup_score final_score
: 1  1 44.2   Male 23.8    A          109.1          120.8    131.7429
: 2  2 41.3   Male 27.5    B          123.2          133.9    136.3850
: 3  3 27.4   Male 30.6    A          140.6          154.0    136.1408
: 4  4 35.3   Male 25.2    C          104.4          116.2    125.0175
: 5  5 37.1   Male 21.8    A          105.0          113.2    123.6257
: 6  6 29.9   Male 18.1    C          125.9          136.2    131.7966

To rename several columns at the same time we can use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
old2new <- c("baseline_score" = "Y1", 
             "followup_score" = "Y2",
             "final_score" = "Y3")
names(dfW)[match(names(old2new),names(dfW))] <- old2new
head(dfW)
#+END_SRC

#+RESULTS:
:   Id  Age Gender  BMI Gene    Y1    Y2       Y3
: 1  1 44.2   Male 23.8    A 109.1 120.8 131.7429
: 2  2 41.3   Male 27.5    B 123.2 133.9 136.3850
: 3  3 27.4   Male 30.6    A 140.6 154.0 136.1408
: 4  4 35.3   Male 25.2    C 104.4 116.2 125.0175
: 5  5 37.1   Male 21.8    A 105.0 113.2 123.6257
: 6  6 29.9   Male 18.1    C 125.9 136.2 131.7966

Other useful functions are =tolower= to convert characters to lower
case and =gsub= to remove a specific pattern in a character vector, e.g.:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gsub(pattern = ".", replacement = "", x = c("a..","b..."), fixed = TRUE)
#+END_SRC

#+RESULTS:
: [1] "a" "b"

Many of the other data processing steps are specific to each study and
we won't discuss them in this document. 


* Descriptive statistics
:PROPERTIES:
:CUSTOM_ID: sec:descriptive
:END:

Before doing any analysis, it is a good practice to describe the data
that are to be analyzed. The has several aims:
- *check that that database contains the population of interest*,
  i.e. individuals in the database are indeed those the we want to
  study and we have all of them.
- *check that the collected values are plausible*, e.g. if the inclusion
  criteria include that the age range is between 18 and 99 years, then
  one should check that this is indeed the case.
- *check that the collected values are coded as expected*, e.g. age is
  usually coded in years (and not in months). 
- *check that the collected values are distributed as expected*,
  e.g. is there missing values? Are the values uniformly spread?
  Bimodal? Concentrated at low or high values?

Note: one should checks that for all the variables of interest. This
can appear time-consuming but can really save you time at latter
stages. 

- *produce your table 1* i.e. a descriptive table of your cohort that
  is almost always included in an article. You can for instance use
  the function =univariateTable= from the Publish package:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
myTable1 <- univariateTable(Gender ~ Age + BMI + Y1 + Y2 + Y3, 
                            data = dfW)
myTable1
#+END_SRC

#+RESULTS:
:   Variable     Level Female (n=100) Male (n=100) Total (n=200) p-value
: 1      Age mean (sd)     33.9 (8.7)     35.1 (9)    34.5 (8.9)  0.3459
: 2      BMI mean (sd)     24.2 (2.6)   23.8 (3.4)        24 (3)  0.4307
: 3       Y1 mean (sd)   109.1 (13.8)   112.2 (17)  110.7 (15.5)  0.1606
: 4       Y2 mean (sd)   119.1 (13.3)   122.4 (17)  120.7 (15.4)  0.1335
: 5       Y3 mean (sd)   126.1 (10.7)   127 (11.5)  126.6 (11.1)  0.5601

You can also export this table in a word document with the package
officer:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
myTable1.doc <- body_add_table(x = read_docx(), 
                               value =  summary(myTable1)) 
print(myTable1.doc, target = "./Table1.docx")
#+END_SRC

#+RESULTS:

To keep the code simple, we only present here a very basic application
of these tools. More complex tables with a nicer display in word can
be obtain with a bit of coding.

\clearpage

- *make synthetic representations of your data* using graphs or
  images. This can be useful to visualize your data and help your
  collaborators to understand what you have collected or what you are
  trying to show.

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(ggplot2)
gg <- ggplot(dfL, aes(x = BMI, y = score, color = Gender, group = Gender))
gg <- gg + geom_point()
gg <- gg + facet_wrap(~outcome, labeller = label_both)
gg <- gg + geom_smooth(method = "lm", se = FALSE)
gg
#+END_SRC

#+RESULTS:
: `geom_smooth()` using formula 'y ~ x'

[[./figures/descriptive.pdf]]

You can then export the figure in a folder =figures= using:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
pdf("./figures/descriptive.pdf", width = 12)
gg + theme(text = element_text(size=25))
dev.off()
#+END_SRC

#+RESULTS:
: `geom_smooth()` using formula 'y ~ x'
: X11cairo 
:        2

- *Compare percentages when considering categorical data*: the usual
  way to compare the distribution of a categorical variable between
  two groups is to run a Fisher test using =fisher.test= in the R
  software. It returns a p-value and an estimate of the odd ratio with
  its confidence interval. For instance, consider the following
  dataset:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
mytable <- rbind(c(8,5), c(4,15))
dimnames(mytable) <- list(c("control","treatment"), c("-","+"))
mytable
#+END_SRC

#+RESULTS:
:           -  +
: control   8  5
: treatment 4 15
The Fisher test outputs:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
fisher.test(mytable)
#+END_SRC

#+RESULTS:
#+begin_example

	Fisher's Exact Test for Count Data

data:  mytable
p-value = 0.02996
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  0.9953576 38.7853302
sample estimates:
odds ratio 
  5.622612
#+end_example

This approach admits two drawbacks:
- the p-value may not agree with the confidence interval of the odd
  ratio regarding the rejection of the null hypothesis
- the odd ratio is a rather complex quantity to understand.
Instead one can use the function =binomMeld.test= (package /exact2x2/)
to perform a test on the proportions:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
binomMeld.test(x1=mytable["control","+"],n1=sum(mytable["control",]),
               x2=mytable["treatment","+"],n2=sum(mytable["treatment",]),
               parmtype="difference")
#+END_SRC

#+RESULTS:
#+begin_example

	melded binomial test for difference

data:  sample 1:(5/13), sample 2:(15/19)
proportion 1 = 0.38462, proportion 2 = 0.78947, p-value = 0.05077
alternative hypothesis: true difference is not equal to 0
95 percent confidence interval:
 -0.001077177  0.715576028
sample estimates:
difference (p2-p1) 
         0.4048583
#+end_example

This time the p-value is consistent with the confidence interval. 
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
## In fact, the p-value is matches the p-value of the confidence interval of
exact2x2(mytable, tsmethod = "central")
#+END_SRC   

#+RESULTS:
#+begin_example

	Central Fisher's Exact Test

data:  mytable
p-value = 0.05077
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  0.9953576 38.7853302
sample estimates:
odds ratio 
  5.622612
#+end_example

\clearpage


* Introduction to the linear model
:PROPERTIES:
:CUSTOM_ID: sec:lm
:END:

Imagine we would like to model the age effect on the outcome, but
accounting for a possible gender and gene effect. In \Rlogo{} we would
use the =lm= function:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lm <- lm(Y1 ~ Gender + Age + Gene + BMI, data = dfW)
e.lm
#+END_SRC

#+RESULTS:
: 
: Call:
: lm(formula = Y1 ~ Gender + Age + Gene + BMI, data = dfW)
: 
: Coefficients:
: (Intercept)   GenderMale          Age        GeneB        GeneC          BMI  
:     91.5610       3.7984      -0.1358       7.8329       5.8120       0.7365

Denote for the \(i-th\) patient its outcome value by \(Y_i\) (can be
any real number), its gender value by \(Gender_i\) (can be "Male" or
"Female"), its gene value by \(Gene_i\) (can be "A", "B", or
"C"), and its BMI value by \(BMI_i\). Mathematically, this linear model can be written:
#+BEGIN_EXPORT latex
\begin{align*}
Y_i =& \alpha + \beta_{Gender} * \Ind[Gender_i="Male"] + \beta_{Age} * Age_i + \beta_{GeneB} *  \Ind[Gene_i="B"] + \beta_{GeneC} * \Ind[Gene_i="C"] \\
& + \beta_{BMI} * BMI_i + \varepsilon_i
\end{align*}
#+END_EXPORT
where \(\boldsymbol{\beta} =
(\alpha,\beta_{Gender},\beta_{Age},\beta_{GeneB},\beta_{GeneC},\beta_{BMI})\) is
the vector of model parameters. Their value is shown just above
(e.g. \(\alpha=21.3988\)). Here \(\Ind[.]\) denotes the indicator
function taking value 1 if "." is true and 0
otherwise. \(\varepsilon_i\) is the residual error, i.e. the
difference between the observed value and the fitted value. Consider
for instance the first individual:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
d[1,]
#+END_SRC

#+RESULTS:
:   Id  Age Gender  BMI Gene    Y1    Y2       Y3
: 1  1 44.2   Male 23.8    A 109.1 120.8 131.7429
its observed value is 109.2 and we can computed its fitted value as:
#+BEGIN_EXPORT latex
\begin{align*}
\hat{Y}_1 &= \widehat{\alpha} + \widehat{\beta}_{Gender} * 0 + \widehat{\beta}_{Age} 48 + \widehat{\beta}_{GeneB} * 0 + \widehat{\beta}_{GeneC} * 0 \\
          &= 91.5610 + 3.7984 * 1 - 0.1358 * 44.2 + 7.8329 * 0 + 5.8120 * 0 + 0.7365 * 23.8 \\
          & = 106.8857
\end{align*}
#+END_EXPORT

Here the hat on top of the \(\beta\) refer to the estimated coefficient.

\clearpage 

This can also be obtained using the =fitted= method in \Rlogo{} (the
discrepancy comes from rounding the coefficient values at the 4th
digit):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
fitted(e.lm)[1] 
#+END_SRC

#+RESULTS:
:        1 
: 106.8839

Often, for conciseness, this linear model can be abbreviated as:
#+BEGIN_EXPORT latex
\begin{align*}
Y_i = X_i \boldsymbol{\beta} + \varepsilon_i
\end{align*}
#+END_EXPORT
where \(X_i = \left(1, \Ind[Gender_i="Male"], Age_i,
\Ind[Gene_i="B"], \Ind[Gene_i="C"]\right)\) and \(X_i
\boldsymbol{\beta}\) is the matrix product between the row vector
\(X_i\) and the column vector \(\boldsymbol{\beta}\). More generally,
i.e. at the population level instead of the individual level, we also
write \(Y = X \boldsymbol{\beta} + \varepsilon\) to describe the
relationship between the random variables \(Y\), \(X\),
\(\varepsilon\).

** Assumptions 
# # https://ademos.people.uic.edu/Chapter12.html#31_getting_a_broad_snapshot:_plot()

A linear model \(Y = X \beta + \varepsilon\) is a model studying the
 effects (\(\beta\)) of covariates (\(X\)) on the expected value of
 the outcome \(Y\). Maximum likelihood (ML) estimation leads to
 unbiased estimates of \(\beta\) if the following assumptions are
 satisfied:
- *(A0)*: no unobserved confounders.
- *(A1)*: \(\Esp[Y_i|X] = X_i \beta\) correct specification of the
  functional form of the covariates.
- *(A2)*: identically distributed and *(A3)* independent
  residuals. \newline Under the normality assumption, it simplifies to
  *(A2)* homoschedasticity \(\Var[Y_i|X]= \sigma^2\) and *(A3)*
  uncorrelatedness \(\forall i \neq j\), \(\Cov[Y_i,Y_j|X]= 0\).
While not needed per se, the assumption of:
- *(A4)*: normally distributed residuals is often mentioned since (i)
  normality of the estimates holds exactly in finite samples (instead
  of asymptotically) i.e. p-values/CIs are reliable even in small
  samples, (ii) it ensures that MLE is the best estimation procedure,
  (iii) checking *(A2)* and *(A3)* is simplified.
Additional assumptions (automatically fullfield under normality) are
typically necessary to ensure reliable and interpretable estimates:
- *(A4-bis)*: approximately symmetric and unimodal - otherwise modeling the
  expected value (aka the mean value) may not be very relevant.
- *(A5)*: absence of outliers - otherwise the estimates may be very
  sensitive to the value of a few observations which is often
  undesirable.

\clearpage

** Interpretation of the regression coefficients
:PROPERTIES:
:CUSTOM_ID: sec:interpretationLM
:END:
If the assumptions (A1-A3), \(\beta_{age}\) reflect the association
between age and the outcome. This means that for fixed gene, gender,
and BMI, if we observe an individual A one year older than an
individual B then we would also expect that its value for the outcome
\(Y\) to differ by \(\beta_{age}\). \newline If we in addition make causal
assumptions (mainly A0: no unobserved confounder) then we can
interpret \(\beta_{Age}\) as the effect of age on the outcome. This
means that if we could change the age of an individual by one unit
then its variation in outcome should be \(\beta_{age}\).


** Hypothesis testing

We want to formally test whether there is an effect of age on the
outcome. We first need to make the distinction between:
- \(\beta^0_{age}\) the true but unknown age effect (may be 1.5)
- \(\widehat{\beta}_{age}\) the estimated age effect (here 1.5326 using maximum likelihood)
We would like to test the null hypothesis of no age effect:
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; \beta^0_{age} = 0
\end{align*}
#+END_EXPORT
 Since the parameters are estimated by ML and assuming that the model
is correctly specified, we know that the asymptotic distribution of
the parameter is Gaussian. This means that for large sample size, the
fluctuation of the estimated values follows a normal distribution. For
instance:
#+BEGIN_EXPORT latex
\begin{align*}
\hat{\beta}_{age} \underset{n \rightarrow \infty}{\sim} \Gaus[\beta^0_{age},\sigma^2_{\beta_{age}}]
\end{align*}
#+END_EXPORT
where \(\sigma^2_{\beta_{age}}\) is the variance of the MLE, i.e. the
uncertainty surrounding our estimation of the association. It follows that:
#+BEGIN_EXPORT latex
\begin{align}
t_{\beta_{age}} = \frac{\hat{\beta}_{age}-\beta^0_{age}}{\sigma^2_{\beta_{age}}} \underset{n \rightarrow \infty}{\sim} \Gaus[0,1] \label{eq:uniWald}
\end{align}
#+END_EXPORT
So under the null hypothesis of no association between the outcome and
the exposure the statistic \(t_{\beta_{age}}\) should follow a standard
normal distribution. Very low or very large values are unlikely to be
observed and would indicate that the null hypothesis does not
hold. This is called a (univariate) Wald test. 

\clearpage 

The result of this tests can be obtained using the =summary=
method [fn:2]:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(e.lm)$coef
#+END_SRC

#+RESULTS:
:               Estimate Std. Error   t value     Pr(>|t|)
: (Intercept) 91.5609817  9.4890315  9.649139 3.049271e-18
: GenderMale   3.7984283  2.1504309  1.766357 7.890869e-02
: Age         -0.1358252  0.1222051 -1.111452 2.677492e-01
: GeneB        7.8328783  2.6882498  2.913746 3.990606e-03
: GeneC        5.8120279  2.5395657  2.288591 2.318060e-02
: BMI          0.7364696  0.3583963  2.054903 4.122862e-02

[fn:2] In reality R is automatically performing a correction that
improves the control of the type 1 error. Indeed we usually don't know
\(\sigma^2_{\beta_{age}}\) and plugging-in its estimate in equation
eqref:eq:uniWald modifies the distribution of \(t_{\beta_{age}}\) in
small samples. The correction uses a Student's t distribution instead
of a Gaussian distribution.


95% confidence intervals for the model parameters can then be obtained
using the =confint= method:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
confint(e.lm)
#+END_SRC

#+RESULTS:
:                   2.5 %     97.5 %
: (Intercept) 72.84607298 110.275890
: GenderMale  -0.44279672   8.039653
: Age         -0.37684645   0.105196
: GeneB        2.53093057  13.134826
: GeneC        0.80332490  10.820731
: BMI          0.02961616   1.443323

Note that based on the estimate and standard errors, we could compute
the p-value ourself:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
beta <- summary(e.lm)$coef[,"Estimate"]
sigma <- summary(e.lm)$coef[,"Std. Error"]
df <- df.residual(e.lm)
t.abs <- abs(beta/sigma)
rbind(asymptotic = 2*(1-pnorm(t.abs)),
      corrected = 2*(1-pt(t.abs, df = df)))
cat("degrees of freedom:", df,"\n")
#+END_SRC

#+RESULTS:
:            (Intercept) GenderMale       Age       GeneB      GeneC        BMI
: asymptotic           0 0.07733600 0.2663737 0.003571198 0.02210311 0.03988840
: corrected            0 0.07890869 0.2677492 0.003990606 0.02318060 0.04122862
: degrees of freedom: 194

\clearpage 
and the confidence intervals:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
cbind(normLower = beta + qnorm(0.025) * sigma, 
      normUpper = beta + qnorm(0.975) * sigma,
      tLower = beta + qt(0.025, df = df) * sigma, 
      tUpper = beta + qt(0.975, df = df) * sigma)
#+END_SRC

#+RESULTS:
:               normLower   normUpper      tLower     tUpper
: (Intercept) 72.96282173 110.1591416 72.84607298 110.275890
: GenderMale  -0.41633879   8.0131954 -0.44279672   8.039653
: Age         -0.37534289   0.1036925 -0.37684645   0.105196
: GeneB        2.56400558  13.1017511  2.53093057  13.134826
: GeneC        0.83457057  10.7894853  0.80332490  10.820731
: BMI          0.03402571   1.4389134  0.02961616   1.443323

** Checking assumptions made when fitting a linear model 

*** *(A0)*: no unobserved confounders
*(A0)* is in general impossible to check.

*** *(A1)*: correct specification of the functional
*(A1)* can be (artificially) decomposed into two part:
- in absence of interaction, *is the effect of the continuous
  variables correctly modeled?* Typically it is modeled as a linear
  effect and the question is is there a non-linear effect. We can look
  at the plot of the covariate vs. the residuals and search for any
  trend:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gg <- ggplot(d, aes(x = BMI, y = residuals(e.lm)))
gg <- gg + geom_point() + geom_smooth() + ylab("residuals")
gg
#+END_SRC

#+RESULTS:
: `geom_smooth()` using method = 'loess' and formula 'y ~ x'

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=25)), filename = "./figures/A1-BMI.pdf")
#+END_SRC

#+RESULTS:
: Saving 6.99 x 7 in image
: `geom_smooth()` using method = 'loess' and formula 'y ~ x'

#+ATTR_LaTeX: :width 0.7\textwidth :placement [!h]
[[file:./figures/A1-BMI.pdf]] 

(similar plots can be automatically generated using the =crPlots= or
=ceresPlots= function from the car package). A p-value for testing the correct
specification of the functional form for the covariate can be obtained
using the =cumres= function from the gof package:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
cumres(e.lm, variable = "BMI")
#+END_SRC

#+RESULTS:
: 
:     p-value(Sup) p-value(L2)
: BMI            0           0
: 
: Based on 1000 realizations.

/Remedies/: if a trend is found, a possible remedy is to use splines to model the
non-linear relationship, e.g. 
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.gam <- mgcv::gam(Y1 ~ Gender + Age + Gene + s(BMI), data = dfW)
#+END_SRC

#+RESULTS:

In this simple example, it looks like a quadratic function of BMI
would be enough:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lm.1 <- lm(Y1 ~ Gender + Age + Gene + BMI + I(BMI^2), data = dfW)
cumres(e.lm.1, variable = "BMI")
#+END_SRC

#+RESULTS:
: 
:     p-value(Sup) p-value(L2)
: BMI        0.268       0.444
: 
: Based on 1000 realizations.
Note that this type of test is not appropriate to detect missing
interaction:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
cumres(e.lm.1, variable = "Age")
#+END_SRC

#+RESULTS:
: 
:     p-value(Sup) p-value(L2)
: Age        0.074       0.768
: 
: Based on 1000 realizations.
while the display of the residuals can be informative
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
gg <- ggplot(dfW, aes(x = Age, y = residuals(e.lm.1))) + geom_point() + geom_smooth()
gg
#+END_SRC

#+RESULTS:
: `geom_smooth()` using method = 'loess' and formula 'y ~ x'


#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=25)), filename = "./figures/A1-Age.pdf")
#+END_SRC
#+RESULTS:
: Saving 6.99 x 7 in image
: `geom_smooth()` using method = 'loess' and formula 'y ~ x'

#+ATTR_LaTeX: :width 0.8\textwidth :placement [!h]
[[file:./figures/A1-Age.pdf]] 

- *checking for interactions* is hard because the number of possible
  interactions grows quickly with the number of covariates. A typical
  test would be to compare a model with interactions to a model
  without interactions:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lm.2 <- update(e.lm, Y1 ~ Gender*Age + Gene + BMI + I(BMI^2))
anova(e.lm.1, e.lm.2)
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Model 1: Y1 ~ Gender + Age + Gene + BMI + I(BMI^2)
: Model 2: Y1 ~ Gender + Age + Gene + BMI + I(BMI^2) + Gender:Age
:   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    
: 1    193 16345.2                                  
: 2    192   509.8  1     15835 5963.5 < 2.2e-16 ***
: ---
: Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Note that in that case a test on the cumulative residuals process
would not detect any issue:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
cumres(e.lm.1, variable = "predicted")
#+END_SRC

#+RESULTS:
: 
:           p-value(Sup) p-value(L2)
: predicted        0.664       0.778
: 
: Based on 1000 realizations.

/Remedies/: this is a harder situation. When only few interactions are
considered, a possible strategy would be to include all of them and
perform backward selection. But then the p-values returned by =lm= for
the parameters related to the interactions (here =Gender=, =Age=, and
=Gender:Age=) will often be incorrect. Otherwise adding all possible
interactions and use a lasso/group-lasso penalty with post selection
inference. If the aim is prediction (and no inference), use more
flexible but less interpretable models (e.g. random forest).

\bigskip

- A last possible issue arise when the *outcome variable is not
  studied on the right scale*. Consider the model using a square root
  transformation:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.sqrt.lm <- lm(sqrt(Y1) ~ Gender*Age + Gene + BMI + I(BMI^2), data = dfW)
#+END_SRC

#+RESULTS:

\clearpage

Diagnostic plots indicates lack of fit (first line, first plot) and
heteroschedasticity (second line first plot):
#+RESULTS:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
autoplot(e.sqrt.lm)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(autoplot(e.sqrt.lm) + theme(text = element_text(size=15)), filename = "./figures/A1-scale.pdf")
#+END_SRC
#+RESULTS:
: Saving 6.99 x 7 in image

#+ATTR_LaTeX: :width 0.8\textwidth :placement [!h]
[[file:./figures/A1-scale.pdf]] 

We can use cumres and see that the link function seems inappropriate:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
cumres(e.sqrt.lm, variable = "predicted")
#+END_SRC

#+RESULTS:
: 
:           p-value(Sup) p-value(L2)
: predicted            0       0.001
: 
: Based on 1000 realizations.

\clearpage

In that case a box-cox transformation can be useful as it suggests to
square the outcome:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
M <- MASS::boxcox(e.sqrt.lm, lambda = seq(-1,4,by=0.1))
M$x[which.max(M$y)]
#+END_SRC

#+RESULTS:
: [1] 1.828283

Note that it seems to sometimes also suggest weird transformations:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
M <- MASS::boxcox(lm(log(Y1) ~ Gender*Age + Gene + BMI + I(BMI^2), data = dfW), lambda = seq(-10,10,by=0.1))
M$x[which.max(M$y)]
#+END_SRC

#+RESULTS:
: [1] 5.4
(the results should be 0)

*** *(A4)*: normal distribution

*(A4)* can be tested using an histogram of the standardized residuals:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
hist(residuals(e.lm.2, type = "pearson"), freq = FALSE, breaks = 10)
curve(dnorm,-3,3,add =TRUE,col = "red")
#+END_SRC

#+RESULTS:

   
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
pdf("./figures/A4-hist-res.pdf")
hist(residuals(e.lm.2, type = "pearson"), freq = FALSE, breaks = 10)
curve(dnorm,-3,3,add =TRUE,col = "red")
dev.off()
#+END_SRC





#+ATTR_LATEX: :width 0.8\textwidth
[[file:./figures/A4-hist-res.pdf]]

where the histogram should be close to the shape of the standard
normal distribution (red curve). We could reject *(A4)* but accept
*(A4-bis)* in the case where the distribution has heavy tails but is
still unimodal and symmetric. While intuitive, this method is
sensitive to the discretization of the residuals values (argument
break) and a qq-plot is often preferred:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
qqtest::qqtest(residuals(e.lm.2, type = "pearson"))
#+END_SRC

#+RESULTS:
: X11cairo 
:        2

#+BEGIN_SRC R :results graphics :file "./figures/A4-qqplot-res.pdf" :exports results :session *R* :cache no
qqtest::qqtest(residuals(e.lm.2, type = "pearson"))
#+END_SRC

#+ATTR_LATEX: :width 0.8\textwidth
[[file:./figures/A4-qqplot-res.pdf]]

Here the points should follow a straight line and be within the shaded
area. We could reject *(A4)* but accept *(A4-bis)* in the case where
deviation to the straight line mostly arise in the tails.  Statistical
test (like a shapiro test) are not recommended since they do not
enable us to know whether we reject *(A4)* or *(A4bis)*. 

\bigskip

/Remedies/: when *(A4)* is rejected but not *(A4-bis)*, the main
concern is about the validity of the traditional asymptotic
results. This is not critical in a linear regression where our
variance estimator is consistent and the central limit theorem ensures
asymptotic normality: instead of having exact p-values/CI they are
only asymptotically valid. If the sample size is not too small they
will hold; otherwise permutation test are a good alternative. In more
complex models, robust standard errors or non-parametric bootstrap can
be used for large enough samples to obtain p-values/CI robust to
deviation to the normal distribution. \newline A more serious problem
arises when *(A4-bis)* is rejected. In that case one should consider
whether the expected outcome is really relevant. Alternative
approaches include transformation of the outcome or use of alternative
regression models (quantile regression, probability index models,
finite mixture models).

\bigskip

Note 1: the =type= argument indicates the type of residuals we want to
extract. Raw residuals are \(\hat{\varepsilon} = Y-\hat{Y}\), i.e. the
observed minus the fitted values. In models more complex than a
univariate linear regression, the raw residuals may not be iid. This
makes it difficult to assess the validity of the assumptions. In such
cases we display instead diagnostics for normalized residuals that, if
the assumptions of the model are correct, should follow a standard
normal distribution.

\bigskip

Note 2: an alternative to the =qqtest= function is the =qqPlot=
function from the car package.

*** *(A2)*: Homeschedasticity
Homoschedasticity can be inspected by displaying the residuals along
the fitted values:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
d$residuals <- residuals(e.lm.2, type = "pearson")
d$fitted <- fitted(e.lm.2)
gg <- ggplot(d, aes(x = fitted)) + ylab("residuals")
gg <- gg + geom_smooth(aes(y = residuals^2-1))
gg <- gg + geom_point(aes(y = residuals))
gg
#+END_SRC

#+RESULTS:
: X11cairo 
:        2
: `geom_smooth()` using method = 'loess' and formula 'y ~ x'


#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=25)), filename = "./figures/A2-smooth.pdf")
#+END_SRC
#+RESULTS:
: Saving 6.99 x 7 in image
: `geom_smooth()` using method = 'loess' and formula 'y ~ x'


#+ATTR_LaTeX: :width 0.8\textwidth :placement [!h]
[[file:./figures/A2-smooth.pdf]]

(see also the function =spreadLevelPlot= from the car package). It is
also possible to have a global statistical test (Breusch-Pagan test):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
ncvTest(e.lm.2)
#+END_SRC

#+RESULTS:
: Non-constant Variance Score Test 
: Variance formula: ~ fitted.values 
: Chisquare = 0.2765166, Df = 1, p = 0.59899

Alternatively one can look along a specific regressor:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gg <- ggplot(d, aes(x = Gender, y = residuals)) + ylab("residuals")
gg <- gg + geom_boxplot()
gg
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=25)), filename = "./figures/A2-boxplot.pdf")
#+END_SRC

#+RESULTS:
: Saving 6.99 x 7 in image

#+ATTR_LaTeX: :width 0.8\textwidth :placement [!h]
[[file:./figures/A2-boxplot.pdf]]

or investigate look how the squared residuals relates to the
regressors:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(lm(residuals(e.lm.2)^2 ~ Gender + Age + Gene + BMI, data = dfW))$coef
#+END_SRC

#+RESULTS:
:                Estimate Std. Error    t value     Pr(>|t|)
: (Intercept)  3.27861402 3.07851696  1.0649979 2.882006e-01
: GenderMale  -2.92345176 0.69766213 -4.1903546 4.227552e-05
: Age         -0.03880792 0.03964689 -0.9788388 3.288787e-01
: GeneB        0.41620970 0.87214618  0.4772247 6.337393e-01
: GeneC        0.18989247 0.82390876  0.2304775 8.179636e-01
: BMI          0.07868374 0.11627415  0.6767088 4.993968e-01

/Remedies/: in presence of global heteroschadasticity (first graph),
transforming the outcome can be a solution. Otherwise one should
reflect about possible source of heteroschadasticity (e.g. correlated
observations, mixture of populations) and model them. When the
heteroschadasticity is related to a single variable, one can for
instance use the =gls= function to model this variance:

\clearpage

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.gls <- gls(Y1 ~ Gender + Age + Gene + BMI + I(BMI^2) + Gender:Age, 
             data = dfW,
             weight = varIdent(form=~1|Gender))
summary(e.gls$modelStruct)
#+END_SRC

#+RESULTS:
: Variance function:
:  Structure: Different standard deviations per stratum
:  Formula: ~1 | Gender 
:  Parameter estimates:
:     Male   Female 
: 1.000000 2.096499

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(
    lm(residuals(e.gls, type = "normalized")^2 ~ Gender + Age + Gene + BMI, data = dfW)
)$coef
#+END_SRC

#+RESULTS:
:                Estimate Std. Error    t value  Pr(>|t|)
: (Intercept)  0.95513614 0.92435359  1.0333017 0.3027491
: GenderMale   0.01093174 0.20947960  0.0521852 0.9584348
: Age         -0.01802386 0.01190435 -1.5140566 0.1316390
: GeneB        0.10417273 0.26187007  0.3978031 0.6912127
: GeneC       -0.05737372 0.24738633 -0.2319195 0.8168450
: BMI          0.02549938 0.03491241  0.7303817 0.4660380


*** *(A3)*: Uncorrelatedness                                     :noexport:

\noindent *(A3)* Independence is hard to check without a-priori information. But
if one suspects correlation along one variable, one can use a
correlagram to test the independence assumption (correlation would
imply violation of A1). Let's for instance use the Id variable as a
proxy for measurement time and see if observations measured in a short
time interval are correlated:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
acf(x = residuals(e.lm))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :results graphics :file "./figures/fig-acf.pdf" :exports results :session *R* :cache yes
acf(x = residuals(e.lm))
#+END_SRC

#+RESULTS[<2021-03-08 20:03:51> 4e9dba48b0b6b6531476da37e0c184b426c6985e]:
[[file:./figures/fig-acf.pdf]]

WARNING: this approach assumes that the observations are ordered and
equaly spaced in time. The corresponding test is the Durbin-Watson
test:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
durbinWatsonTest(e.lm)
#+END_SRC

#+RESULTS:
:  lag Autocorrelation D-W Statistic p-value
:    1       0.1154692      1.766587   0.072
:  Alternative hypothesis: rho != 0

*** *(A5)*: Influential observations

The =influence= method can be used to output what is the impact of
each observation on each estimated parameter:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
if.lme <- influence(e.lm.2)
if.lme$coefficient[1:6,1:4]
#+END_SRC

#+RESULTS:
:     (Intercept)    GenderMale           Age         GeneB
: 1 -0.0768500758 -1.729088e-02 -8.430716e-06 -6.556256e-03
: 2  0.0033441763  8.848004e-04 -1.583090e-06 -1.330066e-03
: 3 -1.4634794147 -8.233340e-02 -2.155624e-04  4.249566e-02
: 4  0.0759756521 -4.035183e-03  4.852756e-05 -8.238371e-04
: 5 -0.0427789759 -1.599394e-03  2.073558e-05 -8.720082e-03
: 6  0.0008298817  4.867202e-05  2.398257e-07 -3.293940e-06

Here the value in the first line and third column indicates by how
much is changed the Age effect when removing the first observation.
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
coef(update(e.lm.2, data = dfW[-1,]))-coef(e.lm.2)
#+END_SRC

#+RESULTS:
:    (Intercept)     GenderMale            Age          GeneB          GeneC            BMI 
:   7.685008e-02   1.729088e-02   8.430716e-06   6.556256e-03   8.024692e-03  -7.151957e-03 
:       I(BMI^2) GenderMale:Age 
:   1.540482e-04  -6.600643e-04

Large values (positive or negative) indicate influential
observations. The following plot displaying in red the coefficient
value and in black the influence of each individual can be useful:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
df1.gg <- data.frame(id = "true", as.data.frame(t(coef(e.lm.2))))
df2.gg <- data.frame(id = as.character(1:NROW(d)), 
                     sweep(if.lme$coefficient, FUN = "+", MARGIN = 2, STATS = coef(e.lm.2)))
dfL1.gg <- reshape2::melt(df1.gg, id.vars = "id")
dfL2.gg <- reshape2::melt(df2.gg, id.vars = "id")
gg.inf <-  ggplot() + facet_wrap(~variable, scales = "free", nrow = 2)
gg.inf <- gg.inf + geom_boxplot(data = dfL2.gg, aes(y = value))
gg.inf <- gg.inf + geom_hline(data = dfL1.gg, aes(yintercept = value), color = "red")
gg.inf
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg.inf + theme(text = element_text(size=14)), filename = "./figures/A5-boxplot.pdf", width = 10)
#+END_SRC

#+RESULTS:
: Saving 10 x 7 in image


#+ATTR_LaTeX: :width \textwidth :placement [!h]
[[file:./figures/A5-boxplot.pdf]]

When the aim is to perform prediction, global influence metrics such
as Cook's distance can be useful:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
autoplot(e.lm.2, which = 4)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(autoplot(e.lm.2, which = 4), filename = "./figures/A5-cook.pdf")
#+END_SRC

#+RESULTS:
: Saving 6.99 x 7 in image

#+ATTR_LaTeX: :width 0.8\textwidth :placement [!h]
[[file:./figures/A5-cook.pdf]]


*** Others [not recommanded unless specific reasons]
Some people recommand to check the correlation between the explanatory
variables, with the argument that when very correlated it is difficult
to disantangle effects and thus to interpret the regression
coefficients. The VIF (variance inflation factor) is typically
recommanded to check that with values higher than 5 considered as
high:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
car::vif(e.lm.2)
#+END_SRC

#+RESULTS:
:                  GVIF Df GVIF^(1/(2*Df))
: Gender      16.405278  1        4.050343
: Age          2.107105  1        1.451587
: Gene         1.070289  2        1.017127
: BMI        153.493940  1       12.389267
: I(BMI^2)   153.046502  1       12.371196
: Gender:Age  17.937937  1        4.235320

I personnally don't recommand this as an automatic check since in many
  settings co-linearity can be better assessed from the meaning of the
  variables than from a statistical test. It is also quite unclear to
  me why 5 is a good cut-off and we see in this example that we get
  values close to five (or higher) even though there is no issue.

\clearpage


* Partial residuals 
:PROPERTIES:
:CUSTOM_ID: sec:pRes
:END:

** With respect to one variable

The partial residuals with respect to age are defined by removing the
effect of all the covariates but age on the outcome:
#+BEGIN_EXPORT latex
\begin{align*}
\hat{\varepsilon}^{Age}_i &= Y_i - \left(\alpha + \beta_{Gender} \Ind[Gender_i="Male"] + \beta_{GeneB} \Ind[Gene_i="B"] + \beta_{GeneC} \Ind[Gene_i="C"]  + \beta_{BMI} BMI_i\right)
\end{align*}
#+END_EXPORT
Using the following model coefficients:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
coef(e.lm)
#+END_SRC 

#+RESULTS:
: (Intercept)  GenderMale         Age       GeneB       GeneC         BMI 
:  91.5609817   3.7984283  -0.1358252   7.8328783   5.8120279   0.7364696

and considering the first individual:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
d[1,]
#+END_SRC

#+RESULTS:
:   Id  Age Gender  BMI Gene    Y1    Y2       Y3 residuals   fitted
: 1  1 44.2   Male 23.8    A 109.1 120.8 131.7429 0.5188666 108.5811

the partial residual relative to age is:
#+BEGIN_EXPORT latex
\begin{align*}
\hat{\varepsilon}^{Age}_1 &= 109.1 - \left(91.5610 + 3.7984 * 1 + 7.8329 * 0 + 5.8120 * 0 + 0.7365 * 23.8 \right) \\
                         &= 109.1 - 112.8881 = -3.7881
\end{align*}
#+END_EXPORT
At the dataset level, this type of partial residual is centered around
the expected value of the covariate times its effect (here
\(-0.1358252*34.4855 \approx -4.684 \)). These partial residuals can be
computed using the =partialResidual= function from the butils package:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
pRes.noI <- partialResiduals(e.lm, var = "Age", keep.intercept = FALSE)
head(pRes.noI)
#+END_SRC

#+RESULTS:
:    Id  Age Gender  BMI Gene    Y1    Y2       Y3     pFit ranef  pResiduals
: 1:  1 44.2   Male 23.8    A 109.1 120.8 131.7429 112.8874     0  -3.7873855
: 2:  2 41.3   Male 27.5    B 123.2 133.9 136.3850 123.4452     0  -0.2452012
: 3:  3 27.4   Male 30.6    A 140.6 154.0 136.1408 117.8954     0  22.7046216
: 4:  4 35.3   Male 25.2    C 104.4 116.2 125.0175 119.7305     0 -15.3304708
: 5:  5 37.1   Male 21.8    A 105.0 113.2 123.6257 111.4144     0  -6.4144463
: 6:  6 29.9   Male 18.1    C 125.9 136.2 131.7966 114.5015     0  11.3984631

or manually:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
keep.coef <- c("(Intercept)","GenderMale","GeneB","GeneC","BMI")
dfW$Y1[1] - model.matrix(e.lm)[1,keep.coef] %*% coef(e.lm)[keep.coef]
#+END_SRC

#+RESULTS:
:           [,1]
: [1,] -3.787385

A graphical display can be obtained using the =autoplot= function
(require the ggplot2 package):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gg <- autoplot(pRes.noI)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=25)), filename = "./figures/fig-butils-plotConf-noI.pdf")
#+END_SRC

#+RESULTS:
: Saving 7 x 7 in image

#+ATTR_LaTeX: :width 0.7\textwidth :placement [!h]
[[file:./figures/fig-butils-plotConf-noI.pdf]]

- An alternative definition do not remove the intercept effect:
#+BEGIN_EXPORT latex
\begin{align*}
\hat{\varepsilon}^{Age,\alpha}_i &= Y_i - \left(\beta_{Gender} \Ind[Gender_i="Male"] + \beta_{GeneB} \Ind[Gene_i="B"] + \beta_{GeneC} \Ind[Gene_i="C"] + \beta_{BMI} BMI_i \right)
\end{align*}
#+END_EXPORT
so now the residuals are centered around the intercept plus the
expected value of age times the age effect (here approximately 0). 

\clearpage

As before the partial residuals can either be obtained via the
=partialResiduals= function:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
pRes.I <- partialResiduals(e.lm, var = "Age", keep.intercept = TRUE)
head(pRes.I)
#+END_SRC

#+RESULTS:
:    Id  Age Gender  BMI Gene    Y1    Y2       Y3     pFit ranef pResiduals
: 1:  1 44.2   Male 23.8    A 109.1 120.8 131.7429 21.32640     0   87.77360
: 2:  2 41.3   Male 27.5    B 123.2 133.9 136.3850 31.88422     0   91.31578
: 3:  3 27.4   Male 30.6    A 140.6 154.0 136.1408 26.33440     0  114.26560
: 4:  4 35.3   Male 25.2    C 104.4 116.2 125.0175 28.16949     0   76.23051
: 5:  5 37.1   Male 21.8    A 105.0 113.2 123.6257 19.85346     0   85.14654
: 6:  6 29.9   Male 18.1    C 125.9 136.2 131.7966 22.94056     0  102.95944

or manually: 
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
keep.coef <- c("GenderMale","GeneB","GeneC","BMI")
dfW$Y1[1] - model.matrix(e.lm)[1,keep.coef] %*% coef(e.lm)[keep.coef]
#+END_SRC

#+RESULTS:
:         [,1]
: [1,] 87.7736

This corresponds to what the =plotConf= function is displaying (R
package lava available on CRAN):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
lava::plotConf(e.lm, var1 = "Age")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
pdf("./figures/fig-lava-plotConf.pdf")
lava::plotConf(e.lm, var1 = "Age")
dev.off()
#+END_SRC


#+ATTR_LaTeX: :width 0.7\textwidth :placement [!h]
[[file:./figures/fig-lava-plotConf.pdf]]

Note that it is also possible to display the partial residuals for a
categorical variable:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
pRes.cat <- partialResiduals(e.lm, var = "Gene", keep.intercept = TRUE)
gg <- autoplot(pRes.cat)
gg
#+END_SRC

#+RESULTS:
: X11cairo 
:        2


#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=25)), filename = "./figures/fig-butils-plotConf-categorical.pdf")
#+END_SRC

#+RESULTS:
: Saving 6.99 x 7 in image

#+ATTR_LaTeX: :width 0.7\textwidth :placement [!h]
[[file:./figures/fig-butils-plotConf-categorical.pdf]]

** With respect to an interaction between two variables (one continuous, one categorical)

Consider now a model where the age effect can be different for males
and females:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lmI <- lm(Y1 ~ Gender * Age + Gene + BMI, data = dfW)
#+END_SRC

#+RESULTS:
The partial residuals can be defined in a similar way as before. Here
the effect of Age and Gender (and their interaction) are not
substracted from the outcome:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gg <- autoplot(partialResiduals(e.lmI, var = c("Age","Gender")))
#+END_SRC

#+RESULTS:


#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(gg + theme(text = element_text(size=20)), 
       filename = "./figures/fig-butils-plotConf-interaction.pdf", width = 10)
#+END_SRC

#+RESULTS:
: Saving 10 x 7 in image

#+ATTR_LaTeX: :width 0.7\textwidth :placement [!h]
[[file:./figures/fig-butils-plotConf-interaction.pdf]]

** Customizing a partial residual plot

The autoplot function returns the ggplot object:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gg <- autoplot(partialResiduals(e.lm, var = "Gene", keep.intercept = TRUE))
class(gg)
#+END_SRC

#+RESULTS:
: [1] "gg"     "ggplot"

So it can be easily customized, e.g. the text can be made bigger by
doing:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gg + theme(text = element_text(size=25))
#+END_SRC

#+RESULTS:

* Reference :noexport:
# help: https://gking.harvard.edu/files/natnotes2.pdf

#+BEGIN_EXPORT latex
\begingroup
\renewcommand{\section}[2]{}
#+END_EXPORT
bibliographystyle:apalike
[[bibliography:bibliography.bib]] 
#+BEGIN_EXPORT latex
\endgroup
#+END_EXPORT

#+BEGIN_EXPORT LaTeX
\appendix
\titleformat{\section}
{\normalfont\Large\bfseries}{}{1em}{Appendix~\thesection:~}

\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\theequation}{\Alph{equation}}

\setcounter{figure}{0}    
\setcounter{table}{0}    
\setcounter{equation}{0}    

\setcounter{page}{1}
#+END_EXPORT

* CONFIG :noexport:
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+LaTeX_CLASS_OPTIONS: [12pt]
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Display of the document
# ## space between lines
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}

# ## margins
#+LATEX_HEADER:\geometry{top=1cm}

# ## personalize the prefix in the name of the sections
#+LaTeX_HEADER: \usepackage{titlesec}
# ## fix bug in titlesec version
# ##  https://tex.stackexchange.com/questions/299969/titlesec-loss-of-section-numbering-with-the-new-update-2016-03-15
#+LaTeX_HEADER: \usepackage{etoolbox}
#+LaTeX_HEADER: 
#+LaTeX_HEADER: \makeatletter
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\noindent}{}{}{}
#+LaTeX_HEADER: \makeatother

** Color
# ## define new colors
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LaTeX_HEADER: \definecolor{myorange}{rgb}{1,0.2,0}
#+LaTeX_HEADER: \definecolor{mypurple}{rgb}{0.7,0,8}
#+LaTeX_HEADER: \definecolor{mycyan}{rgb}{0,0.6,0.6}
#+LaTeX_HEADER: \newcommand{\lightblue}{blue!50!white}
#+LaTeX_HEADER: \newcommand{\darkblue}{blue!80!black}
#+LaTeX_HEADER: \newcommand{\darkgreen}{green!50!black}
#+LaTeX_HEADER: \newcommand{\darkred}{red!50!black}
#+LaTeX_HEADER: \definecolor{gray}{gray}{0.5}

# ## change the color of the links
#+LaTeX_HEADER: \hypersetup{
#+LaTeX_HEADER:  citecolor=[rgb]{0,0.5,0},
#+LaTeX_HEADER:  urlcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER:  linkcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER: }

** Font
# https://tex.stackexchange.com/questions/25249/how-do-i-use-a-particular-font-for-a-small-section-of-text-in-my-document
#+LaTeX_HEADER: \newenvironment{comment}{\small \color{gray}\fontfamily{lmtt}\selectfont}{\par}
#+LaTeX_HEADER: \newenvironment{activity}{\color{orange}\fontfamily{qzc}\selectfont}{\par}

** Symbols
# ## valid and cross symbols
#+LaTeX_HEADER: \RequirePackage{pifont}
#+LaTeX_HEADER: \RequirePackage{relsize}
#+LaTeX_HEADER: \newcommand{\Cross}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{56}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\Valid}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{52}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\CrossR}{ \textcolor{red}{\Cross} }
#+LaTeX_HEADER: \newcommand{\ValidV}{ \textcolor{green}{\Valid} }

# ## warning symbol
#+LaTeX_HEADER: \usepackage{stackengine}
#+LaTeX_HEADER: \usepackage{scalerel}
#+LaTeX_HEADER: \newcommand\Warning[1][3ex]{%
#+LaTeX_HEADER:   \renewcommand\stacktype{L}%
#+LaTeX_HEADER:   \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}%
#+LaTeX_HEADER:   \xspace
#+LaTeX_HEADER: }

# # R Software
#+LATEX_HEADER: \newcommand\Rlogo{\textbf{\textsf{R}}\xspace} % 

** Code
# Documentation at https://org-babel.readthedocs.io/en/latest/header-args/#results
# :tangle (yes/no/filename) extract source code with org-babel-tangle-file, see http://orgmode.org/manual/Extracting-source-code.html 
# :cache (yes/no)
# :eval (yes/no/never)
# :results (value/output/silent/graphics/raw/latex)
# :export (code/results/none/both)
#+PROPERTY: header-args :session *R* :tangle yes :cache no ## extra argument need to be on the same line as :session *R*

# Code display:
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

# ## change font size input (global change)
# ## doc: https://ctan.math.illinois.edu/macros/latex/contrib/listings/listings.pdf
# #+LATEX_HEADER: \newskip\skipamount   \skipamount =6pt plus 0pt minus 6pt
#+LATEX_HEADER: \lstdefinestyle{code-tiny}{basicstyle=\ttfamily\footnotesize}
# #+LATEX_HEADER: \lstset{style=code-tiny}
# ## change font size input (local change, put just before BEGIN_SRC)
# ## #+ATTR_LATEX: :options basicstyle=\ttfamily\scriptsize
# ## change font size output (global change)
# ## \RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\tiny,formatcom = {\color[rgb]{0.5,0,0}}}

** Lists
#+LATEX_HEADER: \RequirePackage{enumitem} % better than enumerate

** Image and graphs
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of} % 
#+LATEX_HEADER: \RequirePackage{caption} % newlines in graphics

#+LaTeX_HEADER: \RequirePackage{tikz-cd} % graph
# ## https://tools.ietf.org/doc/texlive-doc/latex/tikz-cd/tikz-cd-doc.pdf

** Table
#+LATEX_HEADER: \RequirePackage{booktabs} % for nice lines in table (e.g. toprule, bottomrule, midrule, cmidrule)

** Inline latex
# @@latex:any arbitrary LaTeX code@@


** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
# #+LaTeX_HEADER: \RequirePackage{amsthm}
# #+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
# #+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \usepackage{ifthen}
#+LATEX_HEADER: \usepackage{xifthen}
#+LATEX_HEADER: \usepackage{xargs}
#+LATEX_HEADER: \usepackage{xspace}

#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

**** Probability
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\logLikelihood[2][1=,2=]{\defOperator{#1}{#2}{\ell}{}{(}{)}{}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\partial #2^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}

#+LATEX_HEADER: \newcommand\Veta{\boldsymbol{\eta}}
#+LATEX_HEADER: \newcommand\VX{\mathbf{X}}


** Notations
