#+TITLE: Partial correlation in linear models
#+Author: Brice Ozenne

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
if(system("whoami",intern=TRUE)=="bozenne"){
  path <- "~/Documents/"
}else{
  path <- "c:/Users/hpl802/Documents/"
}
setwd(file.path(path,"GitHub/bozenne.github.io/doc/2022_07_08-partial-correlation/"))
#+END_SRC

#+RESULTS:

* Summary

This document starts by presenting how to extract from a (univariate)
linear regression model partial correlation coefficients. It also
precise what type of "partial" (i.e. adjusted on which covariate) we
get. When having multiple measurements of pairs of variables, various
technics to estimate (partial) correlations are being compared.

* Example

For illustration we will use the following packages:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(LMMstar)
library(ggplot2)
library(lme4)
library(lmerTest)
library(Matrix)
library(data.table)
#+END_SRC

#+RESULTS:

and dataset citep:bland1995calculating:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
data("bland1995", package = "rmcorr")
bland1995$Subject <- as.factor(bland1995$Subject)
bland1995$time <- unlist(tapply(bland1995$Subject,bland1995$Subject, function(x){1:length(x)}))
head(bland1995)
#+END_SRC

#+RESULTS:
:   Subject   pH PacO2 time
: 1       1 6.68  3.97    1
: 2       1 6.53  4.12    2
: 3       1 6.43  4.09    3
: 4       1 6.33  3.97    4
: 5       2 6.85  5.27    1
: 6       2 7.06  5.37    2

\clearpage

The aim is to relate intramural pH and PaCO2 using eight subjects:

#+BEGIN_SRC R :exports code :results output :session *R* :cache no
gg <- ggplot(bland1995, aes(x = pH, y = PacO2,
                            group = Subject, color = Subject))
gg <- gg + geom_point() + geom_smooth(method = "lm", se = FALSE)
gg
#+END_SRC

#+RESULTS:
: `geom_smooth()` using formula 'y ~ x'

#+BEGIN_SRC R :exports none :results output raw drawer :session *R* :cache no
ggsave(gg + theme(text = element_text(size=15),
                  axis.line = element_line(size = 1),
                  axis.ticks = element_line(size = 1),
                  axis.ticks.length=unit(.25, "cm")), filename = file.path("figures","gg-describe-dataset.pdf") )
#+END_SRC

#+RESULTS:
:results:
Saving 6.99 x 7 in image
`geom_smooth()` using formula 'y ~ x'
:end:

#+ATTR_LaTeX: :width 1\textwidth :options trim={0 0 0 0} :placement [!h]
[[./figures/gg-describe-dataset.pdf]]


\clearpage

* Partial partial in multiple linear regression

Consider the linear model:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lmm <- lmm(pH ~ Subject + PacO2, data = bland1995)
eTable.lmm <- model.tables(e.lmm)
eTable.lmm
#+END_SRC

#+RESULTS:
#+begin_example
              estimate         se df      lower       upper      p.value
(Intercept)  6.9298543 0.12946898 38  6.6677580  7.19195056 0.000000e+00
Subject2     0.7046113 0.07735488 38  0.5480145  0.86120804 4.277623e-11
Subject3     0.9500127 0.06109545 38  0.8263314  1.07369394 0.000000e+00
Subject4     0.9715577 0.07350906 38  0.8227464  1.12036905 8.881784e-16
Subject5     0.8603817 0.05839543 38  0.7421663  0.97859708 0.000000e+00
Subject6     0.9264284 0.06599450 38  0.7928295  1.06002730 0.000000e+00
Subject7     0.6921056 0.10490935 38  0.4797277  0.90448342 8.670218e-08
Subject8     0.7033361 0.06157141 38  0.5786913  0.82798087 7.460699e-14
PacO2       -0.1083230 0.02989281 38 -0.1688379 -0.04780822 8.471081e-04
#+end_example

We claim the partial correlation (adjusting =pH= and =PacO2= for
=Subject=) can be deduced from the Wald statistic and degrees of
freedom:

#+BEGIN_EXPORT latex
\begin{align}
\rho = \frac{\frac{\beta}{\sigma_{\beta}}}{\sqrt{\frac{\beta^2}{\sigma^2_{\beta}}+df}} = \frac{\beta}{\sqrt{\beta^2+df*\sigma_{\beta}^2}} \label{eq:pCor-formula}
\end{align}
#+END_EXPORT

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
eTable.lmm["PacO2","estimate"]/sqrt(eTable.lmm["PacO2","estimate"]^2+eTable.lmm["PacO2","df"]*eTable.lmm["PacO2","se"]^2)
#+END_SRC

#+RESULTS:
: [1] -0.5067697

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Wald <- eTable.lmm["PacO2","statistic"]
Wald/sqrt(Wald^2+eTable.lmm["PacO2","df"])
#+END_SRC

#+RESULTS:
: [1] -0.5067697

The proof can be split in three steps:
- 1. :: the F-statistic testing the effect of each factor equals the
  Wald-statistic squared (divided by 1, the number of parameters)

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Wald^2
#+END_SRC

#+RESULTS:
: [1] 13.13132

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
anova(e.lmm)
#+END_SRC

#+RESULTS:
: 		Multivariate Wald test 
: 
:                  F-statistic       df p.value    
:    mean: Subject      48.247 (7,38.0)  <0.001 ***
:        : PacO2        13.131 (1,38.0)  <0.001 ***

- 2. :: this F-statistic equals \(\frac{MSSR}{MSSE}\) where \(MSSR =
  SSR/1\) and \(MSSE = SSE/(n-p)\) with \(SSE\) and \(SSR\) being the
  explained and residual sum of squares. We can check that this
  extends to multiple regression using the usual anova table:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
anova(lm(pH ~ Subject + PacO2, data = bland1995))
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Response: pH
:           Df  Sum Sq Mean Sq F value    Pr(>F)    
: Subject    7 2.86484 0.40926  46.600 < 2.2e-16 ***
: PacO2      1 0.11532 0.11532  13.131 0.0008471 ***
: Residuals 38 0.33373 0.00878                      
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

which is to be compared to[fn::\Warning Since \Rlogo output type 1 anova only the last and second to
last line are relevant. The first line (=Subject=) is for a model
without =PacO2= so it should be expected that the F-value does not
match with the one of =Subject= in a model with =PacO2=.]

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
sigma2 <- as.double(sigma(e.lmm))
beta <- eTable.lmm["PacO2","estimate"]
sigma_beta <- eTable.lmm["PacO2","se"]
c(MSSE = sigma2, MSSR = sigma2 * beta^2 /sigma_beta^2)
#+END_SRC

#+RESULTS:
:        MSSE        MSSR 
: 0.008782435 0.115324959

This result can be easily proved when considering a model with a single
regressor:
#+BEGIN_EXPORT latex
\[ Y = X\beta + \varepsilon\text{, } \varepsilon\sim\Gaus(0,\sigma^2)\]
#+END_EXPORT
where we would have centered the outcome \(Y\). Here we denote by
\(X\) the design matrix, \(n\) the number of observations and \(p=1\)
the number of coefficients, \(H = X (X\trans{X})^{-1} \trans{X}\) the
hat matrix and \(\widehat{\beta} = (X\trans{X})^{-1} \trans{X}Y\) the
OLS estimator of the regression coefficients.
#+BEGIN_EXPORT latex
\begin{align*}
\Var(Y) = Y\trans{Y} =& YH\trans{Y} + Y(1-H)\trans{Y} \\
SST =& SSR + SSE \\
    =& \hat{\beta} (X\trans{X}) \trans{\hat{\beta}} + Y (1-H) \trans{Y} \\
    =& \sigma^2 (\hat{\beta} \Sigma^{-1}_{\hat{\beta}} \trans{\hat{\beta}} + n-p) \\
\frac{MSSR}{MSSE} &= \frac{\hat{\beta}^2}{\Sigma_{\hat{\beta}}} = Wald^2
\end{align*}
#+END_EXPORT

- 3. :: the \(R^2\) is defined as the proportion of variance explained, so
  using the previous results we get:
#+BEGIN_EXPORT latex
\begin{align*}
R^2 =& \frac{SSR}{SSR + SSE} \\
    =& \frac{1}{1 + SSE/SSR} \\
    =& \frac{1}{1 + (n-p)/(\beta^2/\sigma^2_\beta)} \\
    = \frac{Wald^2}{Wald^2 + n-p}
\end{align*}
#+END_EXPORT

This formula matches exactly the partial correlation coefficient when
*both* outcome are adjusted for =Subject=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.partialCor <- partialCor(list(pH ~ Subject, PacO2 ~ Subject),
                           data = bland1995)
print(e.partialCor, digit = 5)
#+END_SRC

#+RESULTS:
: 		Partial correlation 
: 
:               estimate    se   df lower  upper p.value
: rho(pH,PacO2)   -0.507 0.125 25.7 -0.71 -0.225 0.00178
: 
:    Note: estimate, standard error, confidence interval have been back-transformed (rho parameters with tanh).

Similar values can be obtained using dedicated packages, e.g.:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(rmcorr)
rmcorr(Subject, PacO2, pH, bland1995)$r
#+END_SRC

#+RESULTS:
: [1] -0.5067697

\clearpage

* Partial correlation with repeated measurements

** Marginal and conditional correlation
There are several references on the subject
citep:bland1995calculating,Lipsitz2001partial,bakdash2017repeated,shan2020correlation. We
will focus on the mixed model approach. The idea is to jointly model
the variance and covariance of all measurements under appropriate
constrains. For instance denoting one measurement \(X\) and the other
measurement \(Y\), both indexed by time \(t\), our target parameter
may be \(\rho = \mathbb{C}or(X(t),Y(t))\) (marginal) assumed independent of
\(t\) while \(X\) and \(Y\) may or may not be stationnary. Another
target parameter could be the correlation between a de-noised version
of \(X\) and \(Y\), where we have for instance removed
individual-specific variations (conditional).

\bigskip

To be more specific let's consider the following statistical model:
#+BEGIN_EXPORT latex
\begin{align*}
X_i(t) &= \mu_{X,i}(t) + u_i + \varepsilon_{X,i}(t) \\
Y_i(t) &= \mu_{Y,i}(t) + v_i + \varepsilon_{Y,i}(t) \\
\text{where } \begin{bmatrix}u \\ v \\ \varepsilon_X(t) \\ \varepsilon_Y(t) \end{bmatrix}
&= \Gaus\left(\begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \end{bmatrix},
\begin{bmatrix}
\tau_u & \tau_{uv} & 0 & 0 \\ \tau_{uv} & \tau_v & 0 & 0 \\ 
 0 & 0 & \sigma_X & \sigma_{XY} \\ 0 & 0 & \sigma_{XY} & \sigma_X \\ 
\end{bmatrix} \right)
\end{align*}
#+END_EXPORT
It implies the following residual covariance matrix:
#+BEGIN_EXPORT latex
\begin{align*}
\Omega = \Var\begin{bmatrix}X(1) \\ X(2) \\ X(3) \\ Y(1) \\ Y(2) \\ Y(3) \end{bmatrix}
&= \begin{bmatrix}
\tau_u + \sigma_X & \tau_u & \tau_u & \tau_{uv} + \sigma_{XY} & \tau_{uv} & \tau_{uv} \\
\tau_u & \tau_u + \sigma_X & \tau_u & \tau_{uv} & \tau_{uv} + \sigma_{XY} & \tau_{uv} \\
\tau_u & \tau_u & \tau_u + \sigma_X & \tau_{uv} & \tau_{uv} & \tau_{uv} + \sigma_{XY} \\
\tau_{uv} + \sigma_{XY} & \tau_{uv}  & \tau_{uv} & \tau_v + \sigma_Y & \tau_v & \tau_v \\
\tau_{uv} & \tau_{uv} + \sigma_{XY} & \tau_{uv}  & \tau_v & \tau_v + \sigma_Y & \tau_v \\
\tau_{uv} & \tau_{uv} & \tau_{uv} + \sigma_{XY}  & \tau_v & \tau_v & \tau_v + \sigma_Y  \\
\end{bmatrix} \\
&= \begin{bmatrix}
\sigma_1 & \sigma_2 & \sigma_2 & \sigma_3 & \sigma_4 & \sigma_4 \\
\sigma_2 & \sigma_1 & \sigma_2 & \sigma_4 & \sigma_3 & \sigma_4 \\
\sigma_2 & \sigma_2 & \sigma_1 & \sigma_4 & \sigma_4 & \sigma_3 \\
\sigma_3 & \sigma_4 & \sigma_4 & \sigma_5 & \sigma_6 & \sigma_6 \\
\sigma_4 & \sigma_3 & \sigma_4 & \sigma_6 & \sigma_5 & \sigma_6 \\
\sigma_4 & \sigma_4 & \sigma_3 & \sigma_6 & \sigma_6 & \sigma_5  \\
\end{bmatrix}
\end{align*}
#+END_EXPORT
and the following residual correlation matrix:
#+BEGIN_EXPORT latex
\[ R = \mathbb{C}or\begin{bmatrix}X(1) \\ X(2) \\ X(3) \\ Y(1) \\ Y(2) \\ Y(3) \end{bmatrix}
= \begin{bmatrix}
1      & \rho_1 & \rho_1 & \rho_2 & \rho_3 & \rho_3 \\
\rho_1 & 1      & \rho_1 & \rho_3 & \rho_2 & \rho_3 \\
\rho_1 & \rho_1 & 1      & \rho_3 & \rho_3 & \rho_2 \\
\rho_2 & \rho_3 & \rho_3 & 1      & \rho_4 & \rho_4 \\
\rho_3 & \rho_2 & \rho_3 & \rho_4 & 1      & \rho_4 \\
\rho_3 & \rho_3 & \rho_2 & \rho_4 & \rho_4 & 1  \\
\end{bmatrix}
\]
#+END_EXPORT


The marginal correlation is:
#+BEGIN_EXPORT latex
\begin{align*}
\rho_M &= \frac{\Cov[u_i + \varepsilon_{X,i}(t),v_i + \varepsilon_{Y,i}(t)]}{\sqrt{\Var[u_i + \varepsilon_{X,i}(t)]\Var[v_i + \varepsilon_{Y,i}(t)]}} \\
&= \frac{\tau_{uv} + \sigma_{XY}}{\sqrt{(\tau_u+\sigma_X)(\tau_v+\sigma_Y)}} = \frac{\sigma_3}{\sqrt{\sigma_1\sigma_5}} =  \rho_2
\end{align*}
#+END_EXPORT
while the conditional correlation is:
#+BEGIN_EXPORT latex
\begin{align*}
\rho_C &= \frac{\Cov[\varepsilon_{X,i}(t),\varepsilon_{Y,i}(t)]}{\sqrt{\Var[\varepsilon_{X,i}(t)]\Var[\varepsilon_{Y,i}(t)]}} \\
&= \frac{\sigma_{XY}}{\sqrt{\sigma_X\sigma_Y}} = \frac{\sigma_3-\sigma_4}{\sqrt{(\sigma_1-\sigma_2)(\sigma_5-\sigma_6)}} =  \frac{\rho_2-\rho_3}{\sqrt{(1-\rho_1)(1-\rho_2)}}
\end{align*}
#+END_EXPORT

** Approximated conditional correlation

We now show that formula \ref{eq:pCor-formula} generalizes to mixed
models. Consider the following mixed model relating \(\VY =
(Y_1,\ldots,Y_T)\) and \(\VX = (X_1,\ldots,X_T)\):
#+BEGIN_EXPORT latex
\[ \VY = \VX \beta + \Vvarepsilon \]
#+END_EXPORT
where \(\boldsymbol{\varepsilon}\sim\Gaus[0,\Omega]\). Introducing the
cholesky decomposition \(\Omega = \omega\trans{\omega} \), we can equivalently study:
#+BEGIN_EXPORT latex
\[ \omega^{-1}\VY = \omega^{-1}\VX + \boldsymbol{\zeta} \]
#+END_EXPORT
where \(\boldsymbol{\zeta}\) follow a standard normal distribution. We
are back the univariate case up to a factor \(\omega^{-1}\).

- 1. :: F-statistics are still equal the Wald statistic squared
  (divided by the number of parameters).
- 2. :: F-statistics still equal \(\frac{MSSR}{MSSE}\). Indeed:
#+BEGIN_EXPORT latex
\begin{align*}
SSE &= \trans{\left(\omega^{-1}\VY\right)}\left(I-\omega^{-1}\VX\left(\trans{\left(\omega^{-1}\VX\right)}\left(\omega^{-1}\VX\right)\right)^{-1} \trans{\left(\omega^{-1}\VX\right)}\right)\left(\omega^{-1}\VY\right) \\
&= \trans{\VY} \Omega^{-1} \VY - \trans{\VY} \Omega^{-1} \VX \left(\trans{\VX}\Omega^{-1}\VX\right)^{-1} \trans{\VX} \Omega^{-1} \VY  \\
&= \trans{\VY} (I-\trans{H})\Omega^{-1} (I-\trans{H}) \VY
\end{align*}
#+END_EXPORT
where \(H = \VX \left(\trans{\VX}\Omega^{-1} \VX \right)^{-1}
\trans{\VX} \Omega^{-1}\). Indeed:
#+BEGIN_EXPORT latex
\[ (I-\trans{H})\Omega^{-1}
(I-\trans{H})= \Omega^{-1} - \trans{H}\Omega^{-1} - \Omega^{-1} H +
\trans{H}\Omega^{-1}H = \Omega^{-1} - \trans{H}\Omega^{-1} \]
#+END_EXPORT
and \(MSSE = \frac{SSE}{n-p} = \sigma^2\) with \(p\) being the rank
of \(X\). Using that \(HH=H\) :
#+BEGIN_EXPORT latex
\begin{align*}
SSR &= \trans{\left(\omega^{-1}\VY\right)}\left(\omega^{-1}\VX\left(\trans{\left(\omega^{-1}\VX\right)}\left(\omega^{-1}\VX\right)\right)^{-1} \trans{\left(\omega^{-1}\VX\right)}\right)\left(\omega^{-1}\VY\right) \\
&= \trans{\VY} \Omega^{-1} \VX \left(\trans{\VX}\Omega^{-1}\VX\right)^{-1} \trans{\VX} \Omega^{-1} \VY  \\
&= \trans{\VY} \trans{H} \Omega^{-1} \VY = \trans{\VY} \trans{H}\trans{H} \Omega^{-1} \VY  \\
&= \trans{\VY} \trans{H} \Omega^{-1} H \VY \\
&= \trans{\widehat{\beta}} \trans{X} \Omega^{-1} X \widehat{\beta}  = \trans{\widehat{\beta}} \Sigma^{-1}_{\widehat{\beta}} \widehat{\beta} 
\end{align*}
#+END_EXPORT
where \(\widehat{\beta} = \left(\trans{\VX}\Omega^{-1} \VX
\right)^{-1}\trans{\VX} \Omega^{-1}\VY\) is the GLS estimator of
\(\beta\). So for a single covariate: 
#+BEGIN_EXPORT latex
\[ F=\frac{MSSR}{MSSE}=\frac{\widehat{\beta}\Sigma^{-1}\widehat{\beta}}{\sigma^2} \]
#+END_EXPORT

- 3. :: Defining \(R^2\) as the proportion of variance explained, we get back
#+BEGIN_EXPORT latex
\[ R^2 = \frac{\beta^2}{\beta^2 + df \sigma^2_{\beta} } \]
#+END_EXPORT
where \(df=n-p\). A corresponding correlation coefficient can computed as:
#+BEGIN_EXPORT latex
\[ \rho = \frac{\beta}{\sqrt{\beta^2 + df \sigma^2_{\beta}} } \]
#+END_EXPORT

\clearpage

** Back to the example

In the example, we see a very small marginal correlation and a large conditional one:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.pcor <- partialCor(c(pH,PacO2)~1, repetition = ~time|Subject, data = bland1995, heterogeneous = 0.5)
e.pcor
#+END_SRC

#+RESULTS:
: 		Partial correlation 
: 
:                    estimate    se   df  lower    upper p.value
: rho(1.pH,1.PacO2) -1.63e-05 0.313 1.24 -0.988  0.98791  1.0000
: r(1.pH,1.PacO2)   -5.09e-01 0.125 2.63 -0.806 -0.00546  0.0489
: 	--------------------------------------------------------
: 	rho: marginal correlation 
: 	r  : correlation conditional on the individual 
: 	estimates, standard errors, confidence intervals have been back-transformed (tanh).

This matches the estimate (but not the uncertainty) of another software:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
c(r = rmcorr(Subject, pH, PacO2, bland1995)$r,
  p = rmcorr(Subject, pH, PacO2, bland1995)$p)
#+END_SRC

#+RESULTS:
:             r             p 
: -0.5067697422  0.0008471081

We can also extract the underlying correlation coefficients:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
round(coef(attr(e.pcor,"lmm"), effects = "correlation"),5)
#+END_SRC

#+RESULTS:
:    rho(1.pH,1.PacO2)    rho(1.pH,2.PacO2) rho(1.PacO2,2.PacO2)       rho(1.pH,2.pH) 
:             -0.00002              0.10168              0.66317              0.88129

that reveal a very strong within =pH= correlation (almost 0.9) and a
rather strong within =PacO2= correlation (about 0.65). The
instantaneous correlation is nearly 0 but the lag correlation is about
0.1 leading to the observed conditional correlation.

\bigskip

An alternative approach is to fit a mixed model on only one outcome,
regressing out the other:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.CS <- lmm(pH ~ PacO2, repetition = ~time|Subject, data = bland1995,
            structure = "CS")
#+END_SRC

#+RESULTS:

Then estimate the partial correlation formula:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.CSaov <- anova(e.CS, effects = "PacO2=0")
confint(e.CSaov, columns = c("estimate","se","df","partial.r"))
#+END_SRC

#+RESULTS:
:       estimate     se   df partial.r
: PacO2   -0.103 0.0295 39.6    -0.486

\clearpage

Here approximate degrees of freedom are used, i.e. 39.6 instead of:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
NROW(bland1995)-2
#+END_SRC

#+RESULTS:
: [1] 45

which would lead to a correlation of:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.CSaov$univariate$statistic/sqrt(e.CSaov$univariate$statistic^2+45)
#+END_SRC

#+RESULTS:
: [1] -0.4627676

** Simulation study (compound symmetry model)

We'll compare \(\rho\) and \(r\) in the case of 3 timepoints,
\(r=0.8\), and 250 individuals:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
n.time <- 3
n.id <- 250
Sigma <- matrix(c(1,0.8,0.8,1),2,2)
Sigma
#+END_SRC

#+RESULTS:
:      [,1] [,2]
: [1,]  1.0  0.8
: [2,]  0.8  1.0

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(11)
df.W <- data.frame(id = unlist(lapply(1:n.id, rep, n.time)),
                   time = rep(1:n.time,n.id),
                   rmvnorm(n.time*n.id, mean = c(3,3), sigma = Sigma)
                   )
head(df.W)
#+END_SRC

#+RESULTS:
:   id time       X1       X2
: 1  1    1 2.483259 2.759470
: 2  1    2 1.034157 1.102983
: 3  1    3 3.636308 2.691506
: 4  2    1 4.463341 4.150878
: 5  2    2 2.510048 2.081439
: 6  2    3 2.103239 2.317938

\clearpage

We use random effects to obtain a constant correlation within \(X\)
and within \(Y\):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
sd.id <- 1.5
df.W$X1 <- df.W$X1 + rnorm(n.id, sd = sd.id/4)[df.W$id]
df.W$X2 <- df.W$X2 + rnorm(n.id, sd = sd.id)[df.W$id]
df.W$id <- as.factor(df.W$id)
df.L <- reshape2::melt(df.W, id.vars = c("id","time")) 
df.L$time2 <- as.factor(as.numeric(as.factor(paste(df.L$variable,df.L$time,sep="."))))
#+END_SRC

#+RESULTS:
This will lead to the following correlation structure:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Sigma.GS <- as.matrix(bdiag(Sigma,Sigma,Sigma))[c(1,3,5,2,4,6),c(1,3,5,2,4,6)]
Sigma.GS[1:3,1:3] <- Sigma.GS[1:3,1:3] + (sd.id/4)^2
Sigma.GS[4:6,4:6] <- Sigma.GS[4:6,4:6] + sd.id^2
cov2cor(Sigma.GS)
#+END_SRC

#+RESULTS:
:           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
: [1,] 1.0000000 0.1232877 0.1232877 0.4155056 0.0000000 0.0000000
: [2,] 0.1232877 1.0000000 0.1232877 0.0000000 0.4155056 0.0000000
: [3,] 0.1232877 0.1232877 1.0000000 0.0000000 0.0000000 0.4155056
: [4,] 0.4155056 0.0000000 0.0000000 1.0000000 0.6923077 0.6923077
: [5,] 0.0000000 0.4155056 0.0000000 0.6923077 1.0000000 0.6923077
: [6,] 0.0000000 0.0000000 0.4155056 0.6923077 0.6923077 1.0000000

We can now estimate two types of correlation: marginal and conditional
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.LMMstar <- partialCor(c(X1,X2) ~ 1, repetition = ~ time|id, data = df.W, heterogeneous = 0.5)
e.LMMstar
#+END_SRC

#+RESULTS:
: 		Partial correlation 
: 
:                estimate     se   df lower upper  p.value
: rho(1.X1,1.X2)    0.427 0.0346 34.7 0.356 0.493 6.76e-13
: r(1.X1,1.X2)      0.798 0.0251 58.9 0.764 0.829 0.00e+00
: 	----------------------------------------------------
: 	rho: marginal correlation 
: 	r  : correlation conditional on the individual 
: 	estimates, standard errors, confidence intervals have been back-transformed (tanh).

The conditional coefficient is identical to what other packages output:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
rmcorr:::rmcorr(id, X1, X2, df.W)$r
#+END_SRC

#+RESULTS:
: [1] 0.7983617

Here the modeled correlation matrix is:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Omega <- sigma(attr(e.LMMstar,"lmm"))
Rho <- cov2cor(Omega)
Rho
#+END_SRC

#+RESULTS:
:             1.X1        2.X1        3.X1        1.X2        2.X2        3.X2
: 1.X1  1.00000000  0.06545230  0.06545230  0.42652595 -0.00432106 -0.00432106
: 2.X1  0.06545230  1.00000000  0.06545230 -0.00432106  0.42652595 -0.00432106
: 3.X1  0.06545230  0.06545230  1.00000000 -0.00432106 -0.00432106  0.42652595
: 1.X2  0.42652595 -0.00432106 -0.00432106  1.00000000  0.68836567  0.68836567
: 2.X2 -0.00432106  0.42652595 -0.00432106  0.68836567  1.00000000  0.68836567
: 3.X2 -0.00432106 -0.00432106  0.42652595  0.68836567  0.68836567  1.00000000

From which the conditional correlation can be deduced:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
(Rho[1,4]-Rho[1,5])/sqrt((1-Rho[1,2])*(1-Rho[4,5]))
#+END_SRC

#+RESULTS:
: [1] 0.7983617

or equivalently:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
(Omega[1,4]-Omega[1,5])/sqrt((Omega[1,1]-Omega[1,2])*(Omega[4,4]-Omega[4,5]))
#+END_SRC

#+RESULTS:
: [1] 0.7983617

Replicating this a thousand times:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
n.id <- 100
n.sim <- 1000
n.cpus <- 25 ## run on the server
warper <- function(n){ 
  df.W <- data.frame(id = unlist(lapply(1:n, rep, n.time)),
                     time = rep(1:n.time,n),
                     rmvnorm(n.time*n, mean = c(3,3), sigma = Sigma)
                     )
  df.W$X1 <- df.W$X1 + rnorm(n, sd = sd.id/4)[df.W$id]
  df.W$X2 <- df.W$X2 + rnorm(n, sd = sd.id)[df.W$id]
  df.W$id <- as.factor(df.W$id)

  res1 <- setNames(c(rmcorr(id, X1, X2, df.W)$r, rmcorr(id, X1, X2, df.W)$CI), c("estimate","lower","upper"))
  res2 <- partialCor(c(X1,X2) ~ 1, repetition = ~ time|id, data = df.W, heterogeneous = 0.5)
  return(rbind(cbind(as.data.frame(as.list(res1)), se = NA, method = "rmcorr"),
               cbind(res2[2,c("estimate","lower","upper","se")],method="lmm")))
}

ls.res <- pbapply::pblapply(1:n.sim,function(iSim){
  cbind(sim = iSim, warper(n.id))
}, cl = n.cpus)
dt.res <- as.data.table(do.call(rbind, ls.res))
#+END_SRC

lead to the same estimate for the two implementations:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
range(dt.res[method=="rmcorr",estimate]-dt.res[method=="lmm",estimate], na.rm=TRUE)
#+END_SRC

#+RESULTS:
: [1] -8.572216e-10  2.108167e-09

and lead to a reasonnable coverage:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dt.res[,.(missing = mean(is.na(estimate)), coverage = mean((0.8>=lower)*(0.8<=upper), na.rm=TRUE)), by = "method"]
#+END_SRC

:    method missing coverage
: 1: rmcorr   0.000 0.941000
: 2:    lmm   0.026 0.949692

** Simulation study (crossed random effect model)

We will modify the previous simulation setting by introducing more
structure on the correlation. More precisely, observations will be
correlated within individual (biological variation) and within
timepoint (batch effect). This violates the compound symmetry
structure and therefore we expect =rmcorr= to give biased
estimates. We will use =lmer= instead of =lmm= as a reference since
=lmer= is very convenient to use and fast when dealing with crossed
random effects. Note that, however, it is not straightforward to have a
measure of uncertainty.

#+BEGIN_SRC R :exports code :results output :session *R* :cache no
n.time <- 4
n.id <- 100
warper <- function(n){
  df.W <- data.frame(id = unlist(lapply(1:n.id, rep, n.time)),
                     time = rep(1:n.time,n.id),
                     rmvnorm(n.time*n.id, mean = c(3,3), sigma = Sigma)
                     )
  df.W$X1 <- df.W$X1 + rnorm(n.id, sd = sd.id/4)[df.W$id]
  df.W$X2 <- df.W$X2 + rnorm(n.id, sd = sd.id)[df.W$id]
  df.W$X1 <- df.W$X1 + rnorm(n.time, sd = sd.id/3)[df.W$time]
  df.W$X2 <- df.W$X2 + rnorm(n.time, sd = sd.id/2)[df.W$time]
  df.W$id <- as.factor(df.W$id)
  df.W$time <- as.factor(df.W$time)

  e.lm <- lm(X1~X2+id+time, data = df.W)
  e.Slm <- summary(e.lm)$coef

  e.lmer <- lmer(X2 ~ X1 + (1|time) + (1|id), data = df.W)
  e.Slmer <- summary(e.lmer)$coefficient

  res0 <- c(estimate = e.Slm["X2","t value"]/sqrt(e.Slm["X2","t value"]^2+df.residual(e.lm)), lower = NA, upper = NA)
  res1 <- setNames(c(rmcorr(id, X1, X2, df.W)$r, rmcorr(id, X1, X2, df.W)$CI), c("estimate","lower","upper"))
  res2 <- c(estimate = e.Slmer["X1","t value"]/sqrt(e.Slmer["X1","t value"]^2+e.Slmer["X1","df"]), lower = NA, upper = NA)

  return(rbind(cbind(as.data.frame(as.list(res0)), method = "lm"),
               cbind(as.data.frame(as.list(res1)), method = "rmcorr"),
               cbind(as.data.frame(as.list(res2)), method= "lmer")))
}

ls.res <- pbapply::pblapply(1:101,function(iSim){
  cbind(sim = iSim, warper(100))
})
dt.res <- as.data.table(do.call(rbind, ls.res))
#+END_SRC

#+RESULTS:
:   |                                                  | 0 % ~calculating    |+                                                 | 2 % ~22s            |++                                                | 4 % ~15s            |+++                                               | 6 % ~16s            |++++                                              | 8 % ~14s            |+++++                                             | 10% ~15s            |++++++                                            | 12% ~13s            |+++++++                                           | 14% ~13s            |++++++++                                          | 16% ~12s            |+++++++++                                         | 18% ~12s            |++++++++++                                        | 20% ~12s            |+++++++++++                                       | 22% ~11s            |++++++++++++                                      | 24% ~11s            |+++++++++++++                                     | 25% ~10s            |++++++++++++++                                    | 27% ~10s            |+++++++++++++++                                   | 29% ~10s            |++++++++++++++++                                  | 31% ~10s            |+++++++++++++++++                                 | 33% ~10s            |++++++++++++++++++                                | 35% ~10s            |+++++++++++++++++++                               | 37% ~10s            |++++++++++++++++++++                              | 39% ~10s            |+++++++++++++++++++++                             | 41% ~09s            |++++++++++++++++++++++                            | 43% ~09s            |+++++++++++++++++++++++                           | 45% ~09s            |++++++++++++++++++++++++                          | 47% ~08s            |+++++++++++++++++++++++++                         | 49% ~08s            |++++++++++++++++++++++++++                        | 51% ~07s            |+++++++++++++++++++++++++++                       | 53% ~07s            |++++++++++++++++++++++++++++                      | 55% ~07s            |+++++++++++++++++++++++++++++                     | 57% ~06s            |++++++++++++++++++++++++++++++                    | 59% ~06s            |+++++++++++++++++++++++++++++++                   | 61% ~06s            |++++++++++++++++++++++++++++++++                  | 63% ~05s            |+++++++++++++++++++++++++++++++++                 | 65% ~05s            |++++++++++++++++++++++++++++++++++                | 67% ~05s            |+++++++++++++++++++++++++++++++++++               | 69% ~05s            |++++++++++++++++++++++++++++++++++++              | 71% ~04s            |+++++++++++++++++++++++++++++++++++++             | 73% ~04s            |++++++++++++++++++++++++++++++++++++++            | 75% ~04s            |+++++++++++++++++++++++++++++++++++++++           | 76% ~04s            |++++++++++++++++++++++++++++++++++++++++          | 78% ~03s            |+++++++++++++++++++++++++++++++++++++++++         | 80% ~03s            |++++++++++++++++++++++++++++++++++++++++++        | 82% ~03s            |+++++++++++++++++++++++++++++++++++++++++++       | 84% ~02s            |++++++++++++++++++++++++++++++++++++++++++++      | 86% ~02s            |+++++++++++++++++++++++++++++++++++++++++++++     | 88% ~02s            |++++++++++++++++++++++++++++++++++++++++++++++    | 90% ~02s            |+++++++++++++++++++++++++++++++++++++++++++++++   | 92% ~01s            |++++++++++++++++++++++++++++++++++++++++++++++++  | 94% ~01s            |+++++++++++++++++++++++++++++++++++++++++++++++++ | 96% ~01s            |++++++++++++++++++++++++++++++++++++++++++++++++++| 98% ~00s            |++++++++++++++++++++++++++++++++++++++++++++++++++| 100% elapsed=16s  
: Advarselsbeskeder:
: 1: I checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
:   Model failed to converge with max|grad| = 0.00242359 (tol = 0.002, component 1)
: 2: I checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
:   Model failed to converge with max|grad| = 0.00331164 (tol = 0.002, component 1)

We can clearly see that the =rmcorr= estimator is biased and very
variable while the =lmer=-based estimator (i.e. using
autoref:eq:pCor-formula) gives reasonnable results:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
rbind(lm = quantile(dt.res[method=="lm",estimate]),
      rmcorr = quantile(dt.res[method=="rmcorr",estimate]),
      lmer = quantile(dt.res[method=="lmer",estimate]))
#+END_SRC

#+RESULTS:
:                0%       25%       50%       75%      100%
: lm     0.74113022 0.7802503 0.7989544 0.8145657 0.8544672
: rmcorr 0.05260171 0.4773873 0.6189341 0.7273679 0.8425171
: lmer   0.73416614 0.7739002 0.7937885 0.8103669 0.8529145

Note that the linear regression approach can be fixed in that example
by adjusting on time. However with more complex covariance pattern it
may not always be possible to find an appropriate =lm= approach.

* Reference
# # help: https://gking.harvard.edu/files/natnotes2.pdf

#+BEGIN_EXPORT latex
\begingroup
\renewcommand{\section}[2]{}
#+END_EXPORT
bibliographystyle:apalike
[[bibliography:bibliography.bib]] 
#+BEGIN_EXPORT latex
\endgroup
#+END_EXPORT

#+BEGIN_EXPORT LaTeX
\appendix \titleformat{\section}
{\normalfont\Large\bfseries}{}{1em}{Appendix~\thesection:~}

\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\theequation}{\Alph{equation}}

\setcounter{figure}{0}    
\setcounter{table}{0}    
\setcounter{equation}{0}    
#+END_EXPORT

* CONFIG :noexport:
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+LaTeX_CLASS_OPTIONS: [12pt]
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Display of the document
# ## space between lines
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}

# ## margins
#+LATEX_HEADER:\geometry{top=1cm}

# ## personalize the prefix in the name of the sections
#+LaTeX_HEADER: \usepackage{titlesec}
# ## fix bug in titlesec version
# ##  https://tex.stackexchange.com/questions/299969/titlesec-loss-of-section-numbering-with-the-new-update-2016-03-15
#+LaTeX_HEADER: \usepackage{etoolbox}
#+LaTeX_HEADER: 
#+LaTeX_HEADER: \makeatletter
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\noindent}{}{}{}
#+LaTeX_HEADER: \makeatother

** Color
# ## define new colors
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LaTeX_HEADER: \definecolor{myorange}{rgb}{1,0.2,0}
#+LaTeX_HEADER: \definecolor{mypurple}{rgb}{0.7,0,8}
#+LaTeX_HEADER: \definecolor{mycyan}{rgb}{0,0.6,0.6}
#+LaTeX_HEADER: \newcommand{\lightblue}{blue!50!white}
#+LaTeX_HEADER: \newcommand{\darkblue}{blue!80!black}
#+LaTeX_HEADER: \newcommand{\darkgreen}{green!50!black}
#+LaTeX_HEADER: \newcommand{\darkred}{red!50!black}
#+LaTeX_HEADER: \definecolor{gray}{gray}{0.5}

# ## change the color of the links
#+LaTeX_HEADER: \hypersetup{
#+LaTeX_HEADER:  citecolor=[rgb]{0,0.5,0},
#+LaTeX_HEADER:  urlcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER:  linkcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER: }

** Font
# https://tex.stackexchange.com/questions/25249/how-do-i-use-a-particular-font-for-a-small-section-of-text-in-my-document
#+LaTeX_HEADER: \newenvironment{note}{\small \color{gray}\fontfamily{lmtt}\selectfont}{\par}
#+LaTeX_HEADER: \newenvironment{activity}{\color{orange}\fontfamily{qzc}\selectfont}{\par}

** Symbols
# ## valid and cross symbols
#+LaTeX_HEADER: \RequirePackage{pifont}
#+LaTeX_HEADER: \RequirePackage{relsize}
#+LaTeX_HEADER: \newcommand{\Cross}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{56}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\Valid}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{52}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\CrossR}{ \textcolor{red}{\Cross} }
#+LaTeX_HEADER: \newcommand{\ValidV}{ \textcolor{green}{\Valid} }

# ## warning symbol
#+LaTeX_HEADER: \usepackage{stackengine}
#+LaTeX_HEADER: \usepackage{scalerel}
#+LaTeX_HEADER: \newcommand\Warning[1][3ex]{%
#+LaTeX_HEADER:   \renewcommand\stacktype{L}%
#+LaTeX_HEADER:   \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}%
#+LaTeX_HEADER:   \xspace
#+LaTeX_HEADER: }

# # R Software
#+LATEX_HEADER: \newcommand\Rlogo{\textbf{\textsf{R}}\xspace} % 

** Code
# Documentation at https://org-babel.readthedocs.io/en/latest/header-args/#results
# :tangle (yes/no/filename) extract source code with org-babel-tangle-file, see http://orgmode.org/manual/Extracting-source-code.html 
# :cache (yes/no)
# :eval (yes/no/never)
# :results (value/output/silent/graphics/raw/latex)
# :export (code/results/none/both)
#+PROPERTY: header-args :session *R* :tangle yes :cache no ## extra argument need to be on the same line as :session *R*

# Code display:
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

# ## change font size input (global change)
# ## doc: https://ctan.math.illinois.edu/macros/latex/contrib/listings/listings.pdf
# #+LATEX_HEADER: \newskip\skipamount   \skipamount =6pt plus 0pt minus 6pt
# #+LATEX_HEADER: \lstdefinestyle{code-tiny}{basicstyle=\ttfamily\tiny, aboveskip =  kipamount, belowskip =  kipamount}
# #+LATEX_HEADER: \lstset{style=code-tiny}
# ## change font size input (local change, put just before BEGIN_SRC)
# ## #+ATTR_LATEX: :options basicstyle=\ttfamily\scriptsize
# ## change font size output (global change)
# ## \RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\tiny,formatcom = {\color[rgb]{0.5,0,0}}}

** Lists
#+LATEX_HEADER: \RequirePackage{enumitem} % better than enumerate

** Image and graphs
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of} % 
#+LATEX_HEADER: \RequirePackage{caption} % newlines in graphics

#+LaTeX_HEADER: \RequirePackage{tikz-cd} % graph
# ## https://tools.ietf.org/doc/texlive-doc/latex/tikz-cd/tikz-cd-doc.pdf

** Table
#+LATEX_HEADER: \RequirePackage{booktabs} % for nice lines in table (e.g. toprule, bottomrule, midrule, cmidrule)

** Inline latex
# @@latex:any arbitrary LaTeX code@@


** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
# #+LaTeX_HEADER: \RequirePackage{amsthm}
# #+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
# #+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \usepackage{ifthen}
#+LATEX_HEADER: \usepackage{xifthen}
#+LATEX_HEADER: \usepackage{xargs}
#+LATEX_HEADER: \usepackage{xspace}

#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

**** Probability
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\logLikelihood[2][1=,2=]{\defOperator{#1}{#2}{\ell}{}{(}{)}{}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\partial #2^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}

#+LATEX_HEADER: \newcommand\Veta{\boldsymbol{\eta}}
#+LATEX_HEADER: \newcommand\sample{\chi}
#+LATEX_HEADER: \newcommand\Hspace{\mathcal{H}}
#+LATEX_HEADER: \newcommand\Tspace{\mathcal{T}}


** Notations
#+LATEX_HEADER: \newcommand\VY{\mathbf{Y}}
#+LATEX_HEADER: \newcommand\VX{\mathbf{X}}
#+LATEX_HEADER: \newcommand\Vvarepsilon{\boldsymbol{\varepsilon}}
