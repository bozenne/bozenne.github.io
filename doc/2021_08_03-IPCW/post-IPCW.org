#+TITLE: Inverse probability of censoring weighting (IPCW) for linear regression
#+Author: Brice Ozenne

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
path <- "c:/Users/hpl802/Documents/Github/bozenne.github.io/doc/2021_08_03-IPCW/"
setwd(path)
#+END_SRC

#+RESULTS:


* Principle

Inverse probability of censoring weighting (IPCW) is a method able to
handle informative drop-out. Intuitively, in presence of informative
drop-out a complete case analysis is a biased approach as individuals
with complete data are not representative of the population. However
with an appropriate re-weighting of the individuals with complete
data, we can "re-balance" our sample and make it representative of the
population. To do so, we divide the population into sub-populations
and attribute weights to individuals who did not drop-out inversely
proportional to the frequency of the drop-out in the
sub-population. Thanks to the weights, individuals who did not
drop-out "represent" the individuals who dropped-out. Thus, overall,
the weighted sample is representative of the population.

* Continuous outcome

** Illustrative example

Consider a study were we follow depressed individual over time. They
have a baseline measurement, then are given a treatment, and then have
a follow-up measurement. We would like to assess the treatment effect
in term of depression score [fn:::To simplfy, there is no control
group - we assume that without treatment the depression score would be
constant.]. The population of interest contain severely and moderately
depressed individuals; the treatment may work differently in each
sub-population. Unfortunately, some study participants dropped-out and
it seems that they are more likely to drop-out when they are severaey
depressed.

\clearpage

We can simulate such a dataset using the following function:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
simTrial <- function(n, rho, dmu, pC){
  require(mvtnorm)
  require(data.table)
  ## simulate data
  Sigma <- 10^2*matrix(c(1,rho,rho,1),2,2)
  ## gather into dataset
  M.Ym <- rmvnorm(n, mean = c(50, 50-dmu[1]), sigma = Sigma)
  M.Ys <- rmvnorm(n, mean = c(75, 75-dmu[2]), sigma = Sigma)
  dtL <- rbind(
    data.table(id = 1:n, mdd = "moderate", time = "T1", Y = M.Ym[,1]),
    data.table(id = 1:n, mdd = "moderate", time = "T2", Y = M.Ym[,2]),
    data.table(id = n+(1:n), mdd = "severe", time = "T1", Y = M.Ys[,1]),
    data.table(id = n+(1:n), mdd = "severe", time = "T2", Y = M.Ys[,2])
  )
  dtL$probaDO <- 0
  dtL[time=="T2", probaDO := ifelse(.SD$mdd=="moderate",pC[1],pC[2])]
  dtL[,dropout := rbinom(.N,prob=probaDO,size=1)]
  dtL[,Yobs:=Y]
  dtL[dropout==1,Yobs:=NA]
  dtL$probaDO <- NULL
  return(dtL)
}
set.seed(11)
dtL <- simTrial(n = 1000, rho = 0.8, dmu = c(25,50), pC = c(0.2,0.7))
print(dtL)
#+END_SRC

#+RESULTS:
#+begin_example
        id      mdd time        Y dropout     Yobs
   1:    1 moderate   T1 44.83259       0 44.83259
   2:    2 moderate   T1 30.34157       0 30.34157
   3:    3 moderate   T1 56.36308       0 56.36308
   4:    4 moderate   T1 64.63341       0 64.63341
   5:    5 moderate   T1 45.10048       0 45.10048
  ---                                             
3996: 1996   severe   T2 30.59793       1       NA
3997: 1997   severe   T2 18.97725       1       NA
3998: 1998   severe   T2 29.80266       1       NA
3999: 1999   severe   T2 30.26518       0 30.26518
4000: 2000   severe   T2 39.15797       0 39.15797
#+end_example

Here we have simulated a two sub-populations of 1000, with a
correlation of 0.5 between baseline and follow-up . The treatment
effect is twice bigger for the severely depressed population but
individuals from this population are also much more likely to
drop-out. Overall the expected treatment effect is:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
(-25-50)/2
#+END_SRC

#+RESULTS:
: [1] -37.5

\bigskip

Without drop-out, we could use a simple linear model to carry-out the analysis:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtW.oracle <- dcast(dtL, formula = id ~ time, value.var = "Y")
dtW.oracle$diff <- dtW.oracle$T2-dtW.oracle$T1
e.oracle <- lm(diff~1, data = dtW.oracle)
summary(e.oracle)$coef
#+END_SRC

#+RESULTS:
:              Estimate Std. Error   t value Pr(>|t|)
: (Intercept) -37.35098  0.3141814 -118.8835        0
leading to an estimate quite close to the true value.

\bigskip

With drop-out, a complete case analysis would lead to a biased
estimator. In this example, we can "see" that the estimated value is
far away from the true one (even when accouting for the uncertainty):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtW <- dcast(dtL, formula = id + mdd ~ time, value.var = "Yobs")
dtW$diff <- dtW$T2-dtW$T1
dtW.CC <- dtW[!is.na(diff)]
e.CC <- lm(diff~1, data = dtW.CC)
summary(e.CC)$coef
#+END_SRC

#+RESULTS:
:              Estimate Std. Error   t value Pr(>|t|)
: (Intercept) -31.42356  0.3909029 -80.38713        0

An alternative approach would be to use a linear mixed model
(i.e. full information):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
require(nlme)
e.FI <- lme(Yobs~time, random = ~1|id, data = dtL, na.action = na.omit)
summary(e.FI)$tTable
#+END_SRC

#+RESULTS:
:                 Value Std.Error   DF   t-value p-value
: (Intercept)  62.59128 0.3239587 1999 193.20760       0
: timeT2      -33.76472 0.3855964 1068 -87.56494       0
which is better than the complete case analysis still biased when the
drop-out mechanism depends on variables other than the baseline value.

\clearpage

 A better approach is to use IPCW. First we model the probability of
not dropping out at follow-up:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtW$observed <- !is.na(dtW$T2)
e.glmW.oracle <- glm(observed ~ mdd, data = dtW,
                     family = binomial(link = "logit"))
#+END_SRC

#+RESULTS:
and then compute the weights for observations with full data:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtW$weight.oracle <- 1/predict(e.glmW.oracle, newdata = dtW,type = "response")
dtW[observed == TRUE, sum(weight.oracle)]
#+END_SRC

#+RESULTS:
: [1] 2000

Note that the weights sum to the total sample size. We then perform
the complete case analysis with these weights:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtW.CC <- dtW[!is.na(diff)]
e.IPCWoracle <- lm(diff~1, data = dtW.CC, weights = dtW.CC$weight.oracle)
summary(e.IPCWoracle)$coef
#+END_SRC

#+RESULTS:
:              Estimate Std. Error  t value Pr(>|t|)
: (Intercept) -36.89889  0.4251421 -86.7919        0

which gives a result much closer to the true value. A more feasible
IPCW would use the baseline score to define the weights:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.glmW <- glm(observed ~ T1, data = dtW,
              family = binomial(link = "logit"))
dtW$weight <- 1/predict(e.glmW, newdata = dtW, type = "response")
dtW[observed == TRUE, sum(weight)]
#+END_SRC

#+RESULTS:
: [1] 2015.739

We then perform the complete case analysis with these new weights:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtW.CC <- dtW[!is.na(diff)]
e.IPCW <- lm(diff~1, data = dtW.CC, weights = dtW.CC$weight)
summary(e.IPCW)$coef
#+END_SRC

#+RESULTS:
:              Estimate Std. Error   t value Pr(>|t|)
: (Intercept) -35.47206   0.423423 -83.77453        0

\clearpage

** Simulation study

The quality of the previous estimators is compared using a simulation
study. The results are summarized by autoref:fig:simulationGaussian.

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
warper <- function(n, rho, dmu, pC){

  ## *** simulate data
  dtL <- simTrial(n = n, rho = rho, dmu = dmu, pC = pC)
  
  ## *** rehape data
  dtW <- dcast(dtL, formula = id + mdd ~ time, value.var = "Yobs")
  dtW$diff <- dtW$T2-dtW$T1
  dtW$observed <- 1-is.na(dtW$T2)

  dtW.oracle <- dcast(dtL, formula = id ~ time, value.var = "Y")
  dtW.oracle$diff <- dtW.oracle$T2-dtW.oracle$T1

  ## *** oracle
  e.lmOracle <- lm(diff~1, data = dtW.oracle)

  ## *** naive and biased analysis
  e.lmNaive <- lm(diff~1, data = dtW)

  ## *** mixed model
  e.lme <- lme(Yobs~time, random = ~1|id, data = dtL, na.action = na.omit)

  ## *** IPCW with oracle weights
  e.glmW.oracle <- glm(observed ~ mdd, data = dtW, family = binomial(link = "logit"))
  dtW$weight.oracle <- 1/predict(e.glmW.oracle, newdata = dtW, type = "response")
  e.lmIPCW.oracle <- lm(diff~1, data = dtW[observed == 1], weights = dtW[observed == 1,weight.oracle])

  ## *** IPCW with feasible weights
  e.glmW <- glm(observed ~ T1, data = dtW, family = binomial(link = "logit"))
  dtW$weight <- 1/predict(e.glmW, newdata = dtW, type = "response")
  e.lmIPCW <- lm(diff~1, data = dtW[observed == 1], weights = dtW[observed == 1,weight])

  ## *** export
  res.oracle <- setNames(summary(e.lmOracle)$coef[1,], c("estimate","se","statistic","p.value"))
  res.naive <- setNames(summary(e.lmNaive)$coef[1,], c("estimate","se","statistic","p.value"))
  res.lme <- setNames(summary(e.lme)$tTable[2,c(1:2,4:5)], c("estimate","se","statistic","p.value"))
  res.IPCW.oracle <- setNames(summary(e.lmIPCW.oracle)$coef[1,], c("estimate","se","statistic","p.value"))
  res.IPCW <- setNames(summary(e.lmIPCW)$coef[1,], c("estimate","se","statistic","p.value"))

  out <- rbind(cbind(model = "oracle", rho = rho, n = n, dmu = diff(dmu), as.data.frame(as.list(res.oracle))),
               cbind(model = "complete case", rho = rho, n = n, dmu = diff(dmu), as.data.frame(as.list(res.naive))),
               cbind(model = "full information", rho = rho, n = n, dmu = diff(dmu), as.data.frame(as.list(res.lme))),
               cbind(model = "IPCW.oracle", rho = rho, n = n, dmu = diff(dmu), as.data.frame(as.list(res.IPCW.oracle))),
               cbind(model = "IPCW", rho = rho, n = n, dmu = diff(dmu), as.data.frame(as.list(res.IPCW))))
  return(out)
}
#+END_SRC

#+RESULTS:


#+BEGIN_SRC R :exports none :results output :session *R* :cache no
# Sanity check
set.seed(11)
warper(n = 1000, rho = 0.8, dmu = c(25,50), pC = c(0.2,0.7))
#+END_SRC

#+RESULTS:
:              model rho    n dmu  estimate        se  statistic p.value
: 1           oracle 0.8 1000  25 -37.35098 0.3141814 -118.88350       0
: 2    complete case 0.8 1000  25 -31.42356 0.3909029  -80.38713       0
: 3 full information 0.8 1000  25 -33.76472 0.3855964  -87.56494       0
: 4      IPCW.oracle 0.8 1000  25 -36.89889 0.4251421  -86.79190       0
: 5             IPCW 0.8 1000  25 -35.47206 0.4234230  -83.77453       0

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
n.sim <- 100
ls.res <- lapply(1:n.sim, function(iSim){
  rbind(warper(n = 1000, rho = 0, dmu = c(25,50), pC = c(0.2,0.7)),
        warper(n = 1000, rho = 0.5, dmu = c(25,50), pC = c(0.2,0.7)),
        warper(n = 1000, rho = 0.8, dmu = c(25,50), pC = c(0.2,0.7)))
})
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
  library(ggplot2)
  library(data.table)
  dt.res <- as.data.table(do.call(rbind,ls.res))
  dt.res[, estimator := factor(model, c("complete case","full information","IPCW","IPCW.oracle","oracle"))]
  dt.res[, correlation := paste0("correlation = ", rho)]

  gg <- ggplot(dt.res, aes(y = estimate))
  gg <- gg + geom_boxplot(aes(fill=estimator))
  gg <- gg + facet_wrap(~correlation)
  gg <- gg + theme(axis.title.x=element_blank(),
                   axis.text.x=element_blank(),
                   axis.ticks.x=element_blank())
  gg <- gg + theme(text = element_text(size=15),
                   axis.line = element_line(size = 1.25),
                   axis.ticks = element_line(size = 2),
                   axis.ticks.length=unit(.25, "cm"),
                   legend.position="bottom",
                   legend.direction = "horizontal")
  ggsave(gg, filename = "./figures/simStudy-bias.pdf", width = 10)
#+END_SRC

#+RESULTS:
: Saving 10 x 7 in image

#+name: fig:simulationGaussian
#+ATTR_LaTeX: :width \textwidth :placement [!h]
#+CAPTION: Comparison between the empirical distributions of the estimators (Gaussian case) for a sample size of 1000 using 100 datasets.
[[./figures/simStudy-bias.pdf]]

\bigskip

* Binary outcome

** Illustrative example
A somehow similar approach can be used for binary endpoints. Consider
now a study comparing the survival probability at 1 year of patients
treated with a new drug vs. standard care. The population is composed
of two types of patients, say some with hypertension and some
without. Survival as well as the treatment effect may differ depending
of the hypertension status. Hypertension may also affect the drop-out
probability.

\clearpage

We can simulate such a dataset using the following function:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
simTrial <- function(n, dmu, dpC){
  require(BuyseTest)
  require(data.table)
  ## simulate data
  dt1  <- simBuyseTest(n.T = n, n.C = n, 
                       argsBin = NULL, argsCont = NULL, 
                       argsTTE = list(scale.T = 1+dmu[1],
                                      scale.C = 1,
                                      scale.Censoring.T = 1+dpC[1],
                                      scale.Censoring.C = 1),
                       latent = TRUE)
  dt2  <- simBuyseTest(n.T = n, n.C = n, 
                       argsBin = NULL, argsCont = NULL, 
                       argsTTE = list(scale.T = 2+dmu[2],
                                      scale.C = 2,
                                      scale.Censoring.T = 2+dpC[2],
                                      scale.Censoring.C = 2),
                       latent = TRUE)
  ## gather into dataset
  dt <- rbind(
    cbind(id = 1:NROW(dt1), group = "G1", dt1),
    cbind(id = NROW(dt1) + 1:NROW(dt2), group = "G2", dt2)
  )
  return(dt)
}
set.seed(11)
tau <- 1

dt <- simTrial(n = 10000, dmu = c(0,1), dpC = c(0,1))
dt$responseUncensored <- dt$eventtimeUncensored<=tau
dt$response <- ifelse((dt$status==1)+(dt$eventtime>tau),dt$eventtime<=tau,NA)
dt$observed <- ifelse((dt$status==1)+(dt$eventtime>tau),1,0)
print(dt)
#+END_SRC

#+RESULTS:
#+begin_example
          id group treatment eventtimeUncensored eventtimeCensoring  eventtime
    1:     1    G1         C          1.63238841         1.74283865 1.63238841
    2:     2    G1         C          0.08938341         1.10407172 0.08938341
    3:     3    G1         C          1.54194414         1.11212966 1.11212966
    4:     4    G1         C          1.04592013         1.00584279 1.00584279
    5:     5    G1         C          0.55276522         0.04955419 0.04955419
   ---                                                                        
39996: 39996    G2         T          4.08919433         1.23091105 1.23091105
39997: 39997    G2         T          3.59736307         8.14939225 3.59736307
39998: 39998    G2         T          7.21110232         3.04114191 3.04114191
39999: 39999    G2         T          0.06096057         0.42120185 0.06096057
40000: 40000    G2         T          6.89615584         2.22637070 2.22637070
       status responseUncensored response observed
    1:      1              FALSE    FALSE        1
    2:      1               TRUE     TRUE        1
    3:      0              FALSE    FALSE        1
    4:      0              FALSE    FALSE        1
    5:      0               TRUE       NA        0
   ---                                            
39996:      0              FALSE    FALSE        1
39997:      1              FALSE    FALSE        1
39998:      0              FALSE    FALSE        1
39999:      1               TRUE     TRUE        1
40000:      0              FALSE    FALSE        1
#+end_example

In absence of drop-out, we can compare the survival
probabilities at 1 year using a logistic regression:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.oracle <- glm(responseUncensored ~ treatment,
                data = dt, family = binomial(link="logit"))
summary(e.oracle)$coef
#+END_SRC

#+RESULTS:
:                Estimate Std. Error   z value     Pr(>|z|)
: (Intercept)  0.07022885 0.01415085   4.96287 6.945906e-07
: treatmentT  -0.23721580 0.02004104 -11.83650 2.527721e-32

In presence of (differential) drop-out, a complete case analysis
(i.e. restricting the analysis to the patients where the survival
status at 1 year is known) would be biased:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dt.cc <- dt[dt$observed==1]
e.cc <- glm(response ~ treatment,
            data = dt.cc, family = binomial(link="logit"))
summary(e.cc)$coef
#+END_SRC

#+RESULTS:
:               Estimate Std. Error   z value      Pr(>|z|)
: (Intercept)  0.4023018 0.01814959  22.16589 7.330653e-109
: treatmentT  -0.3656280 0.02510139 -14.56605  4.618541e-48

A first idea would be to re-use the IPCW approach, first fitting a
logistic model for the probability of being observed at 1-year and
then computing the weights:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.IPCmodel <- glm(observed ~ group*treatment, data = dt, family = binomial(link="logit"))
dt$IPCweights <- 1/predict(e.IPCmodel, newdata = dt, type = "response")
sum(dt$IPCweights)
#+END_SRC

#+RESULTS:
: [1] 62570.28

The subsequent estimator will not be correct: 
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dt.cc <- dt[dt$observed==1]
e.IPCWcc <- glm(response ~ treatment, data = dt.cc,
                family = binomial(link="logit"), weights = dt.cc$IPCweights)
summary(e.IPCWcc)$coef
#+END_SRC

#+RESULTS:
: Advarselsbesked:
: I eval(family$initialize) : non-integer #successes in a binomial glm!
:               Estimate Std. Error   z value      Pr(>|z|)
: (Intercept)  0.4593548 0.01451679  31.64300 9.465106e-220
: treatmentT  -0.2997806 0.02029810 -14.76889  2.324953e-49

as we disregarded the duration of observation among the censored
individuals. Intuitively, individuals censored early are more at risk
of dying and therefore should "transfer" more weight than those
censored late, e.g. just before 1 year, who don't really need to
transfer weights. This can be perform using a survival model (here a
Cox model) and using as weights the inverse of the probability of not
being censored at the earliest between when the event occured and 1
year:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(survival)
library(riskRegression)
e.IPCmodel2 <- coxph(Surv(eventtime,status==0) ~ group*treatment,
                     data = dt, x = TRUE, y = TRUE)
iPred <- predictCox(e.IPCmodel2, newdata = dt,
                    time = pmin(dt$eventtime,tau)-(1e-12), diag = TRUE)$survival
dt$IPCweights2 <- dt$observed/iPred
sum(dt$IPCweights2)
#+END_SRC

#+RESULTS:
: [1] 40028.88

We can then use the weights in a logistic model:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dt.cc <- dt[dt$observed==1]
e.IPCWcc <- glm(response ~ treatment, data = dt.cc, family = binomial(link="logit"), weights = dt.cc$IPCweights2)
summary(e.IPCWcc)$coef
#+END_SRC

#+RESULTS:
: Advarselsbesked:
: I eval(family$initialize) : non-integer #successes in a binomial glm!
:                Estimate Std. Error    z value     Pr(>|z|)
: (Intercept)  0.06439848 0.01414818   4.551716 5.321009e-06
: treatmentT  -0.23858152 0.02003591 -11.907693 1.079216e-32

which is very close to the true value. Note that this estimator is
implemented in the riskRegression package:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.wglm <- wglm(regressor.event = ~treatment,
               formula.censor = Surv(eventtime,status==0)~group*treatment,
               times = 1,
               data = dt[,.(eventtime,status,group,treatment)])
summary(e.wglm)
#+END_SRC

#+RESULTS:
#+begin_example
     IPCW logistic regression : 
----------------------------------------------------------------------------------
  - time: 1
glm(XX_status.1_XX ~ treatment, family = binomial(link = "logit"), 
    weights = "XX_IPCW.1_XX")

               Estimate Std. Error   z value     Pr(>|z|)
(Intercept)  0.06439847 0.01931759  3.333670 0.0008570818
treatmentT  -0.23858152 0.02652596 -8.994264 0.0000000000
----------------------------------------------------------------------------------
#+end_example

Another, similar, weighted estimator is implemented in the mets package:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.mets <- binreg(formula = Event(eventtime,status) ~ treatment,
                 cens.model = ~group*treatment,
                 time = 1, data = dt, cens.code = 0, cause = 1)
e.mets
#+END_SRC

#+RESULTS:
#+begin_example

     n events
 40000  14351

 40000 clusters
log-coeffients:
             Estimate   Std.Err      2.5%     97.5% P-value
(Intercept)  0.064776  0.019716  0.026134  0.103418   0.001
treatmentT  -0.236638  0.027400 -0.290341 -0.182936   0.000

exp(coeffients):
              Estimate  Std.Err     2.5%    97.5% P-value
[(Intercept)] 1.066920 0.021035 1.025692 1.108148  0.0015
[treatmentT]  0.789277 0.021626 0.746890 0.831663  0.0000
#+end_example


** Simulation study

The quality of the previous estimators is compared using a simulation
study. The results are summarized by autoref:fig:simulationBinary and autoref:fig:simulationBinarySD 
#+name: fig:simulationBinary
#+ATTR_LaTeX: :width \textwidth :placement [!h]
#+CAPTION: Comparison between the empirical distributions of the estimators (binary case) 
#+CAPTION: across sample size. Based on 1000 replicates.
[[./figures/simStudy-bin-bias.pdf]]

#+name: fig:simulationBinarySD
#+ATTR_LaTeX: :width \textwidth :placement [!h]
#+CAPTION: Comparison between the modeled standard errors of the estimates (boxplot) and the empirical ones (triangles linked by a line)
#+CAPTION: across sample size. Based on 1000 replicates.
[[./figures/simStudy-bin-sd.pdf]]

* Reference :noexport:
# help: https://gking.harvard.edu/files/natnotes2.pdf

#+BEGIN_EXPORT latex
\begingroup
\renewcommand{\section}[2]{}
#+END_EXPORT
bibliographystyle:apalike
[[bibliography:bibliography.bib]] 
#+BEGIN_EXPORT latex
\endgroup
#+END_EXPORT

#+BEGIN_EXPORT LaTeX
\appendix
\titleformat{\section}
{\normalfont\Large\bfseries}{}{1em}{Appendix~\thesection:~}

\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\theequation}{\Alph{equation}}

\setcounter{figure}{0}    
\setcounter{table}{0}    
\setcounter{equation}{0}    

\setcounter{page}{1}
#+END_EXPORT

* CONFIG :noexport:
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+LaTeX_CLASS_OPTIONS: [12pt]
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Display of the document
# ## space between lines
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}

# ## margins
#+LATEX_HEADER:\geometry{top=1cm}

# ## personalize the prefix in the name of the sections
#+LaTeX_HEADER: \usepackage{titlesec}
# ## fix bug in titlesec version
# ##  https://tex.stackexchange.com/questions/299969/titlesec-loss-of-section-numbering-with-the-new-update-2016-03-15
#+LaTeX_HEADER: \usepackage{etoolbox}
#+LaTeX_HEADER: 
#+LaTeX_HEADER: \makeatletter
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\noindent}{}{}{}
#+LaTeX_HEADER: \makeatother

** Color
# ## define new colors
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LaTeX_HEADER: \definecolor{myorange}{rgb}{1,0.2,0}
#+LaTeX_HEADER: \definecolor{mypurple}{rgb}{0.7,0,8}
#+LaTeX_HEADER: \definecolor{mycyan}{rgb}{0,0.6,0.6}
#+LaTeX_HEADER: \newcommand{\lightblue}{blue!50!white}
#+LaTeX_HEADER: \newcommand{\darkblue}{blue!80!black}
#+LaTeX_HEADER: \newcommand{\darkgreen}{green!50!black}
#+LaTeX_HEADER: \newcommand{\darkred}{red!50!black}
#+LaTeX_HEADER: \definecolor{gray}{gray}{0.5}

# ## change the color of the links
#+LaTeX_HEADER: \hypersetup{
#+LaTeX_HEADER:  citecolor=[rgb]{0,0.5,0},
#+LaTeX_HEADER:  urlcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER:  linkcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER: }

** Font
# https://tex.stackexchange.com/questions/25249/how-do-i-use-a-particular-font-for-a-small-section-of-text-in-my-document
#+LaTeX_HEADER: \newenvironment{note}{\small \color{gray}\fontfamily{lmtt}\selectfont}{\par}
#+LaTeX_HEADER: \newenvironment{activity}{\color{orange}\fontfamily{qzc}\selectfont}{\par}

** Symbols
# ## valid and cross symbols
#+LaTeX_HEADER: \RequirePackage{pifont}
#+LaTeX_HEADER: \RequirePackage{relsize}
#+LaTeX_HEADER: \newcommand{\Cross}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{56}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\Valid}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{52}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\CrossR}{ \textcolor{red}{\Cross} }
#+LaTeX_HEADER: \newcommand{\ValidV}{ \textcolor{green}{\Valid} }

# ## warning symbol
#+LaTeX_HEADER: \usepackage{stackengine}
#+LaTeX_HEADER: \usepackage{scalerel}
#+LaTeX_HEADER: \newcommand\Warning[1][3ex]{%
#+LaTeX_HEADER:   \renewcommand\stacktype{L}%
#+LaTeX_HEADER:   \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}%
#+LaTeX_HEADER:   \xspace
#+LaTeX_HEADER: }

# # R Software
#+LATEX_HEADER: \newcommand\Rlogo{\textbf{\textsf{R}}\xspace} % 

** Code
# Documentation at https://org-babel.readthedocs.io/en/latest/header-args/#results
# :tangle (yes/no/filename) extract source code with org-babel-tangle-file, see http://orgmode.org/manual/Extracting-source-code.html 
# :cache (yes/no)
# :eval (yes/no/never)
# :results (value/output/silent/graphics/raw/latex)
# :export (code/results/none/both)
#+PROPERTY: header-args :session *R* :tangle yes :cache no ## extra argument need to be on the same line as :session *R*

# Code display:
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

# ## change font size input (global change)
# ## doc: https://ctan.math.illinois.edu/macros/latex/contrib/listings/listings.pdf
# #+LATEX_HEADER: \newskip\skipamount   \skipamount =6pt plus 0pt minus 6pt
# #+LATEX_HEADER: \lstdefinestyle{code-tiny}{basicstyle=\ttfamily\tiny, aboveskip =  kipamount, belowskip =  kipamount}
# #+LATEX_HEADER: \lstset{style=code-tiny}
# ## change font size input (local change, put just before BEGIN_SRC)
# ## #+ATTR_LATEX: :options basicstyle=\ttfamily\scriptsize
# ## change font size output (global change)
# ## \RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\tiny,formatcom = {\color[rgb]{0.5,0,0}}}

** Lists
#+LATEX_HEADER: \RequirePackage{enumitem} % better than enumerate

** Image and graphs
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of} % 
#+LATEX_HEADER: \RequirePackage{caption} % newlines in graphics

#+LaTeX_HEADER: \RequirePackage{tikz-cd} % graph
# ## https://tools.ietf.org/doc/texlive-doc/latex/tikz-cd/tikz-cd-doc.pdf

** Table
#+LATEX_HEADER: \RequirePackage{booktabs} % for nice lines in table (e.g. toprule, bottomrule, midrule, cmidrule)

** Inline latex
# @@latex:any arbitrary LaTeX code@@


** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
# #+LaTeX_HEADER: \RequirePackage{amsthm}
# #+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
# #+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \usepackage{ifthen}
#+LATEX_HEADER: \usepackage{xifthen}
#+LATEX_HEADER: \usepackage{xargs}
#+LATEX_HEADER: \usepackage{xspace}

#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

**** Probability
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\logLikelihood[2][1=,2=]{\defOperator{#1}{#2}{\ell}{}{(}{)}{}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\partial #2^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}

#+LATEX_HEADER: \newcommand\Veta{\boldsymbol{\eta}}
#+LATEX_HEADER: \newcommand\VX{\mathbf{X}}
#+LATEX_HEADER: \newcommand\sample{\chi}
#+LATEX_HEADER: \newcommand\Hspace{\mathcal{H}}
#+LATEX_HEADER: \newcommand\Tspace{\mathcal{T}}


** Notations
