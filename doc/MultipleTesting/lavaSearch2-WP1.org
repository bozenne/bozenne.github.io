#+TITLE: Assessing the effect of an exposure on multiple outcomes (with \Rlogo{} code)
#+Author: Brice Ozenne

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
options(width = 120)
path <-  "c:/Users/hpl802/Documents/GitHub/bozenne.github.io/doc/MultipleTesting/"
setwd(path)
#+END_SRC

#+RESULTS:

* Summary
:PROPERTIES:
:UNNUMBERED: t
:END:

We propose a strategy to assess the effect of an exposure (e.g. a
disease, a genetic factor) on several outcomes (e.g. psychological
outcomes, the binding potential measured in several brain regions)
while accounting for possible risk factors and confounders. This
strategy, called /multiple univariage regressions/ strategy, models
the relationship between each outcome and the exposure using a
separate model. Once the models have been correctly fitted, a global
test can be used to test whether there is any effect of the exposure
on the outcomes. After that, multiple tests are performed to test
outcome-specific effects of the exposure where the Dunnett adjustment
is used to control the type 1 error citep:pipper2012versatile. An
adjustment is used to improved the control of the type 1 error in
small sample sizes (e.g. n<100). This adjustment has been shown to
beneficial in several settings (using simulation studies) but does not
always perfectly control the type 1 error rate. It is advised to check
that validity of the adjustment when using very small samples or
models with many parameters.

\bigskip

The proposed strategy can be used with any type of outcomes for which
  a can fit a model with asymptotically linear estimators
  (cite:tsiatis2007semiparametric, section 3). This includes
  generalized linear model or Cox models. It makes it very flexible
  and the strategy makes only few assumptions on the joint
  distribution. The drawback is that it is not the most efficient
  approach. For instance modelling the joint distribution of the
  outcomes, e.g. using a latent variable model / mixed model in the
  case of normally distributed outcomes, will be a more efficient
  strategy. Another limitation is that with the proposed approach a
  treatment effect specific to each outcome will be estimated, while
  in some context the investigator may want to constrain the treatment
  effect to be the same for some outcomes. Finally, to be feasible the
  strategy requires the number of outcomes to be not too large (<100)
  and smaller than the number of observations (low-dimensional
  setting).

\bigskip

This document we aim at giving a basic understanding of the strategy
  and how to implement it. In particular, we don't claim that the
  proposed strategy is valid or optimal results in every application
  (it is probably not the case) and many other approaches exists but
  won't be discussed here. The document is organized as follow: first
  section [[#sec:Rsoftware]] present how to install and set up the
  \Rlogo{} software. Then basic functions to import and process the
  data are presented in section [[#sec:dataManagement]]. The goals, a
  minima, of the descriptive analysis are presented in section
  [[#sec:descriptive]], as well as code to export figures and tables to
  Word documents. Section [[#sec:LM]] recalls how to perform a univariate
  linear regression in \Rlogo{} and present tools to assess the
  parametric assumptions and section [[#sec:multipleLM]] presents the
  /multiple univariate regressions/ strategy. Finally appendix
  [[#SM:statistics]] recalls some statistical concepts about modeling and
  hypothesis testing.

\clearpage

* Simulation of the data :noexport:

To be able to assess the validity of the proposed strategies, we will
use simulated data containing:
- a variable identifying each patient: =Id=
- 10 outcomes per patient: =Y1= to =Y10=.
- 3 possible exposures per patient: =age= that is not related to the outcomes, =BMI=
  that has the same effect on all outcomes, and =MDI= that has a
  different effect per outcome.
We use the =lvm= function from the /lava/ package to define these variables:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
m.sim <- lava::lvm(Y1 ~ 0*age + 0.25*BMI + 0.1*MDI + 1*eta,
                   Y2[0:2] ~ 0*age + 0.25*BMI + 0.2*MDI + 2*eta,
                   Y3 ~ 0*age + 0.25*BMI + 0.15*MDI + 3*eta,
                   Y4[0:0.5] ~ 0*age + 0.25*BMI + 0.175*MDI + 1*eta,
                   Y5[0:3] ~ 0*age + 0.25*BMI + 0.075*MDI + 2*eta 
                   )
transform(m.sim, Id ~ eta) <- function(x){paste0("Subj",1:NROW(x))}
categorical(m.sim, labels = c("male","female")) <-  ~ Gender
distribution(m.sim, ~age) <-  gaussian.lvm(mean = 35, sd = 5)
distribution(m.sim, ~BMI) <-  gaussian.lvm(mean = 22, sd = 3)
distribution(m.sim, ~MDI) <-  gaussian.lvm(mean = 20, sd = 5)
latent(m.sim) <- ~eta
#+END_SRC

#+RESULTS:

From the code above we can see that the variance of the outcomes
 differs between outcomes and that the correlation between pairs of
 outcomes is also variable. We now simulate data using =lava::sim=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10)
dfW <- lava::sim(m.sim, n = 50, latent = FALSE)
#+END_SRC

#+RESULTS:
We round the values to 2 digits:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
digit.cols <- c("age","BMI","MDI",paste0("Y",1:5))
dfW[,digit.cols] <- round(dfW[,digit.cols],2)
#+END_SRC

#+RESULTS:

and re-order its columns:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfW <- dfW[,c("Id","Gender",digit.cols)]
#+END_SRC

#+RESULTS:

We can now display first lines of the dataset:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
head(dfW)
#+END_SRC
#+RESULTS:
:      Id Gender   age   BMI   MDI   Y1    Y2    Y3    Y4   Y5
: 1 Subj1 female 30.57 21.76 25.82 7.64  8.73  7.72 10.42 8.44
: 2 Subj2 female 41.36 25.55 12.38 7.11  8.79  6.99  8.45 8.26
: 3 Subj3   male 26.97 28.56  7.41 7.88  9.89 13.51 10.79 7.90
: 4 Subj4 female 40.61 23.22 16.46 8.99 14.38 13.82 11.44 9.75
: 5 Subj5 female 45.79 19.78 18.56 7.60  8.77  8.38  7.94 6.17
: 6 Subj6 female 37.14 16.13 17.82 6.99  9.97  6.74  8.29 8.78

and export the data using =write.csv=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
write.csv(dfW, file = file.path(path,"data.csv"), row.names = FALSE)
write.table(dfW, file = file.path(path,"data.txt"), row.names = FALSE)
xlsx::write.xlsx(dfW, file = file.path(path,"data.xlsx"), row.names = FALSE)
#+END_SRC

#+RESULTS:

\clearpage

* Software
:PROPERTIES:
:CUSTOM_ID: sec:Rsoftware
:END:
We advise to use the \Rlogo{} software to implement these strategies. It can
be downloaded at https://cloud.r-project.org/. R studio provide a
convenient user interface that can be downloaded at
https://www.rstudio.com/products/rstudio/.  The \Rlogo{} code used to carry
out the two strategies will be display in boxes:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
1+1 ## comment about the code
#+END_SRC

#+RESULTS:
: [1] 2

while the \Rlogo{} output will be displayed in dark red below the box. 

\bigskip

When starting a fresh \Rlogo{} session, only the core functionalities of
\Rlogo{} are available. Additional functionalities called packages can
be downloaded from the CRAN using the command =install.packages=. We
will need the following packages:
#+BEGIN_SRC R :exports code :results silent :session *R* :eval never
install.packages(pkgs = c("lava","lavaSearch2","multcomp","qqtest","gof","reshape2"))
#+END_SRC

After having installed the packages, one needs to load them using the
command =library= to use them in the current \Rlogo{} session:
#+BEGIN_SRC R  :results silent   :exports both  :session *R* :cache no
library(lava) ## definition and estimation of latent variable models
library(lavaSearch2) ## small sample correction 
library(multcomp) ## adjustment for multiple comparisons
library(qqtest) ## diagnostics: qqplots
library(gof) ## diagnostics: linearity assumption
library(reshape2) ## data management
#+END_SRC

I also recommend the following packages:
- /data.table/: for data management. See
  https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
  for an introduction and
  https://github.com/Rdatatable/data.table/wiki for more
  documentation.  A synthetic description of the functionalities can
  be found at
  https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf
- /ggplot2/: for data visualization. See
  http://r4ds.had.co.nz/data-visualisation.html for an introduction. A
  synthetic description of the functionalities can be found at
  https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

\clearpage

* Data management
:PROPERTIES:
:CUSTOM_ID: sec:dataManagement
:END:

** Working directory

The working directory is where R, by default, look for files
to import and export data or figures. The current working directory
can be accessed using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
getwd()
#+END_SRC

#+RESULTS:
: [1] "c:/Users/hpl802/AppData/Roaming/R"

It can be changed using the function =setwd()=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
path <- "c:/"
setwd(path)
#+END_SRC

#+RESULTS:

We can check that the working directory has indeed changed calling
again =getwd()=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
getwd()
#+END_SRC

#+RESULTS:
: [1] "c:/"

** Importing the data

It is a good idea to start by checking that the working directory
contains the data we want to import. For instance the file =data.csv=
is storing the data, we can use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
file.exists("data.csv")
#+END_SRC

#+RESULTS:
: [1] TRUE

We can also list all files in the current directory with a =.csv= extension using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
list.files(pattern = ".csv") 
#+END_SRC

#+RESULTS:
: [1] "data.csv"

We can also display the first lines of the file using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
readLines("data.csv")[1:3]
#+END_SRC

#+RESULTS:
: [1] "\"Id\",\"Gender\",\"age\",\"BMI\",\"MDI\",\"Y1\",\"Y2\",\"Y3\",\"Y4\",\"Y5\""
: [2] "\"Subj1\",\"female\",30.57,21.76,25.82,7.64,8.73,7.72,10.42,8.44"            
: [3] "\"Subj2\",\"female\",41.36,25.55,12.38,7.11,8.79,6.99,8.45,8.26"

We can see that the columns are separated with =,= and that the =.=
indicates the decimal values. Moreover the words such as the columns
names or the subject identities are surrounded by =\"= (e.g. =\"Id\"=
stand for Id). Finally in this example there is no missing values but
if there was it is important to know how they are encoded.

\clearpage

 The command to import the data depends on the type of file. Here for
a =.csv= file we use =read.csv=. Luckily the default arguments =sep=,
=dec=, =quote= are correctly specified:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
args(read.csv)
#+END_SRC

#+RESULTS:
: function (file, header = TRUE, sep = ",", quote = "\"", dec = ".", 
:     fill = TRUE, comment.char = "", ...) 
: NULL

The argument =header= set to =TRUE= indicates that the first line of
the dataset contains the column names (and not the actual data). The
=...= indicates there are additional arguments that are not shown here
(see the documentation using =help(read.csv)=). For instance, in
presence of missing values, one would need to specify the argument
=na.string=. Here it is sufficient to do:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfW <- read.csv("data.csv")
#+END_SRC

#+RESULTS:

Other functions exists to import other types of data,
e.g. =read.table= for =.txt= files, =read.xlsx= from the xlsx package
for =.xlsx= file, or =read.spss= from the foreign package for spss
data files. One should always inspect if R has correctly imported the
data, e.g. using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
str(dfW)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	50 obs. of  10 variables:
 $ Id    : Factor w/ 50 levels "Subj1","Subj10",..: 1 12 23 34 45 47 48 49 50 2 ...
 $ Gender: Factor w/ 2 levels "female","male": 1 1 2 1 1 1 2 1 2 2 ...
 $ age   : num  30.6 41.4 27 40.6 45.8 ...
 $ BMI   : num  21.8 25.6 28.6 23.2 19.8 ...
 $ MDI   : num  25.82 12.38 7.41 16.46 18.56 ...
 $ Y1    : num  7.64 7.11 7.88 8.99 7.6 6.99 3.76 6.94 6.57 6.89 ...
 $ Y2    : num  8.73 8.79 9.89 14.38 8.77 ...
 $ Y3    : num  7.72 6.99 13.51 13.82 8.38 ...
 $ Y4    : num  10.42 8.45 10.79 11.44 7.94 ...
 $ Y5    : num  8.44 8.26 7.9 9.75 6.17 8.78 2.41 5.38 5.04 5.22 ...
#+end_example

In this example, the two columns contain character strings (=Factor=
is a type of character strings in R) and the rest contains numerical
values.

** Data processing

Often the raw data needs to be transformed before being analyzed:
- A typical example is when one need to deal with the variable:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gender <- c(1,0,1,0,1) ## what is 1? what is 0?
#+END_SRC

#+RESULTS:
This is already better:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
female <- c(1,0,1,0,1) ## we can guess that 1: female and 0: male
#+END_SRC

#+RESULTS:

\clearpage 

but it is a good practice in such situation to rename the actual
values into something understandable:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
factor(gender, levels = 0:1, labels = c("Female","Male")) 
#+END_SRC

#+RESULTS:
: [1] Male   Female Male   Female Male  
: Levels: Female Male

- With repeated measurements per individual, one often needs to
  reshape his dataset from the wide format (one line per individual)
  to the long format (one line per measurement). This can be done
  using the =melt= method:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfL <- melt(dfW, 
            id.vars = c("Id","Gender","age","BMI","MDI"),
            value.name = "score",
            variable.name = "outcome")
head(dfL)
#+END_SRC

#+RESULTS:
:      Id Gender   age   BMI   MDI outcome score
: 1 Subj1 female 30.57 21.76 25.82      Y1  7.64
: 2 Subj2 female 41.36 25.55 12.38      Y1  7.11
: 3 Subj3   male 26.97 28.56  7.41      Y1  7.88
: 4 Subj4 female 40.61 23.22 16.46      Y1  8.99
: 5 Subj5 female 45.79 19.78 18.56      Y1  7.60
: 6 Subj6 female 37.14 16.13 17.82      Y1  6.99

The opposite operation can be performed using =dcast=.

- It is often a good idea to restrict the dataset to the relevant
  variables (e.g. remove genetic data if they are not of interest). It
  is easier to work with and to display in the next steps. This can
  for instance be done by defining the variables of interest:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
keep.var <- c("Id","BMI","MDI","Y1","Y2","Y3","Y4","Y5")
#+END_SRC

#+RESULTS:
We can check that the variables defined in =keep.var= are in =dfW=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
keep.var %in% names(dfW)
#+END_SRC

#+RESULTS:
: [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE

and then subset the initial dataset:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dfW.red <- dfW[,keep.var]
head(dfW.red)
#+END_SRC

#+RESULTS:
:      Id   BMI   MDI   Y1    Y2    Y3    Y4   Y5
: 1 Subj1 21.76 25.82 7.64  8.73  7.72 10.42 8.44
: 2 Subj2 25.55 12.38 7.11  8.79  6.99  8.45 8.26
: 3 Subj3 28.56  7.41 7.88  9.89 13.51 10.79 7.90
: 4 Subj4 23.22 16.46 8.99 14.38 13.82 11.44 9.75
: 5 Subj5 19.78 18.56 7.60  8.77  8.38  7.94 6.17
: 6 Subj6 16.13 17.82 6.99  9.97  6.74  8.29 8.78

- Often after having imported the data we want to change its column
  names. First we need to know the current column names. The =names=
  function can be used to output all the column names:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
names(dfW.red)
#+END_SRC

#+RESULTS:
: [1] "Id"  "BMI" "MDI" "Y1"  "Y2"  "Y3"  "Y4"  "Y5"

Alternatively the =grep= function will output any column name
containing a given string of characters:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
grep(pattern = "Y", x = names(dfW), value = TRUE)
#+END_SRC

#+RESULTS:
: [1] "Y1" "Y2" "Y3" "Y4" "Y5"

Then, we can rename a specific column using:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
names(dfW.red)[names(dfW.red) == "Id"] <- "new.Id.name"
head(dfW.red)
#+END_SRC

#+RESULTS:
:   new.Id.name   BMI   MDI   Y1    Y2    Y3    Y4   Y5
: 1       Subj1 21.76 25.82 7.64  8.73  7.72 10.42 8.44
: 2       Subj2 25.55 12.38 7.11  8.79  6.99  8.45 8.26
: 3       Subj3 28.56  7.41 7.88  9.89 13.51 10.79 7.90
: 4       Subj4 23.22 16.46 8.99 14.38 13.82 11.44 9.75
: 5       Subj5 19.78 18.56 7.60  8.77  8.38  7.94 6.17
: 6       Subj6 16.13 17.82 6.99  9.97  6.74  8.29 8.78

To rename several columns at the same time we can use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
old2new <- c("Y1" = "y1", "Y2" = "y2", "Y3" = "y3")
names(dfW.red)[match(names(old2new),names(dfW.red))] <- old2new
head(dfW.red)
#+END_SRC

#+RESULTS:
:   new.Id.name   BMI   MDI   y1    y2    y3    Y4   Y5
: 1       Subj1 21.76 25.82 7.64  8.73  7.72 10.42 8.44
: 2       Subj2 25.55 12.38 7.11  8.79  6.99  8.45 8.26
: 3       Subj3 28.56  7.41 7.88  9.89 13.51 10.79 7.90
: 4       Subj4 23.22 16.46 8.99 14.38 13.82 11.44 9.75
: 5       Subj5 19.78 18.56 7.60  8.77  8.38  7.94 6.17
: 6       Subj6 16.13 17.82 6.99  9.97  6.74  8.29 8.78

Other useful functions are =tolower= to convert characters to lower
case and =gsub= to remove a specific pattern in a character vector, e.g.:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
gsub(pattern = ".", replacement = "", x = c("a..","b..."), fixed = TRUE)
#+END_SRC

#+RESULTS:
: [1] "a" "b"

Many of the other data processing steps are specific to each study and
we won't discuss them in this document. 

\clearpage

* Descriptive statistics
:PROPERTIES:
:CUSTOM_ID: sec:descriptive
:END:

Before doing any analysis, it is a good practice to describe the data
that are to be analyzed. The has several aims:
- *check that that database contains the population of interest*,
  i.e. individuals in the database are indeed those the we want to
  study and we have all of them.
- *check that the collected values are plausible*, e.g. if the inclusion
  criteria include that the age range is between 18 and 99 years, then
  one should check that this is indeed the case.
- *check that the collected values are coded as expected*, e.g. age is
  usually coded in years (and not in months). 
- *check that the collected values are distributed as expected*,
  e.g. is there missing values? Are the values uniformly spread?
  Bimodal? Concentrated at low or high values?

Note: one should checks that for all the variables of interest. This
can appear time-consuming but can really save you time at latter
stages. 

- *produce your table 1* i.e. a descriptive table of your cohort that
  is almost always included in an article. You can for instance use
  the function =univariateTable= from the Publish package:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(Publish)
myTable1 <- univariateTable(Gender ~ age + BMI + MDI + Y1 + Y2 + Y3 + Y4 + Y5, 
                            data = dfW)
myTable1
#+END_SRC

#+RESULTS:
:   Variable     Level female (n=30) male (n=20) Total (n=50) p-value
: 1      age mean (sd)    36.2 (5.8)  34.6 (5.0)   35.6 (5.5) 0.31204
: 2      BMI mean (sd)    21.5 (3.3)  23.1 (3.2)   22.2 (3.4) 0.09325
: 3      MDI mean (sd)    19.2 (5.8)  19.2 (5.7)   19.2 (5.7) 0.97596
: 4       Y1 mean (sd)     7.2 (1.6)   7.2 (2.0)    7.2 (1.8) 0.93155
: 5       Y2 mean (sd)     9.6 (2.9)   9.5 (2.1)    9.6 (2.6) 0.82411
: 6       Y3 mean (sd)     8.5 (3.4)   8.4 (3.2)    8.4 (3.3) 0.91260
: 7       Y4 mean (sd)     8.8 (2.1)   9.3 (1.7)    9.0 (1.9) 0.39083
: 8       Y5 mean (sd)     7.4 (2.9)   7.0 (2.9)    7.2 (2.9) 0.65171

You can also export this table in a word document with the package
officer:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
library(officer)
myTable1.doc <- body_add_table(x = read_docx(), 
                               value =  summary(myTable1)) 
print(myTable1.doc, target = "./Table1.docx")
#+END_SRC

#+RESULTS:
: [1] "c:/Users/hpl802/Documents/GitHub/lavaSearch2/inst/vignettes/Table1.docx"

To keep the code simple, we only present here a very basic application
of these tools. More complex tables with a nicer display in word can
be obtain with a bit of coding.

\clearpage

- *make synthetic representations of your data* using graphs or
  images. This can be useful to visualize your data and help your
  collaborators to understand what you have collected or what you are
  trying to show.

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(ggplot2)
gg <- ggplot(dfL, aes(x = MDI, y = score, color = Gender, group = Gender))
gg <- gg + geom_point()
gg <- gg + facet_wrap(~outcome, labeller = label_both)
gg <- gg + geom_smooth(method = "lm", se = FALSE)
gg
#+END_SRC

#+RESULTS:

[[./figures/descriptive.pdf]]

You can then export the figure using:
#+BEGIN_SRC R :exports code :results output :session *R* :cache no
pdf("./figures/descriptive.pdf")
gg
dev.off()
#+END_SRC

#+RESULTS:
: null device 
:           1

- *Compare percentages when considering categorical data*: the usual
  way to compare the distribution of a categorical variable between
  two groups is to run a Fisher test using =fisher.test= in the R
  software. It returns a p-value and an estimate of the odd ratio with
  its confidence interval. For instance, consider the following
  dataset:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
mytable <- rbind(c(8,5),
                 c(4,15))
dimnames(mytable) <- list(c("control","treatment"),
                          c("-","+"))
mytable
#+END_SRC

#+RESULTS:
:           -  +
: control   8  5
: treatment 4 15
The Fisher test outputs:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
fisher.test(mytable)
#+END_SRC

#+RESULTS:
#+begin_example

	Fisher's Exact Test for Count Data

data:  mytable
p-value = 0.02996
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  0.9953576 38.7853302
sample estimates:
odds ratio 
  5.622612
#+end_example

This approach admits two drawbacks:
- the p-value may not agree with the confidence interval of the odd
  ratio regarding the rejection of the null hypothesis
- the odd ratio is a rather complex quantity to understand.
Instead one can use the function =binomMeld.test= (package /exact2x2/)
to perform a test on the proportions:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
binomMeld.test(x1=mytable["control","+"],n1=sum(mytable["control",]),
               x2=mytable["treatment","+"],n2=sum(mytable["treatment",]),
               parmtype="difference")
#+END_SRC

#+RESULTS:
#+begin_example

	melded binomial test for difference

data:  sample 1:(5/13), sample 2:(15/19)
proportion 1 = 0.38462, proportion 2 = 0.78947, p-value = 0.05077
alternative hypothesis: true difference is not equal to 0
95 percent confidence interval:
 -0.001077177  0.715576028
sample estimates:
difference (p2-p1) 
         0.4048583
#+end_example

This time the p-value is consistent with the confidence interval. In
fact, the p-value is matches the p-value of the confidence interval of
=fisher.test=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
exact2x2(mytable, tsmethod = "central")
#+END_SRC   

#+RESULTS:
#+begin_example

	Central Fisher's Exact Test

data:  mytable
p-value = 0.05077
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
  0.9953576 38.7853302
sample estimates:
odds ratio 
  5.622612
#+end_example

The appropriate reference for this approach is citep:fay2015combining.
\clearpage

* Univariate analysis using a univariate linear regression
:PROPERTIES:
:CUSTOM_ID: sec:LM
:END:

Imagine we want to assess the effect of MDI on \(Y_1\)
adjusting for age and BMI using a univariate linear
regression. Mathematically the model can be written:
#+BEGIN_EXPORT latex
\begin{align}
Y_1 = \alpha + \beta_{age} age + \beta_{BMI} BMI + \beta_{MDI} MDI + \varepsilon \label{eq:lm}
\end{align}
#+END_EXPORT
where \(\varepsilon\) are the residuals assumed to be independent and
identically distributed (iid).

** Fitting a univariate linear regression in \Rlogo{}

We can use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lm <- lm(Y1 ~ age + BMI + MDI, data = dfW)
#+END_SRC

#+RESULTS:

We can extract the value of the model coefficients using =coef=:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
coef(e.lm)
#+END_SRC

#+RESULTS:
:  (Intercept)          age          BMI          MDI 
: -1.413215636  0.006305252  0.247124506  0.151044284

** Interpretation of the regression coefficients
:PROPERTIES:
:CUSTOM_ID: sec:interpretationLM
:END:
If the assumptions (A0-A2) hold we can interpret \(\beta_{MDI}\) as a
correlation coefficient. This means that for fixed age and BMI, if we
observe an individual A with value of MDI higher by one unit compared
to individuals B then we would also expect that its value for \(Y_1\)
differ by \(\beta_{MDI}\) compared the other individual. If we in
addition make causal assumptions (mainly no unobserved confounder)
then we can interpret \(\beta_{MDI}\) as the effect of MDI on the
outcome. This means that if we could change the MDI of an individual
by one unit then its variation in outcome should be \(\beta_{MDI}\).

** Model checking
This step aims to identify gross violations of model assumptions that
could compromise the validity or the stability of the estimate. It is
described in a separate document available at XXXX.

** Hypothesis testing

We want to formally test whether there is an effect of MDI on the
outcome. This is equivalent to test the null hypothesis:
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; \beta_{MDI,0} = 0
\end{align*}
#+END_EXPORT
 Since the parameters are estimated by ML and assuming that the model
is correctly specified, we know that the asymptotic distribution of
the parameter is Gaussian. This means that for large sample size, the
fluctuation of the estimated values follows a normal distribution. For
instance:
#+BEGIN_EXPORT latex
\begin{align*}
\hat{\beta} \underset{n \rightarrow \infty}{\sim} \Gaus[\beta,\sigma^2_\beta]
\end{align*}
#+END_EXPORT
where \(\sigma^2_\beta\) is the variance of the MLE, i.e. the
uncertainty surrounding our estimation of the association. It follows that:
#+BEGIN_EXPORT latex
\begin{align}
t_{\beta} = \frac{\hat{\beta}-\beta_0}{\sigma^2_\beta} \underset{n \rightarrow \infty}{\sim} \Gaus[0,1] \label{eq:uniWald}
\end{align}
#+END_EXPORT
So under the null hypothesis of no association between the outcome and
the exposure the statistic \(t_{\beta}\) should follow a standard
normal distribution. Very low or very large values are unlikely to be
observed and would indicate that the null hypothesis does not
hold. This is called a (univariate) Wald test. The result of this
tests can be obtained using the =summary= method [fn:2]:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(e.lm)$coef
#+END_SRC

#+RESULTS:
:                 Estimate Std. Error    t value     Pr(>|t|)
: (Intercept) -1.413215636 1.95732599 -0.7220134 4.739407e-01
: age          0.006305252 0.03613264  0.1745030 8.622360e-01
: BMI          0.247124506 0.05790521  4.2677422 9.756957e-05
: MDI          0.151044284 0.03441289  4.3891775 6.598863e-05

[fn:2] In reality R is automatically performing a correction that
improves the control of the type 1 error. Indeed we usually don't know
\(\sigma^2_\beta\) and plugging-in its estimate in equation
eqref:eq:uniWald modifies the distribution of \(t_{\beta}\) in small
samples. The correction uses a Student's t distribution instead of a
Gaussian distribution.




95% confidence intervals for the model parameters can then be obtained
using the =confint= method:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
confint(e.lm)
#+END_SRC

#+RESULTS:
:                   2.5 %     97.5 %
: (Intercept) -5.35310851 2.52667723
: age         -0.06642598 0.07903648
: BMI          0.13056736 0.36368165
: MDI          0.08177473 0.22031384

\clearpage

* Multivariate analysis using multiple univariate linear regressions 
:PROPERTIES:
:CUSTOM_ID: sec:multipleLM
:END:

We now want to simultaneously test the effect of MDI on all the five
outcomes. To achieve it, we fit separately for each outcome a
univariate linear regression. Mathematically the model can be written:
#+BEGIN_EXPORT latex
\begin{align*}
\begin{bmatrix} 
Y_1  &= \alpha_{Y_{1}} + \beta_{Y_1,age} age + \beta_{Y_1,BMI} BMI + \beta_{Y_1,MDI} MDI + \varepsilon_{Y_1} \\
Y_2  &= \alpha_{Y_{2}} + \beta_{Y_2,age} age + \beta_{Y_2,BMI} BMI + \beta_{Y_2,MDI} MDI + \varepsilon_{Y_2} \\
Y_3  &= \alpha_{Y_{3}} + \beta_{Y_3,age} age + \beta_{Y_3,BMI} BMI + \beta_{Y_3,MDI} MDI + \varepsilon_{Y_3} \\
Y_4  &= \alpha_{Y_{4}} + \beta_{Y_4,age} age + \beta_{Y_4,BMI} BMI + \beta_{Y_4,MDI} MDI + \varepsilon_{Y_4} \\
Y_5  &= \alpha_{Y_{5}} + \beta_{Y_5,age} age + \beta_{Y_5,BMI} BMI + \beta_{Y_5,MDI} MDI + \varepsilon_{Y_5} 
\end{bmatrix} 
\end{align*}
#+END_EXPORT
where
\(\varepsilon_{1},\varepsilon_{2},\varepsilon_{3},\varepsilon_{4},\varepsilon_{5}\)
are the residual errors. The residuals are assumed to have zero mean
and finite variance, respectively,
\(\sigma^2_{1},\sigma^2_{2},\sigma^2_{3},\sigma^2_{4},\sigma^2_{5}\). Here
we make no assumption on the correlation structure between the
residuals.

** Fitting multiple linear regression in \Rlogo{}

We can estimate all the 5 models and store them into a list:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
ls.lm <- list(Y1 = lm(Y1 ~ age + BMI + MDI, data = dfW),
              Y2 = lm(Y2 ~ age + BMI + MDI, data = dfW),
              Y3 = lm(Y3 ~ age + BMI + MDI, data = dfW),
              Y4 = lm(Y4 ~ age + BMI + MDI, data = dfW),
              Y5 = lm(Y5 ~ age + BMI + MDI, data = dfW)
              )
#+END_SRC

#+RESULTS:

** Interpretation of the regression coefficients

Same as in the univariate case (see section [[#sec:interpretationLM]]).

** Diagnostics tools for univariate linear regression in \Rlogo{}

Same as in the univariate case (see section [[#sec:diagLM]]). This model
checking needs to be done for each outcome.

** Hypothesis testing

We now want to test:
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; \beta_{Y_1, MDI, 0} = 0
 \text{ and } \beta_{Y_2, MDI, 0} = 0
 \text{ and } \beta_{Y_3, MDI, 0} = 0
 \text{ and } \beta_{Y_4, MDI, 0} = 0
 \text{ and } \beta_{Y_5, MDI, 0} = 0
\end{align*}
#+END_EXPORT

The p-values returned by =summary= are no more valid since we are
performing multiple tests (here 5 tests). A basic solution would be to
collect the p-values:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
vec.p.value <- unlist(lapply(ls.lm, function(x){
    summary(x)$coef["MDI","Pr(>|t|)"]
}))
#+END_SRC

#+RESULTS:

\clearpage

and adjust them for multiple comparisons using Bonferroni:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
p.adjust(vec.p.value, method = "bonferroni")
#+END_SRC

#+RESULTS:
:           Y1           Y2           Y3           Y4           Y5 
: 3.299432e-04 4.218369e-02 3.552579e-01 2.276690e-07 8.565878e-01

While easy to use this approach tends to be too conservative
(i.e. give to large p-values) when the test statistics are
correlated. This is usually the case when the outcomes are
correlated. We will therefore use a more efficient correction called
the Dunnett approach. First we need to define the null hypothesis
that we want to test via a contrast matrix. For simple null hypotheses
like the one we are considering in this example, we can use the
function =createContrast= that will create the matrix for us:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
resC <- createContrast(ls.lm, var.test = "MDI", add.variance = TRUE)
#+END_SRC

#+RESULTS:

This function defines for each model the appropriate contrast matrix:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
resC$mlf
#+END_SRC
#+RESULTS:
#+begin_example
$Y1
    (Intercept) age BMI MDI sigma2
MDI           0   0   0   1      0

$Y2
    (Intercept) age BMI MDI sigma2
MDI           0   0   0   1      0

$Y3
    (Intercept) age BMI MDI sigma2
MDI           0   0   0   1      0

$Y4
    (Intercept) age BMI MDI sigma2
MDI           0   0   0   1      0

$Y5
    (Intercept) age BMI MDI sigma2
MDI           0   0   0   1      0

attr(,"class")
[1] "mlf"
#+end_example

and right hand side of the null hypothesis:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
resC$null
#+END_SRC

#+RESULTS:
: Y1: MDI Y2: MDI Y3: MDI Y4: MDI Y5: MDI 
:       0       0       0       0       0

\clearpage

We will now call =glht2= to perform the adjustment for multiple
comparisons but first we need to convert the list into a =mmm= object:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
class(ls.lm) <- "mmm"
e.glht_lm <- glht2(ls.lm, linfct = resC$contrast, rhs = resC$null)
e.glht_lm
#+END_SRC

#+RESULTS:
#+begin_example

	 General Linear Hypotheses

Linear Hypotheses:
             Estimate
Y1: MDI == 0  0.15104
Y2: MDI == 0  0.16770
Y3: MDI == 0  0.14907
Y4: MDI == 0  0.19860
Y5: MDI == 0  0.09806
#+end_example

We can now correct for multiple comparisons using the (single-step)
Dunnett approach:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(e.glht_lm, test = adjusted("single-step"))
#+END_SRC

#+RESULTS:
#+begin_example

	 Simultaneous Tests for General Linear Hypotheses

Linear Hypotheses:
             Estimate Std. Error t value Pr(>|t|)    
Y1: MDI == 0  0.15104    0.03441   4.389   <0.001 ***
Y2: MDI == 0  0.16770    0.06093   2.752   0.0286 *  
Y3: MDI == 0  0.14907    0.08067   1.848   0.1996    
Y4: MDI == 0  0.19860    0.03039   6.535   <0.001 ***
Y5: MDI == 0  0.09806    0.07057   1.390   0.4208    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- single-step method)
#+end_example

Note that the p-value for the global test equals to the smallest
 p-value. This means that we reject the global null hypothesis
 whenever we reject the null hypothesis for any of the outcome (after
 adjustment for multiple comparisons!).


 For comparison one can change the argument in =adjust= to apply the
 Bonferroni adjustment:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(e.glht_lm, test = adjusted("bonferroni"))
#+END_SRC

#+RESULTS:
#+begin_example

	 Simultaneous Tests for General Linear Hypotheses

Linear Hypotheses:
             Estimate Std. Error t value Pr(>|t|)    
Y1: MDI == 0  0.15104    0.03441   4.389  0.00033 ***
Y2: MDI == 0  0.16770    0.06093   2.752  0.04218 *  
Y3: MDI == 0  0.14907    0.08067   1.848  0.35526    
Y4: MDI == 0  0.19860    0.03039   6.535 2.28e-07 ***
Y5: MDI == 0  0.09806    0.07057   1.390  0.85659    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- bonferroni method)
#+end_example

Finally, confidence intervals can be obtained using the =confint=
function:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
confint(e.glht_lm)
#+END_SRC

#+RESULTS:
#+begin_example

	 Simultaneous Confidence Intervals

Fit: NULL

Quantile = 2.5215
95% family-wise confidence level
 

Linear Hypotheses:
             Estimate lwr      upr     
Y1: MDI == 0  0.15104  0.06427  0.23782
Y2: MDI == 0  0.16770  0.01407  0.32133
Y3: MDI == 0  0.14907 -0.05434  0.35248
Y4: MDI == 0  0.19860  0.12197  0.27524
Y5: MDI == 0  0.09806 -0.07987  0.27599
#+end_example
Note that by default the =confint= function output confidence
intervals using the (single-step) Dunnett approach.


\clearpage

* Multivariate model :noexport:

** Random intercept model

*** Reshape the data from wide to long format
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtL <- melt(dtW, 
            id.vars = c("Id","E0","E1","E2"),
            measure.vars = c("Y1","Y2","Y3","Y4","Y5"),
            value.name = "Y",
            variable.name = "region")
#+END_SRC

#+RESULTS:

Display reshaped dataset:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtL
#+END_SRC

#+RESULTS:
#+begin_example
      Id         E0          E1         E2 region          Y
  1:  n1 -0.4006375 -0.76180434 -0.3911042     Y1  1.0046984
  2:  n2 -0.3345566  0.41937541 -0.2498675     Y1  0.2264810
  3:  n3  1.3679540 -1.03994336  1.1551047     Y1 -0.1255308
  4:  n4  2.1377671  0.71157397 -0.8647272     Y1  0.3643000
  5:  n5  0.5058193 -0.63321301 -0.8666783     Y1 -1.0312430
 ---                                                        
246: n46 -1.4196451  1.06587933 -0.3134741     Y5 -1.5671398
247: n47 -1.6066772  0.53064987 -1.7036595     Y5  1.0095687
248: n48  0.8929259  0.10198345 -1.3505147     Y5  1.6133809
249: n49  0.1481680  1.33778247 -1.1020937     Y5 -0.4073399
250: n50  1.2270284  0.08723477 -1.0995430     Y5 -0.2423385
#+end_example

*** Fit the model

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lme <- lme(Y ~ region + E0 + E1 + E2,
             random =~ 1|Id, 
             data = dtL)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
anova(e.lme)
#+END_SRC

#+RESULTS:
:             numDF denDF   F-value p-value
: (Intercept)     1   196 0.0172758  0.8956
: region          4   196 1.1994831  0.3124
: E0              1    46 0.0368633  0.8486
: E1              1    46 1.2447933  0.2703
: E2              1    46 0.3986999  0.5309


#+BEGIN_SRC R :exports both :results output :session *R* :cache no
getVarCov(e.lme, type = "marginal")
#+END_SRC

#+RESULTS:
: Id n1 
: Marginal variance covariance matrix
:        1      2      3      4      5
: 1 5.2708 2.9900 2.9900 2.9900 2.9900
: 2 2.9900 5.2708 2.9900 2.9900 2.9900
: 3 2.9900 2.9900 5.2708 2.9900 2.9900
: 4 2.9900 2.9900 2.9900 5.2708 2.9900
: 5 2.9900 2.9900 2.9900 2.9900 5.2708
:   Standard Deviations: 2.2958 2.2958 2.2958 2.2958 2.2958

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
dtL$res.lme <- residuals(e.lme, type = "pearson")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
ggplot(dtL, aes(x = region, y = res.lme)) + geom_boxplot()
leveneTest(y = dtL$res.lme, group = dtL$region)
#+END_SRC

#+RESULTS:
: Levene's Test for Homogeneity of Variance (center = median)
:        Df F value    Pr(>F)    
: group   4  4.9497 0.0007456 ***
:       245                      
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
outTest <- cor.testDT(data = dtL, format = "long", col.value = "res.lme", col.group = "region",
                      reorder = NULL)
#+END_SRC

#+RESULTS:
: ========================================================================================================================

** Latent variable model

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
m <- lvm(Y1 ~ E0 + E1 + E2 + eta,
         Y2 ~ E0 + E1 + E2 + eta,
         Y3 ~ E0 + E1 + E2 + eta,
         Y4 ~ E0 + E1 + E2 + eta,
         Y5 ~ E0 + E1 + E2 + eta
         )
latent(m) <- ~eta
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e <- estimate(m, data = dtW)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
getVarCov2(e)
#+END_SRC

#+RESULTS:
: uncorrected variance-covariance matrix 
: 
:           Y1       Y2       Y3        Y4       Y5
: Y1 1.3840571 2.005664 2.444097 0.8121274 1.888262
: Y2 2.0056638 6.115769 5.419284 1.8007265 4.186834
: Y3 2.4440965 5.419284 7.518300 2.1943605 5.102065
: Y4 0.8121274 1.800727 2.194361 1.3346606 1.695320
: Y5 1.8882616 4.186834 5.102065 1.6953204 7.799161

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
sCorrect(e) <- TRUE
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
getVarCov2(e)
#+END_SRC

#+RESULTS:
:           Y1       Y2       Y3        Y4       Y5
: Y1 1.5044099 2.180069 2.656627 0.8827472 2.052458
: Y2 2.1800693 6.647575 5.890526 1.9573114 4.550906
: Y3 2.6566266 5.890526 8.172065 2.3851744 5.545722
: Y4 0.8827472 1.957311 2.385174 1.4507180 1.842740
: Y5 2.0524582 4.550906 5.545722 1.8427396 8.477349

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
M.res.lvm <- residuals2(e, type = "normalized")

## sort data by id
setkeyv(dtL, "Id")

dtL[,res.lvm := as.numeric(NA)]
dtL[region == "Y1", res.lvm := M.res.lvm[,1]]
dtL[region == "Y2", res.lvm := M.res.lvm[,2]]
dtL[region == "Y3", res.lvm := M.res.lvm[,3]]
dtL[region == "Y4", res.lvm := M.res.lvm[,4]]
dtL[region == "Y5", res.lvm := M.res.lvm[,5]]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
ggplot(dtL, aes(x = region, y = res.lvm)) + geom_boxplot()
leveneTest(y = dtL$res.lvm, group = dtL$region)
#+END_SRC

#+RESULTS:
: Levene's Test for Homogeneity of Variance (center = median)
:        Df F value Pr(>F)
: group   4  0.1548 0.9607
:       245

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
outTest <- cor.testDT(data = dtL, format = "long", col.value = "res.lvm", col.group = "region",
                      reorder = NULL)
#+END_SRC

#+RESULTS:
: ========================================================================================================================

* References

#+BEGIN_EXPORT latex
\begingroup
\renewcommand{\section}[2]{}
#+END_EXPORT
bibliographystyle:apalike
[[bibliography:bibliography.bib]] 
#+BEGIN_EXPORT latex
\endgroup
#+END_EXPORT

 # @@latex:any arbitrary LaTeX code@@

\clearpage

\appendix

* Statistics: definitions and notations 
:PROPERTIES:
:CUSTOM_ID: SM:statistics
:END:

** Variables

We can differentiate several types of random variables: outcomes,
exposure, risk factors, confounders, and mediators. To explicit the
difference between these types of variables we consider a set of
random variables \((Y,E,X_1,X_2,M)\) whose relationships are
displayed on autoref:fig:pathDiagram:
- *outcome* (\(Y\)): random variables that are observed with noise. It
  can be for instance the 5HT-4 binding in a specific brain
  region. When considering several outcomes we will denote in bold
  variable that stands for a vector of random variables:
  \(\mathbf{Y}=(Y_1,Y_2,\ldots,Y_m)\). This happens for instance when
  studying the binding in several brain regions. In such a case we
  expect the outcomes to be correlated.
- *exposure* (\(E\)): a variable that may affect the outcome or be
  associated with the outcome /and/ we are interested in studying this
  effect/association. It can for instance be a genetic factor that is
  hypothesized to increase the 5HT-4 binding, or a disease like
  depression that is associated with a change in binding (we don't
  know whether one causes the other or whether they have a common
  cause, e.g. a genetic variant).
- *risk factor/confounder* (\(X_1,X_2\)): a variable that
  may affect the outcome or be associated with the outcomes /but/ we
  are /not/ interested in studying their effect/association. Risk
  factors (denoted by \(X_1\)) are only associated with the outcomes
  and confounders that are both associated with the outcome and the
  exposure. We usually need to account for confounders the statistical
  model in order to obtain unbiased estimates while accounting for
  risk factors only enables to obtain more precise estimates (at least
  in linear models).
- *mediator* (\(M\)): a variable that modulate the effect of the
  exposure, i.e. stands on the causal pathway between the exposure and
  the outcome. For instance, the permeability of the blood-brain
  barrier may modulate the response to drugs and can act as a
  mediator. It is important to keep in mind that when we are
  interested in the (total) effect of \(E\) on \(Y\), we should /not/
  adjust the analysis on \(M\)[fn:3]. Doing so we would remove the effect of
  \(E\) mediated by \(M\) and therefore bias the estimate of the total
  effect (we would only get the direct effect).

In the following we will assume that we do not measure any mediator
variable and therefore ignore this type of variable. Also we will call
*covariates* the variables \(E,X_1,X_2\).

#+header: :width 3 :height 3 :R-dev-args bg="lightgrey"
#+BEGIN_SRC R :results graphics :file "./figures/pathDiagram.pdf" :exports results :session *R* :cache no
m <- lvm(Y~E+X1+X2+M,M~E,E~X2)
plot(m, plot.engine="rgraphviz") ## visnetwork ## igraph
#+END_SRC

#+name: fig:pathDiagram
#+ATTR_LATEX: :width 0.7\textwidth
#+CAPTION: Path diagram relating the variables Y, E, M, \(X_1\) and \(X_2\)
[[./figures/pathDiagram.pdf]]

[fn:3] This may not be true in specific types of confounding but we
will ignore that.




\clearpage 

** Assumptions

We can distinguish two types of assumptions:
- *causal assumptions*: saying which variables are related and in
  which direction. This can be done by drawing a path diagram similar
  to autoref:fig:pathDiagram. In simple univariate models it may seems
  unnecessary to draw the path diagram since the system of variables is
  very simple to visualize. In multivariate model, it is often very
  useful to draw it. Some of these assumptions are untestable,
  e.g. often we cannot decide whether it is \(E\) that impacts \(Y\)
  or whether it is \(Y\) that impacts \(E\) just based on the data.

- *modeling assumptions*: specifying the type of relationship between
  variables (e.g. linear) and the marginal or joint distribution
  (e.g. Gaussian). Often these assumptions can be tested and relaxed
  using a more flexible model. While appealing, there are some
  drawbacks with using a very flexible model: more data are needed to
  get precise estimates and the interpretation of the results is more
  complex.

** Statistical model
A statistical model \(\model\) is set of possible probability
distributions. For instance when we fit a Gaussian linear model for
\(Y_1\) with just an intercept \(\model=\left\{\Gaus[\mu,\sigma^2];\mu
\in \Real, \; \sigma^2 \in \Real^+ \right\}\): \(\model\) is the set
containing all possible univariate normal distributions.

** Model parameters

The model parameters are the (non random) variables that enable the
statistical model to "adapt" to different settings. They will be
denoted \(\Theta\). They are the one that are estimated when we fit
the statistical model using the data or that we specify when we
simulate data. In the previous example, we could simulate data
corresponding to a Gaussian linear model using the =rnorm= function in
R:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
rnorm
#+END_SRC

#+RESULTS:
: function (n, mean = 0, sd = 1) 
: .Call(C_rnorm, n, mean, sd)
: <bytecode: 0x000000001d7eb938>
: <environment: namespace:stats>

We would need to specify:
- \(n\) the sample size
- \(\Theta=(\mu,\sigma^2)\) the model parameters, here \(\mu\) corresponds to =mean= and \(\sigma\) to =sd=.

\bigskip

The true model parameters are the model parameters that have generated
the observed data. They will be denoted \(\Theta_0\). For instance if
in reality the binding potential is normally distributed with mean 5
and variance \(2^2=4\), then
\(\Theta_0=(\mu_0,\sigma_0^2)=(5,4)\). Then doing our experiment we
observed data such as:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10)
Y_1.XP1 <- rnorm(10, mean = 5, sd = 2)
Y_1.XP1
#+END_SRC

#+RESULTS:
:  [1] 5.037492 4.631495 2.257339 3.801665 5.589090 5.779589 2.583848 4.272648 1.746655 4.487043

If we were to re-do the experiment we would observe new data but \(\Theta_0\) would not change:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Y_1.XP2 <- rnorm(10, mean = 5, sd = 2)
Y_1.XP2
#+END_SRC

#+RESULTS:
:  [1] 7.203559 6.511563 4.523533 6.974889 6.482780 5.178695 3.090112 4.609699 6.851043 5.965957

The estimated parameters are the parameters that we estimate when we
fit the statistical model. They will be denoted \(\hat{\Theta}\). We
usually try to find parameters whose value maximize the chance of
simulating the observed data under the estimated model (maximum
likelihood estimation, MLE). For instance in the first experiment all
values are positive so we would not estimate a negative mean value. In
our example, \(\hat{\mu}\) the MLE of \(\mu\) reduces to the empirical
average and \(\hat{\sigma}^2\) the MLE of \(\sigma^2\) to the
empirical variance:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Theta_hat.XP1 <- c(mu_hat = mean(Y_1.XP1),
                   sigma2_hat = var(Y_1.XP1))
Theta_hat.XP1
#+END_SRC

#+RESULTS:
:     mu_hat sigma2_hat 
:   4.018686   1.959404

Clearly the estimated coefficients vary across experiments:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Theta_hat.XP2 <- c(mu_hat = mean(Y_1.XP2),
                   sigma2_hat = var(Y_1.XP2))
Theta_hat.XP2
#+END_SRC

#+RESULTS:
:     mu_hat sigma2_hat 
:   5.739183   1.799311

** Parameter of interest

The statistical model may contain many parameters, most of them are
often not of interest but are needed to obtain valid estimates
(e.g. account for confounders). In most settings, the parameter of
interest is one (or several) model parameter(s) - or simple
transformation of them. For instance if we are interested in the
average binding potential in the population our parameter of interest
is \(\mu\).

\bigskip

Often, the aim of a study is to obtain the best estimate of the
parameter of interest \(\mu\). Best means:
- *unbiased*: if we were able to replicate the study many times,
  i.e. get several estimates \(\hat{\mu}_1,\hat{\mu}_2,\ldots,\hat{\mu}_K\), the
  average estimate \(<\hat{\mu}>=\frac{\hat{\mu}_1+\hat{\mu}_2+\ldots+\hat{\mu}_K}{K}\) would coincide with the true one \(\mu_0\).
- *minimal variance*: if we were able to replicate the study many
  times, the variance of the estimates
  \(\frac{(\hat{\mu}_1-<\hat{\mu}>)^2+\ldots+(\hat{\mu}_K-<\hat{\mu}>)^2}{K-1}\)
  should be as low as possible.

There will often be a trade-off between these two objectives. A very
flexible method is more likely to give an unbiased estimate
(e.g. being able to model non-linear relationship) at the price of
greater uncertainty about the estimates. Often we favor unbiasedness
over minimal variance. Indeed, if several studies are published with
the same parameter of interest, one can pool the results to obtain an
estimate with lower variance. Note that we have no guarantee that it
will reduce the bias.

** Contrast matrix

When dealing with many parameters it is convenient to define the null
hypothesis via a contrast matrix. An example of null hypothesis is:
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; \beta_{MDI,0} = 0
\end{align*}
#+END_EXPORT
If we consider \(\Theta=(\alpha,\beta_{age},\beta_{BMI},\beta_{MDI})\),
this null hypothesis can be equivalently written:

#+BEGIN_EXPORT latex
\begin{align*}
c=[0 \; 0 \; 0 \; 1]
\end{align*}
#+END_EXPORT
such that: 
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; c \trans{\Theta}_{0} = 0
\end{align*}
#+END_EXPORT
Indeed
#+BEGIN_EXPORT latex
\begin{align*}
c \trans{\Theta}_{0} = 0 * \alpha_0 + 0 * \beta_{age,0} + 0 * \beta_{BMI,0} + 1 * \beta_{MDI,0} = \beta_{MDI,0}
\end{align*}
#+END_EXPORT

#+RESULTS:

An example where the contrast matrix is useful is
- when one wish to test linear combination of parameters,
  e.g. consider the null hypothesis:
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; \beta_{MDI,0} = \beta_{BMI,0}
\end{align*}
#+END_EXPORT
Here the contrast matrix would be:
#+BEGIN_EXPORT latex
\begin{align*}
c=[0 \; 0 \; -1 \; 1]
\end{align*}
#+END_EXPORT
- when one wish to test several hypotheses simultaneously,
  e.g. consider the null hypothesis:
#+BEGIN_EXPORT latex
\begin{align*}
(\Hypothesis[0]) \; \beta_{BMI,0} = 0 \text{ or } \beta_{MDI,0} = 0 \\
\end{align*}
#+END_EXPORT
Here the contrast matrix would be:
#+BEGIN_EXPORT latex
\begin{align*}
C = \begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\end{align*}
#+END_EXPORT
 
In \Rlogo{}, the method =createContrast= helps to define the contrast
matrix:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Clin <- createContrast(e.lm, par = c("MDI - BMI = 0"),
                       add.variance = FALSE, rowname.rhs = FALSE)
Clin$contrast
#+END_SRC

#+RESULTS:
:             (Intercept) age BMI MDI
: - BMI + MDI           0   0  -1   1

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Csim <- createContrast(e.lm, par = c("BMI = 0","MDI = 0"),
                       add.variance = FALSE, rowname.rhs = FALSE)
Csim$contrast
#+END_SRC

#+RESULTS:
:     (Intercept) age BMI MDI
: BMI           0   0   1   0
: MDI           0   0   0   1

Then the contrast matrix can be send to =glht= to obtain p-values and
confidence intervals:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
elin.glht <- glht(e.lm, linfct = Clin$contrast)
summary(elin.glht)
#+END_SRC

#+RESULTS:
: 
: 	 Simultaneous Tests for General Linear Hypotheses
: 
: Fit: lm(formula = Y1 ~ age + BMI + MDI, data = dfW)
: 
: Linear Hypotheses:
:                  Estimate Std. Error t value Pr(>|t|)
: - BMI + MDI == 0 -0.09608    0.06993  -1.374    0.176
: (Adjusted p values reported -- single-step method)

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
esim.glht <- glht(e.lm, linfct = Csim$contrast)
summary(esim.glht)
#+END_SRC

#+RESULTS:
#+begin_example

	 Simultaneous Tests for General Linear Hypotheses

Fit: lm(formula = Y1 ~ age + BMI + MDI, data = dfW)

Linear Hypotheses:
         Estimate Std. Error t value Pr(>|t|)    
BMI == 0  0.24712    0.05791   4.268 0.000195 ***
MDI == 0  0.15104    0.03441   4.389 0.000132 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- single-step method)
#+end_example

* Power and type 1 error :noexport:

** Multiple linear regression: no adjustment vs. Bonferroni vs. Dunnett
:PROPERTIES:
:CUSTOM_ID: appendix:massUnivariate
:END:

Function replicating the analysis for a given sample size:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
warper_type1power <- function(n.sample){

    ## simulate data
    iDf <- lava::sim(m.sim, n = n.sample, latent = FALSE)

    ## fit model
    iLs <- list(Y1 = lm(Y1 ~ E0+E1+E2, data = iDf),
                Y2 = lm(Y2 ~ E0+E1+E2, data = iDf),
                Y3 = lm(Y3 ~ E0+E1+E2, data = iDf),
                Y4 = lm(Y4 ~ E0+E1+E2, data = iDf),
                Y5 = lm(Y5 ~ E0+E1+E2, data = iDf)
                )
    class(iLs) <- "mmm"

    ## type 1 error
    iC.E0 <- createContrast(iLs, var.test = "E0", add.variance = TRUE)
    iGlht.E0 <- glht2(iLs, linfct = iC.E0$contrast, rhs = iC.E0$null)

    ## power
    iC.E1 <- createContrast(iLs, var.test = "E1", add.variance = TRUE)
    iGlht.E1 <- glht2(iLs, linfct = iC.E1$contrast, rhs = iC.E1$null)

    ## export
    vec.minP <- c("type1.none" = min(summary(iGlht.E0, test = adjusted("none"))$test$pvalues),
                  "type1.bonferroni" = min(summary(iGlht.E0, test = adjusted("bonferroni"))$test$pvalues),
                  "type1.dunnett" = min(summary(iGlht.E0, test = adjusted("single-step"))$test$pvalues),
                  "power.none" = min(summary(iGlht.E1, test = adjusted("none"))$test$pvalues),
                  "power.bonferroni" = min(summary(iGlht.E1, test = adjusted("bonferroni"))$test$pvalues),
                  "power.dunnett" = min(summary(iGlht.E1, test = adjusted("single-step"))$test$pvalues))
    return(vec.minP)
}
#+END_SRC

#+RESULTS:

Perform simulation study:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10)
n.cpus <- 4
n.sim <- 1e3

cl <- snow::makeSOCKcluster(n.cpus)
doSNOW::registerDoSNOW(cl)

pb <- txtProgressBar(max = n.sim, style=3)
opts <- list(progress = function(n) setTxtProgressBar(pb, n))

ls.res <- foreach::`%dopar%`(
                       foreach::foreach(i=1:n.sim,
                                        .options.snow=opts,
                                        .packages = c("multcomp","lavaSearch2")), {
                                            warper_type1power(50)
                                        })

parallel::stopCluster(cl)
M.p <- Reduce(rbind,ls.res)
#+END_SRC

#+RESULTS:

Type 1 error:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans(M.p[,1:3]<=0.05)
#+END_SRC

#+RESULTS:
:       type1.none type1.bonferroni    type1.dunnett 
:            0.165            0.034            0.057

Power:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans(M.p[,4:6]<=0.05)
#+END_SRC

#+RESULTS:
:       power.none power.bonferroni    power.dunnett 
:            0.381            0.137            0.178

\clearpage

** Latent variable model: no adjustment vs. Bonferroni vs. Dunnett :noexport:

Fonction replicating the analysis for a given sample size:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
## define model
m <- lvm(Y1 ~ E0 + E1 + E2 + eta,
         Y2 ~ E0 + E1 + E2 + eta,
         Y3 ~ E0 + E1 + E2 + eta,
         Y4 ~ E0 + E1 + E2 + eta,
         Y5 ~ E0 + E1 + E2 + eta
         )
latent(m) <- ~eta

warper_type1power <- function(n.sample){ ## n.sample <- 50

    ## simulate data
    iDf <- lava::sim(m.sim, n = n.sample, latent = FALSE)

    ## fit model
    iE <- estimate(m, data = iDf)
    sCorrect(iE) <- TRUE

    ## type 1 error
    iC.E0 <- createContrast(iE, var.test = "E0", add.variance = TRUE)
    iF.E0 <- compare2(iE, contrast = iC.E0$contrast, null = iC.E0$null)
    iGlht.E0 <- glht2(iE, linfct = iC.E0$contrast, rhs = iC.E0$null)

    ## power error
    iC.E1 <- createContrast(iE, var.test = "E1", add.variance = TRUE)
    iF.E1 <- compare2(iE, contrast = iC.E1$contrast, null = iC.E1$null)
    iGlht.E1 <- glht2(iE, linfct = iC.E1$contrast, rhs = iC.E1$null)

    ## export
    vec.minP <- c("type1.Ftest" = iF.E0$p.value,
                  "type1.none" = min(summary(iGlht.E0, test = adjusted("none"))$test$pvalues),
                  "type1.bonferroni" = min(summary(iGlht.E0, test = adjusted("bonferroni"))$test$pvalues),
                  "type1.dunnett" = min(summary(iGlht.E0, test = adjusted("single-step"))$test$pvalues),
                  "power.Ftest" = iF.E1$p.value,
                  "power.none" = min(summary(iGlht.E1, test = adjusted("none"))$test$pvalues),
                  "power.bonferroni" = min(summary(iGlht.E1, test = adjusted("bonferroni"))$test$pvalues),
                  "power.dunnett" = min(summary(iGlht.E1, test = adjusted("single-step"))$test$pvalues))
    return(vec.minP)
}
#+END_SRC

#+RESULTS:

Perform simulation study:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10)
n.cpus <- 3
n.sim <- 1e3

cl <- snow::makeSOCKcluster(n.cpus)
doSNOW::registerDoSNOW(cl)

pb <- txtProgressBar(max = n.sim, style=3)
opts <- list(progress = function(n) setTxtProgressBar(pb, n))

ls.resLVM <- foreach::`%dopar%`(
                          foreach::foreach(i=1:n.sim,
                                           .options.snow=opts,
                                           .packages = c("multcomp","lavaSearch2")), {
                                               warper_type1power(50)
                                           })

parallel::stopCluster(cl)
M.pLVM <- Reduce(rbind,ls.resLVM)
#+END_SRC

#+RESULTS:

Type 1 error:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans(M.pLVM[,1:4]<=0.05)
#+END_SRC

#+RESULTS:
:      type1.Ftest       type1.none type1.bonferroni    type1.dunnett 
:            0.073            0.168            0.044            0.057

Power:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans(M.pLVM[,5:8]<=0.05)
#+END_SRC

#+RESULTS:
:      power.Ftest       power.none power.bonferroni    power.dunnett 
:            0.276            0.400            0.168            0.199

* Different parametrisations of Gaussian models :noexport:

** Random intercept model
*** using nlme::gls

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.gls <- gls(Y ~ region + E0 + E1 + E2,
             correlation = corCompSymm(form =~1|Id), 
             data = dtL, 
             method = "ML")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
logLik(e.gls)
#+END_SRC

#+RESULTS:
: 'log Lik.' -470.2986 (df=10)

*** using nlme::lme

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lme <- lme(Y ~ region + E0 + E1 + E2,
             random =~ 1|Id, 
             data = dtL, 
             method = "ML")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
logLik(e.lme)
#+END_SRC

#+RESULTS:
: 'log Lik.' -470.2986 (df=10)

*** using lme4::lmer

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lmer <- lmer(Y ~ region + E0 + E1 + E2 + (1|Id),
               data = dtL, 
               REML = FALSE)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
logLik(e.lmer)
#+END_SRC

#+RESULTS:
: 'log Lik.' -470.2986 (df=10)


*** using lava

Defining the model:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
m.ranint <- lvm(c(Y1,Y2,Y3,Y4,Y5)~ beta0 * E0 + beta1 * E1 + beta2 * E2 + 1*eta)
variance(m.ranint, ~Y1+Y2+Y3+Y4+Y5) <- as.list(rep("sigma2",5))
latent(m.ranint) <- ~eta
#+END_SRC

#+RESULTS:

Fit model:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.ranint <- estimate(m.ranint, data = dtW)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
logLik(e.ranint)
#+END_SRC

#+RESULTS:
: 'log Lik.' -470.2986 (df=10)

** MANOVA

*** using manova
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.manova <- manova(cbind(Y1,Y2,Y3,Y4,Y5) ~ E0 + E1 + E2, data = dtW)
e.manova
#+END_SRC

#+RESULTS:
#+begin_example
Call:
   manova(cbind(Y1, Y2, Y3, Y4, Y5) ~ E0 + E1 + E2, data = dtW)

Terms:
                      E0       E1       E2 Residuals
resp 1            0.0012   0.3645   1.8085   69.2029
resp 2            0.0655   3.3287   4.8072  239.5034
resp 3            0.7615   8.6185  13.2868  375.9150
resp 4            0.1084   0.0205   0.1527   95.5452
resp 5            0.1597  15.8821   0.5535  246.4111
Deg. of Freedom        1        1        1        46

Residual standard errors: 1.226544 2.281797 2.858682 1.441204 2.314468
Estimated effects may be unbalanced
#+end_example

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.manova$coefficients
#+END_SRC

#+RESULTS:
:                      Y1          Y2         Y3          Y4          Y5
: (Intercept) -0.38648123  0.11227103  0.4314597  0.10647397  0.11534656
: E0          -0.01097614 -0.03299929 -0.1195223 -0.04377409 -0.08288801
: E1          -0.16366671 -0.14651257 -0.2292348  0.04281923 -0.54508495
: E2          -0.21367875  0.34837266  0.5791694  0.06208583  0.11821111

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
summary(e.manova)
#+END_SRC

#+RESULTS:
:           Df   Pillai approx F num Df den Df  Pr(>F)  
: E0         1 0.006858  0.05801      5     42 0.99766  
: E1         1 0.090953  0.84045      5     42 0.52873  
: E2         1 0.224913  2.43750      5     42 0.05002 .
: Residuals 46                                          
: ---
: Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

*** using lava

Estimate the model:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
m.manova <- lvm(c(Y1,Y2,Y3,Y4,Y5)~E0+E1+E2+1*eta, eta ~ 0)
e.lvmManova <- estimate(m.manova, data = dtW)
sCorrect(e.lvmManova) <- TRUE
#+END_SRC

#+RESULTS:

Mean structure:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
eCoef.lvmManova <- coef(e.lvmManova)
eNameCoef.lvmManova <- names(eCoef.lvmManova)

rbind("(Intercept)" = eCoef.lvmManova[1:5],
      "E0" = eCoef.lvmManova[grep("~E0$",eNameCoef.lvmManova)],
      "E1" = eCoef.lvmManova[grep("~E1$",eNameCoef.lvmManova)],
      "E2" = eCoef.lvmManova[grep("~E2$",eNameCoef.lvmManova)])
#+END_SRC

#+RESULTS:
:                      Y1          Y2         Y3          Y4          Y5
: (Intercept) -0.38648123  0.11227103  0.4314597  0.10647397  0.11534656
: E0          -0.01097614 -0.03299929 -0.1195223 -0.04377409 -0.08288801
: E1          -0.16366671 -0.14651257 -0.2292348  0.04281923 -0.54508495
: E2          -0.21367875  0.34837266  0.5791694  0.06208583  0.11821111

Variance structure:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
sqrt(colMeans(residuals2(e.lvmManova)^2))
#+END_SRC

#+RESULTS:
:       Y1       Y2       Y3       Y4       Y5 
: 1.226544 2.281797 2.858682 1.441204 2.314468

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
resC <- createContrast(e.lvmManova, var.test = "E2")
compare2(e.lvmManova, contrast = resC$contrast, null = resC$null)
#+END_SRC

#+RESULTS:
#+begin_example

	- Wald test -

	Null Hypothesis:
	[Y1~E2] = 0
	[Y2~E2] = 0
	[Y3~E2] = 0
	[Y4~E2] = 0
	[Y5~E2] = 0

data:  
F-statistic = 2.0936, df1 = 5, df2 = 62.77, p-value = 0.07789
sample estimates:
               Estimate   Std.Err       df       2.5%     97.5%
[Y1~E2] = 0 -0.21367875 0.2363002 51.50187 -0.6879589 0.2606014
[Y2~E2] = 0  0.34837266 0.3020515 71.00163 -0.2539007 0.9506460
[Y3~E2] = 0  0.57916938 0.3641231 70.53412 -0.1469546 1.3052934
[Y4~E2] = 0  0.06208583 0.2769659 66.34014 -0.4908415 0.6150131
[Y5~E2] = 0  0.11821111 0.3212436 72.08529 -0.5221633 0.7585855
#+end_example

* CONFIG :noexport:
# #+LaTeX_HEADER:\affil{Department of Biostatistics, University of Copenhagen, Copenhagen, Denmark}
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Code
#+PROPERTY: header-args :session *R*
#+PROPERTY: header-args :tange yes % extract source code: http://orgmode.org/manual/Extracting-source-code.html
#+PROPERTY: header-args :cache no
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

** Display 
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\usepackage{authblk} % enable several affiliations (clash with beamer)
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}
#+LATEX_HEADER:\geometry{top=2cm}

** Image
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of}
#+LATEX_HEADER: \RequirePackage{caption} 

** Notations
#+LaTeX_HEADER: \newcommand\model{\mathcal{M}}
#+LaTeX_HEADER: \newcommand\Rlogo{\textbf{\textsf{R}}}

** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{ifthen}
#+LATEX_HEADER: \RequirePackage{xspace} % space for newcommand macro
#+LATEX_HEADER: \RequirePackage{xifthen}
#+LATEX_HEADER: \RequirePackage{xargs}
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
#+LaTeX_HEADER: \RequirePackage{amsthm}
#+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
#+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

*** Shortcuts

**** Probability
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}


