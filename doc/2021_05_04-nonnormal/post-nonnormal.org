#+TITLE: Handling deviation from normality
#+Author: Brice Ozenne

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
library(lava)
library(ggplot2)
library(data.table)
library(truncnorm)
path <- "~/Documents/Github/bozenne.github.io/doc/2021_05_04-nonnormal/"
setwd(path)
#+END_SRC

#+RESULTS:
: Fejl i setwd(path) : cannot change working directory

\noindent A question that often arise in consultations is:

#+BEGIN_CENTER
"My data is not normally distributed. What should I do?"
#+END_CENTER

\noindent I find it difficult to answer as it is vague. It is a bit
like going to the doctor and asking him:

#+BEGIN_CENTER
 "My body temperature is outside of
the 36-37\textdegree C normal range. What should I do?"
#+END_CENTER

\noindent In some cases statistical methods are robust (i.e. ensure
approximate type 1 error control and give nearly efficient estimators)
to "small" deviations from normality. So while it is a good idea to
check for normality, it is not a good idea to change statistical tool
just because you get a significant p-value out of a Shapiro test. If I
follow the analogy, medicine can be harmful and one should not take
medicine every time ones body temperature is outside the normal
range. It is however a good idea to check what is going on when your
temperature is high to make sure it does not hide something bad. First
thing to consider is the type of variable for which the concern about
normality arised:
  - *outcome*: non-normality can be a threat to the interpretability
    of the estimate and the validity of subsquent hypothesis
    test. Example of non-normality are shown in autoref:fig:examples
    and corresponding solution are discussed section [[#sec:outcome]].
  - *exposure*: generally not a problem. One notable exception is in
    presence of outliers when assuming a linear exposure effect as the
    outliers may have an unacceptable influence on the exposure effect
    (see autoref:fig:leverage for example). This will be discussed in
    section [[#sec:exposure]].
  - *covariate*: generally not a problem. One notable exception is in
    presence of outliers when assuming a linear exposure effect as the
    outliers may have an unacceptable influence on the exposure
    effect. Possible solutions relaxing the linearity assumption
    (using splines or categorizing the covariate). If the covariate is
    not strongly related to the outcome one can also consider
    excluding it from the model. It is not further discussed in this
    document.

\noindent Before discussing any solution, the next section what is understood as
a 'good' or a 'valid' statistical procedure.

\clearpage

* Statistical properties
:PROPERTIES:
:CUSTOM_ID: sec:properties
:END:

When we use a statistical procedure to estimate a parameter of
interest (say \(\theta\)), we generally want to report an estimate of
this parameter based on the data we have (denoted
\(\widehat{\theta}\)). We generally also want to report the
uncertainty around this estimate via a confidence intervals (denoted
\(IC_{\widehat{\theta}} = [L_{\widehat{\theta}} ;
U_{\widehat{\theta}}]\)) or report a p-value that reflects whether a
value (say \(\theta_0\)) is compatible with the data at hand. We will
therefore distinguish three properties for a statistical procedure:
- *validity of the estimate*: the estimate should tend to the true
  value as we increase the sample size (consistency) or the average
  estimate should tend to the true value as repeat the
  experiment (unbiasedness). 
- *validity of the uncertainty*: the confidence interval should have
  proper coverage, typically they should contain the true value with
  probablity 95%. The p-value should have a uniform distribution under
  the null meaning that the type 1 error is controlled at its nominal
  level, typically 5%.
- *robustness*: means that the validity of the estimate and
  uncertainty is not (dramatically) altered when the sample is altered
  by a fixed proportion of extreme values (say 1% of outliers).
- *efficiency*: the procedure is optimal in the sense that it makes
  the best use of the data at hand. \newline For instance we expect
  the estimates to be as precise as possible, i.e. to have the
  narrowest possible confidence intervals. We also expect that the
  type 2 error should be the smallest possible (i.e. highest
  power). In other terms, whenever the null hypothesis is false, the
  test should rejected it as often as possible.

\bigskip

  In the following we will assume that these properties are of
  decreasing interest: having unbiased estimates is the most important
  as quantifying uncertainty and efficiency can be fixed by
  replicating studies and performing a meta-analysis. A non-efficient
  estimator can still give useful and valid results - it will "just"
  waste ressources but not be misleading (if used and interpreted
  correctly). Typically robustness can be acquired at the expense of
  efficiency (and interpretability).

\clearpage

* Non-normal exposure
:PROPERTIES:
:CUSTOM_ID: sec:exposure
:END:

Consider the following example shown in autoref:fig:leverage (example 5.1 in cite:maronna2019robust) evaluating the association between the
contents of Western Australian rocks. Consider first a *linear
relationship*:
- one point has a significant impact on the regression slope: 0.135
  (p<0.001) vs 0.030 (p=0.18) when exluded. The 'red' regression line
  does not seems to be a reasonnable summary of the association.
- even after removal of observation 15, the exposure is still far from
  following a normal distribution as it seems very
  right-skewed. Nevertheless the 'blue' regression line seems to be a
  reasonnable summary of the association.
- modeling the median instead of the mean only partially solves the
  issue: observation 15 has still a noticeable influence on the slope
  (0.080 vs. 0.056). In fact in a more extreme example shown in the
  right panel of As shown in autoref:fig:leverageExtreme one could
  construct example where a small fraction of outliers would still be
  very influencial.

\noindent Consider instead a *non-linear* relationship:
- using thin plate regression splines seems to provide a reasonnable
  summary of the association with a different slope for small copper
  value compared to high copper content. The later slope is probably
  estimated with a lot of uncertainty due to only few observations
  with high copper content: this could be seen from the width of the
  confidence intervals of the regression line.

#+LaTeX: \vspace{-1cm}

#+LaTeX: \begin{minipage}{0.475\linewidth} 
#+name: fig:leverage
#+ATTR_LaTeX: :width \textwidth :placement [H]
#+CAPTION: Real data with non-normal exposure.
[[./figures/examples-leverage.pdf]]
#+LaTeX: \end{minipage}
#+LaTeX: \begin{minipage}{0.475\linewidth} 
#+name: fig:leverageExtreme
#+ATTR_LaTeX: :width \textwidth :placement [H]
#+CAPTION: Simulated data with non-normal exposure.
[[./figures/examplesExtreme-leverage.pdf]]

#+LaTeX: \end{minipage}

\clearpage

\noindent This example illustrate several key considerations:
- the exposure does not need to be normally distributed for
  statistical methods to apply BUT parametric assumptions, such as
  linearity, make common statistical tools sensitive to extreme
  values.
- it can be a good idea to restrict the study of the exposure to
  commonly observed values. This is similar to an inclusion criteria in
  a clinical trial on the disease severity (e.g. not including
  terminally ill patients).
- some methods sometimes refered as 'robust', like median or quantile
  regression, have been developped to handle extreme values in the
  outcome not in the exposure and thus may not lead to a satisfactory
  solution. For instance, median regression minimizes the average of
  the absolute residuals and can therefore be greatly influenced by a
  single leverage point.
- solutions include relaxing parametric assumptions or using more
  specialized 'robust' technics, e.g. estimators based on a robust
  summary of the residual (e.g. median of the absolute
  residuals). They typically make the interpretation more challenging
  either because there is no more a single number describing the
  association or the single number is no more 'just' a mean or median
  difference in the population of interest.


** \Rlogo code

*** No covariate

Load data:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(RobStatTM) 
data(mineral)
mineral <- mineral[order(mineral$copper),]
head(mineral)
#+END_SRC

#+RESULTS:
:    copper zinc
: 41      4    4
: 48     12    3
: 49     14   10
: 36     17   15
: 42     18   10
: 26     19   17

Quick assessment of normality of the exposure:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
shapiro.test(mineral$copper)
#+END_SRC

#+RESULTS:
: 
: 	Shapiro-Wilk normality test
: 
: data:  mineral$copper
: W = 0.67314, p-value = 1.374e-09

Analysis:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(quantreg)
library(mgcv)
library(robust)
library(RobStatTM)

set.seed(1)
e.mean <- lm(zinc ~ copper, data = mineral)
e15.mean <- lm(zinc ~ copper, data = mineral[-NROW(mineral),])
e.median <- rq(zinc ~ copper, data = mineral)
e15.median <- rq(zinc ~ copper, data = mineral[-NROW(mineral),])
e.lmRob <- lmRob(zinc ~ copper, data = mineral)
e.lmRob2 <- lmrobdetMM(zinc ~ copper, data = mineral,
                       control = lmrobdet.control(family = "bisquare"))
e.gam <- gam(zinc ~ s(copper), data = mineral)

rbind(mean.all = summary(e.mean)$coef["copper",],
      median.all = summary(e.median, se = "boot")$coef["copper",],
      rob.all = summary(e.lmRob)$coef["copper",],
      rob2.all = summary(e.lmRob2)$coef["copper",],
      mean.red = summary(e15.mean)$coef["copper",],
      median.red = summary(e15.median, se = "boot")$coef["copper",])
#+END_SRC

#+RESULTS:
:              Estimate Std. Error   t value     Pr(>|t|)
: mean.all   0.13456951 0.01982765 6.7869632 1.181421e-08
: median.all 0.08024691 0.05326053 1.5066864 1.380606e-01
: rob.all    0.01471517 0.02424047 0.6070496 5.465113e-01
: rob2.all   0.03118872 0.02082673 1.4975332 1.404186e-01
: mean.red   0.02974749 0.02205388 1.3488551 1.834611e-01
: median.red 0.05590062 0.04380294 1.2761843 2.077867e-01

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
pdf("figures/examples-leverage.pdf", width = 6, height = 6)
plot(mineral$copper,mineral$zinc, xlab = "Copper", ylab = "Zinc")
points(mineral$copper,fitted(e.mean), col = "red", type = "l", lwd = 2)
points(mineral$copper,fitted(e.median), col = "orange", type = "l", lwd = 2)
points(mineral$copper[-NROW(mineral)],fitted(e15.mean), col = "blue", type = "l", lwd = 2)
points(mineral$copper,fitted(e.lmRob2), col = "green", type = "l", lwd = 2)
points(mineral$copper,fitted(e.gam), col = "lightblue", type = "l", lwd = 2)
legend("topleft", pch = 19, col = c("red","orange","blue","lightblue","green"), legend = c("lm()","rq()","lm() without the last observation","gam()","lmrobdetMM (bisquare family)"))
dev.off()
#+END_SRC

#+RESULTS:
: windows 
:       2

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
n <- 100
set.seed(1)

df.signal <- data.frame(X = rnorm(n),
                        Y = rnorm(n))
df.signal <- df.signal[order(df.signal$X),]
df.extreme <- data.frame(X = rnorm(n/25, mean = 10),
                         Y = rnorm(n/25, mean = 7))
df.all <- rbind(df.signal, df.extreme)

e.mean <- lm(Y ~ X, data = df.all)
e.median <- rq(Y ~ X, data = df.all)
e15.mean <- lm(Y ~ X, data = df.signal)
e15.median <- rq(Y ~ X, data = df.signal)
e.lmRob <- lmRob(Y ~ X, data = df.all)
e.lmRob2 <- lmrobdetMM(Y ~ X, data = df.all, control = lmrobdet.control(family = "bisquare"))

rbind(mean.all = summary(e.mean)$coef["X",],
      median.all = summary(e.median, se = "boot")$coef["X",],
      rob.all = summary(e.lmRob)$coef["X",],
      rob2.all = summary(e.lmRob2)$coef["X",],
      mean.red = summary(e15.mean)$coef["X",],
      median.red = summary(e15.median, se = "boot")$coef["X",])

pdf("figures/examplesExtreme-leverage.pdf", width = 6, height = 6)
plot(x = df.all$X, y = df.all$Y)
points(df.all$X,fitted(e.mean), col = "red", type = "l", lwd = 2)
points(df.all$X,fitted(e.median), col = "orange", type = "l", lwd = 2)
points(df.signal$X,fitted(e15.mean), col = "blue", type = "l", lty = "dashed", lwd = 2)
points(df.signal$X,fitted(e15.median), col = "lightblue", type = "l", lty = "dashed", lwd = 2)
points(df.all$X,fitted(e.lmRob2), col = "green", type = "l", lwd = 2)
legend("topleft", pch = 19, col = c("red","orange","blue","lightblue","green"), legend = c("lm()","rq()","lm() without 4 outliers","rq() without 4 outliers","lmrobdetMM (bisquare family)"))
dev.off()
#+END_SRC

#+RESULTS:
:                Estimate Std. Error      t value     Pr(>|t|)
: mean.all    0.586078870 0.04967204 11.798968789 8.889013e-21
: median.all  0.474072290 0.18334652  2.585662925 1.113167e-02
: rob.all    -0.011424431 0.13750542 -0.083083498 9.339480e-01
: rob2.all    0.013981341 0.10995733  0.127152428 8.990701e-01
: mean.red   -0.001060386 0.10772703 -0.009843269 9.921663e-01
: median.red  0.094752411 0.10607825  0.893231304 3.739216e-01
: windows 
:       2

\clearpage

*** With covariates

We now consider a more complex example (example 5.2 in
cite:maronna2019robust) involving multiple covariates:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(robustbase) 
data(wood, package = "robustbase")
head(wood)
#+END_SRC

#+RESULTS:
:      x1     x2    x3    x4    x5     y
: 1 0.573 0.1059 0.465 0.538 0.841 0.534
: 2 0.651 0.1356 0.527 0.545 0.887 0.535
: 3 0.606 0.1273 0.494 0.521 0.920 0.570
: 4 0.437 0.1591 0.446 0.423 0.992 0.450
: 5 0.547 0.1135 0.531 0.519 0.915 0.548
: 6 0.444 0.1628 0.429 0.411 0.984 0.431

\noindent We fit each model:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
e.lm <- lm(y ~ x1 + x2 + x3 + x4 + x5, data = wood)
e.lmRob2 <- lmrobdetMM(y ~ x1 + x2 + x3 + x4 + x5, data = wood,
                       control = lmrobdet.control(family = "bisquare"))
#+END_SRC

#+BEGIN_SRC R :exports none :results output :session *R* :cache no
e.lmRob <- lmRob(y ~ x1 + x2 + x3 + x4 + x5, data = wood)
coef(e.lmRob)-coef(e.lmRob2) ## very similar
#+END_SRC

#+RESULTS:
:   (Intercept)            x1            x2            x3            x4            x5 
: -0.0010528610  0.0008666411 -0.0042322187 -0.0004069150 -0.0021614313  0.0028035266

\noindent  And extract the partial residuals
- either adding the intercept and the contribution to the variable of
  interest to the residuals:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
pres.lm <-  residuals(e.lm) + coef(e.lm)["(Intercept)"] + wood$x1 * coef(e.lm)["x1"]
pres.robust <- residuals(e.lmRob2) + coef(e.lmRob2)["(Intercept)"] + wood$x1 * coef(e.lmRob2)["x1"]
#+END_SRC

#+RESULTS:

- or using the =residuals= method  and re-centering the result (to
  match the original mean instead of 0):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Mpres.lm2 <- residuals(e.lm, type = "partial")
centerX.lm <- sum(coef(e.lm)[paste0("x",2:5)] * colMeans(wood)[paste0("x",2:5)])
pres.lm2 <- Mpres.lm2[,"x1"] + attr(Mpres.lm2, "constant") - centerX.lm
#+END_SRC

#+RESULTS:

Both approaches give the same up to a constant:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
range(pres.lm - pres.lm2)
#+END_SRC

#+RESULTS:
: [1] -1.110223e-16  1.110223e-16

\noindent  We can then combine the partial residuals in a single data.frame:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
df.pres <- rbind(data.frame(method = "lm", x1 = wood2$x1, res = Mpres.lm),
                 data.frame(method = "robust", x1 = wood2$x1,
                            res = Mpres.robust))
#+END_SRC
#+RESULTS:

and evaluate the model fit along x1 value, keep the other covariates at their reference level (here 0):
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
grid.data <- data.frame(x1 = seq(min(wood2$x1),max(wood2$x1),by=0.01),
                        x2 = 0, x3 = 0, x4 = 0, x5 = 0)
df.pfit <- rbind(data.frame(method = "lm", x1 = grid.data$x1,
                            fit = predict(e.lm, newdata = grid.data)),
                 data.frame(method = "robust", x1 = grid.data$x1,
                            fit = predict(e.lmRob2, newdata = grid.data))
                 )
#+END_SRC

#+RESULTS:

to obtain the following graphical display:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(ggplot2)
ggP <- ggplot(mapping = aes(x=x1))
ggP <- ggP + geom_point(data = df.pres,
                        mapping = aes(y = res, color = method))
ggP <- ggP + geom_line(data = df.pfit,
                       mapping = aes(y = fit, color = method))
ggP <- ggP + labs(x = "x1", y = "Y (partial residuals w.r.t. x1)")
ggP
#+END_SRC

#+name: fig:example-covariate
#+ATTR_LaTeX: :width 1\textwidth :options trim={0 0 0 0} :placement [!h]
[[./figures/examples-covariates.pdf]]


#+BEGIN_SRC R :exports none :results output :session *R* :cache no
ggsave(ggP, filename = file.path("figures","examples-covariates.pdf"), width = 8, height = 4)
#+END_SRC

#+RESULTS:

#+LaTeX:\vspace{-1cm}

\noindent This dataset was created to have 4 unusual observations (those with
 \(x_1<0.45\)). The ordinary linear regression has the better overall
 fit (smaller variability of the residuals) whereas the robust
 approach has a better fit on the subset of 'usual' observations
 (smaller variability of the residuals when excluding the 4 unusual
 observations).

\clearpage

* Non-normal outcome
:PROPERTIES:
:CUSTOM_ID: sec:outcome
:END:

To ease the discussion, we will consider a simple example where we
want to compare the outcome distribution between two groups. We assume
to have no missing data and no measurement error, and that no external
covariate is relevant. In that case, we can visualize the distribution
of the outcome per group and perform the comparison "visually". We
will consider four examples (autoref:fig:examples):
- Normally distributed outcomes: no problem here.
- Student distributed outcomes: symetric and unimodal but with outliers.
- Gamma distributed outcomes: asymetric distribution. 
- Normally distributed outcomes with ceilling effect: many
  observations have exactly the same value.

#+name: fig:examples
#+ATTR_LaTeX: :width 0.9\textwidth :placement [H]
#+CAPTION: Example of simulated non-normal outcome.
[[./figures/examples-hist.pdf]]

\noindent \Warning if any, distributional assumptions are usually made on the
residual terms, e.g:
#+begin_export latex
\begin{align*}
Y = X \beta + \varepsilon \text{ where } \varepsilon \sim \Gaus[0,\sigma^2]
\end{align*}
#+end_export


and not on the outcome \(Y\). Concretely, we don't assume that the
outcome is normally distributed but that within groups (or once we
remove the group effect) it is normally distributed. In example 1, the
outcome is clearly not normally distributed (it is bimodal) but within
groups it is normally distributed.



#+BEGIN_SRC R :exports none :results output :session *R* :cache no
library(data.table)
set.seed(10)
n <- 1e5
dt <- rbind(data.table(Y = rnorm(n, mean = 0, sd = 1), group = "C", example = "1. Gaussian"),
            data.table(Y = rnorm(n, mean = 1, sd = 1), group = "T", example = "1. Gaussian"),
            data.table(Y = rt(n, df = 4), group = "C", example = "2. Student"),
            data.table(Y = rt(n, df = 4) + 1, group = "T", example = "2. Student"),
            data.table(Y = rgamma(n, shape = 2, rate = 1/2), group = "C", example = "3. Gamma"),
            data.table(Y = 2*rgamma(n, shape = 2, rate = 1/2), group = "T", example = "3. Gamma"),
            data.table(Y = pmin(rnorm(n, mean = 0, sd = 1),2.5), group = "C", example = "4. Ceilling effect"),
            data.table(Y = pmin(rnorm(n, mean = 1, sd = 1),2.5), group = "T", example = "4. Ceilling effect"))
ggDens <- ggplot(dt, aes(x=Y,fill=group))
ggDens <- ggDens + geom_histogram(breaks = seq(-3,10, by = 0.1), position = "dodge") + facet_wrap(~example)
ggDens <- ggDens + coord_cartesian(xlim = c(-3,7)) + ylab("Number of observations")
ggDens <- ggDens + theme(text = element_text(size=15), 
                          axis.line = element_line(size = 1.25),
                          axis.ticks = element_line(size = 2),
                          axis.ticks.length=unit(.25, "cm"))
ggsave(ggDens, filename = file.path("figures","examples-hist.pdf"), width = 9, height = 6)
#+END_SRC

#+RESULTS:
: data.table 1.15.4 using 4 threads (see ?getDTthreads).  Latest news: r-datatable.com
: Advarselsbesked:
: pakke 'data.table' blev bygget under R version 4.2.3
: Advarselsbesked:
: [1m[22mThe `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.
: [36mℹ[39m Please use the `linewidth` argument instead.
: [90mThis warning is displayed once every 8 hours.[39m
: [90mCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.[39m


** Should we worry about normality? 

Most statistical procedures do not require any normality assumption to
provide consistent estimates with asymptotically valid confidence
intervals and p-values. For instance t-tests and linear regressions
can be shown to provide consistent and asymptotically normally
distributed estimates in large iid [fn::independent and identically
distributed] samples, regardless to whether they are normally
distributed as soon as their first two moments are finite[fn::For some
statistical tests, this requires to use robust instead of model-based
standard error]. Here it is important to distringuish between the
distribution of the outcome (say \(Y\)) and the distribution of the
parameter of interest, often the mean of \(Y\). Averaging "normalizes"
the distribution, which is formalized in the central limit theorem,
and illustrated on autoref:fig:distAverage:

#+begin_src R :exports none :results output :session *R* :cache no
set.seed(10)


dtAverage <- rbind(
  data.table(Y = sapply(1:n, function(x){mean(rgamma(1, shape = 2, rate = 1/2))}), group = "C", n = "sample size = 1"),
  data.table(Y = sapply(1:n, function(x){mean(2*rgamma(1, shape = 2, rate = 1/2))}), group = "T", n = "sample size = 1"),
  data.table(Y = sapply(1:n, function(x){mean(rgamma(10, shape = 2, rate = 1/2))}), group = "C", n = "sample size = 10"),
  data.table(Y = sapply(1:n, function(x){mean(2*rgamma(10, shape = 2, rate = 1/2))}), group = "T", n = "sample size = 10"),
  data.table(Y = sapply(1:n, function(x){mean(rgamma(100, shape = 2, rate = 1/2))}), group = "C", n = "sample size = 100"),
  data.table(Y = sapply(1:n, function(x){mean(2*rgamma(100, shape = 2, rate = 1/2))}), group = "T", n = "sample size = 100")
)
ggAv <- ggplot(dtAverage, aes(x=Y,fill=group)) + ylab("Number of averages")
ggAv <- ggAv + geom_histogram(breaks = seq(0,20,length.out=100), position = "dodge") + facet_wrap(~n, scales = "free")
ggAv <- ggAv + theme(text = element_text(size=10),
                     legend.position = "bottom",
                     axis.line = element_line(size = 1.25),
                     axis.ticks = element_line(size = 2),
                     axis.ticks.length=unit(.25, "cm"))
ggsave(ggAv, filename = file.path("figures","examples-histAverage.pdf"), height = 4, width = 7)
#+end_src

#+RESULTS:

#+name: fig:distAverage
#+ATTR_LaTeX: :width \textwidth :placement [!h]
#+CAPTION: Distribution of the estimated mean along the sample size.
[[./figures/examples-histAverage.pdf]]

This means that (almost) regardless to the input data, we will be able
to estimate parameters which follows a normal distribution, i.e. for
which we can quantify the uncertainty. Results from the M-estimation
theory or the maximum likelihood theory can be used to show that
finding parameters that minimize an error that is the lack of fit
relative to individual observations lead to consistent
estimates. Concretely, this means that the coverage/type 1 error
control of many standard procedures such as the t-test and the linear
regression will be at their nominal level in large samples, even
though the normality assumption is not fullfilled
(autoref:fig:coverage) \ldots for large enough sample sizes.

\clearpage

#+name: fig:coverage
#+ATTR_LaTeX: :width \textwidth :placement [!h]
#+CAPTION: Coverage of the t-test.
[[./figures/examples-coverage.pdf]]

\noindent Does that mean we should not worry about normality? No:
1. we may have a valid test / consistent estimate of a meaningless
   parameter.
2. we may only have a small sample.
3. our estimator may not be efficient. This is usually not a problem,
  except when we loose so much efficiency that the estimate becomes
  very variable. This typically happen in presence of outliers.
In the following we will discuss issues 1 to 3.
  
#+begin_src R :exports none :results output :session *R* :cache no
set.seed(10)

seqN <- c(5,10,15,25,50,75,100)
n.sim <- 50000
out <- NULL

for(iIndex in 1:length(seqN)){
  iN <- seqN[iIndex]

  M.p <- do.call(rbind,pbapply::pblapply(1:n.sim,function(i){
    c(t.test(rnorm(iN, mean = 0, sd = 1),
             rnorm(iN, mean = 0, sd = 1))$p.value,
      t.test(rt(iN, df = 4),
             rt(iN, df = 4))$p.value,
      t.test(rgamma(iN, shape = 2, rate = 1/2),
             rgamma(iN, shape = 2, rate = 1/2))$p.value,
      t.test(pmin(rnorm(iN, mean = 1, sd = 1),2.5),
             pmin(rnorm(iN, mean = 1, sd = 1),2.5))$p.value)
  }, cl = 10))
  out <- rbind(out,
               data.table(n = iN, sim = 1:n.sim, p.value = M.p[,1], example = "1. Gaussian"),
               data.table(n = iN, sim = 1:n.sim, p.value = M.p[,2], example = "2. Student"),
               data.table(n = iN, sim = 1:n.sim, p.value = M.p[,3], example = "3. Gamma"),
               data.table(n = iN, sim = 1:n.sim, p.value = M.p[,4], example = "4. Ceilling effect")
               )
}
#+end_src

#+begin_src R :exports none :results output :session *R* :cache no
dt.gg <- out[,.(rep = .N, type1=mean(p.value<=0.05)),by=c("n","example")]
ggCov <- ggplot(dt.gg, aes(x=n,y=type1,group=example,color=example))
ggCov <- ggCov + geom_abline(slope = 0, intercept =p 0.05, color="black") 
ggCov <- ggCov + geom_line() + geom_point() + coord_cartesian(ylim = c(0.03,0.055))
ggCov <- ggCov + ylab("type 1 error") + xlab("sample size")
ggCov <- ggCov + theme(text = element_text(size=10), 
                       axis.line = element_line(size = 1.25),
                       axis.ticks = element_line(size = 2),
                       axis.ticks.length=unit(.25, "cm"))
ggsave(ggCov, filename = "figures/examples-coverage.pdf", height = 5)
#+end_src

#+RESULTS:
: Saving 7 x 5 in image

\clearpage

** Issue 1: parameter of interest

By default, we generally use the mean to define our parameter of
interest. In our example the difference in mean between the two groups
meaning that we summarize the distribution of the outcome for each
group by its mean (also refered to as 'expected value') and then take the difference
between groups. This is somehow arbitrary, we could have used another
summary statistic like the standard deviation, the median (or any
other quantile), the mode, \ldots. However it is not completely arbitrary:
- it is *convenient* to model and compute: many estimators and softwares
  have been developped for modeling the mean. Also this can be done
  in a numerically stable and efficient way.
- it is a *natural* choice if the outcome is normally distributed as
  the mean and the variance fully characterize the distribution so no
  need to model other summary statistics. In particular, for normal
  distributions the mean is equal to the median and the mode of the
  distribution.
- it is *easy to interpret* if the outcome is normally distributed as
  it is the average but also most likely value.

When the distribution is not normal, the last two arguments might not
be true. While they approximately hold if the distribution is unimodal
and symmetric, they are not valid for asymetric or bimodal
distribution. For instance, the mean of a binary variable will
correspond to a value that is never observed! If we look at
autoref:fig:mmm, we can see that the mean is not the most likely value
(i.e. the mode). The median is slightly closer to the mode but does
not really provide a satisfactory improvement.

#+begin_src R :exports none :results output :session *R* :cache no
set.seed(10)

n <- 1e5
dt2 <- rbind(data.table(Y = rgamma(n, shape = 2, rate = 1/2), group = "C", example = "3. Gamma"),
             data.table(Y = 2*rgamma(n, shape = 2, rate = 1/2) + 1, group = "T", example = "3. Gamma"))
dt2S <- dt2[,.(mean = mean(Y), median =  median(Y), mode = as.numeric(names(which.max(table(round(Y,1)))))), by = c("group","example")]
ggMean <- ggplot(dt2, aes(x=Y,fill=group))
ggMean <- ggMean + geom_histogram(breaks = seq(0,20,by=0.1), alpha = 0.25, position = "dodge") + facet_wrap(~example)
ggMean <- ggMean + geom_vline(data = dt2S, aes(xintercept = mean, linetype = "mean", color = group), size = 2)
ggMean <- ggMean + geom_vline(data = dt2S, aes(xintercept = median, linetype = "median", color = group), size = 2)
ggMean <- ggMean + geom_vline(data = dt2S, aes(xintercept = mode, linetype = "mode", color = group), size = 2)
ggMean <- ggMean + facet_wrap(~group)
ggMean <- ggMean + coord_cartesian(xlim = c(0,15)) + ylab("Number of observations") + labs(linetype = "summary statistic")
ggMean <- ggMean + scale_linetype_manual(values = c("solid","longdash","dotted"))
ggMean <- ggMean + theme(text = element_text(size=12), 
                         axis.line = element_line(linewidth = 1.25),
                         axis.ticks = element_line(size = 2),
                         axis.ticks.length=unit(0.25, "cm"),
                         legend.key.size=unit(0.75, "cm"),
                         legend.key.height=unit(0.75, "cm"),
                         legend.position = "bottom")
ggsave(ggMean, filename = "figures/meanMedianMode.pdf", height = 4)
#+end_src

#+RESULTS:
: [1m[22mSaving 7 x 4 in image

#+name: fig:mmm
#+ATTR_LaTeX: :width 0.85\textwidth :placement [!h]
#+CAPTION: Mean, median, and mode two asymetric distributions.
[[./figures/meanMedianMode.pdf]]

\clearpage

 In such a case, it can be a good idea to define a new parameter of
interest. One could for instance apply a transformation that
normalizes the distribution (e.g. log-transformation, see
autoref:fig:logmean), estimate the mean of the transformed data (here
1.1 vs 1.8), and compare them across groups (here 0.7). In the case of
a *log-transformation*, the back-transformed difference has a nice
interpration: it is a multiplicative effect (exp(0.7)=2, i.e. the mean
in the treatment is twice larger than in the control group). So,
instead of studying an additive group effect (on the mean), *the
parameter of interest is a multiplicative group effect* (on the
mean). Technically this requires additional assumptions, such as
homoschedasticity, that are not discussed here.


#+begin_src R :exports none :results output :session *R* :cache no
  set.seed(10)

  n <- 1e5
  dt2 <- rbind(data.table(Y = rgamma(n, shape = 2, rate = 1/2), group = "C", example = "3. Gamma"),
               data.table(Y = 2*rgamma(n, shape = 2, rate = 1/2), group = "T", example = "3. Gamma"))
  dt2 <- rbind(dt2,
               dt2[,.(Y=log(Y),group=group,example="3. log(Gamma)")])

  dt2S <- dt2[example=="3. log(Gamma)",.(mean = mean(Y), median =  median(Y), mode = as.numeric(names(which.max(table(round(Y,1)))))), by = c("group","example")]
#  dt2S <- rbind(dt2S,
#                dt2S[,.(mean=exp(mean),median=exp(median),mode=exp(mode), example = "3. Gamma"),by=c("group")])
  ggLMean <- ggplot(dt2, aes(x=Y,fill=group))
  ggLMean <- ggLMean + geom_histogram(breaks = seq(-10,10,by=0.1), alpha = 0.25, position = "dodge") + facet_wrap(~example)
  ggLMean <- ggLMean + geom_vline(data = dt2S, aes(xintercept = mean, linetype = "mean", color = group), size = 2)
  ggLMean <- ggLMean + geom_vline(data = dt2S, aes(xintercept = median, linetype = "median", color = group), size = 2)
  ggLMean <- ggLMean + geom_vline(data = dt2S, aes(xintercept = mode, linetype = "mode", color = group), size = 2)
  ggLMean <- ggLMean + facet_wrap(~example)
  ggLMean <- ggLMean + coord_cartesian(xlim = c(-3,7)) + ylab("Number of observations") + labs(linetype = "summary statistic")
  ggLMean <- ggLMean + scale_linetype_manual(values = c("solid","longdash","dotted"))
  ggLMean <- ggLMean + theme(text = element_text(size=12), 
                             axis.line = element_line(size = 1.25),
                             axis.ticks = element_line(size = 2),
                             axis.ticks.length=unit(0.25, "cm"),
                             legend.key.size=unit(0.75, "cm"),
                             legend.key.height=unit(0.75, "cm"),
                             legend.position = "bottom")
ggsave(ggLMean, filename = "figures/logmean.pdf", height = 4)
#+end_src

#+RESULTS:
: [1m[22mSaving 7 x 4 in image

#+name: fig:logmean
#+ATTR_LaTeX: :width 0.85\textwidth :placement [!h]
#+CAPTION: Mean, median, and mode on the log-transformed data
[[./figures/logmean.pdf]]

\noindent There are other possible parameter of interest, e.g.:
- The *Mann-Whitney parameter* \(\Prob[X\geq Y] +
  \frac{1}{2}\Prob[X=Y]\): this is the probability that a randomly
  chosen individual from the active group has a larger value than a
  randomly individual from the control group. 
  + \(\rightarrow\) :: it is closely related to the
    Wilcoxon-Mann-Whitney test and the AUC
  + \(\rightarrow\) :: not (completely) straightforward causal
    interpretation citep:fay2018causal.
  + \(\rightarrow\) :: implementation: see the function =wmwTest= from
    the asht package \newline \Warning in presence of
    heteroschedasticity (variance that differs between groups) one
    should use another tool (see the BuyseTest package) 

- One could dichotomize the outcome to *compare the probability of a
  high outcome value* between the two groups. This can be relevant in
  presence of a important ceiling effect.
  + \(\rightarrow\) :: implementation: see the function
    =uncondExact2x2= from the exact2x2 package for comparing
    proportions.

\clearpage

** Issue 2: handling small samples

In small samples, traditional methods will not provide a very accurate
type 1 error control or coverage as illustrated in
autoref:fig:coverage.
- *permutation methods* can be used to obtain exact type 1 error control
  under exchangeability. Exchangeability is violated when
  testing a mean difference between the groups while there is a
  difference in variance. In such a case studentized permutation
  should be used instead citep:chung2016asymptotically. \newline
  \(\rightarrow\) this will produce valid p-values but no confidence
  intervals
- *bootstrap resampling methods* can be used to reduce the coverage
  error error in small samples. This includes studentized
  non-parametric bootstrap where the bootstrap test statistic is used
  to estimate the quantiles used in the confidence intervals (instead
  +/- 1.96) or bias-corrected and accelerated (BCa) bootstrap interval
  (see the boot package). \newline \Warning Not all bootstrap methods
  have good sample properties, e.g. the 'standard' non-parametric
  bootstrap using the quantiles of the boostrap distribution of the
  parameter of interest does not have very attractive small sample
  properties.
There are also analytic correction for improving the small sample
properties but there typically are specific to a statistical
model/test and are not discussed here.

** Issue 3: handling outliers

Most of the statistics will quantify some kind of average difference
between groups. One observation with a very large value may have large
influence on this average. If that is a concern, rank-based statistics
(e.g. median, Mann-Whitney parameter, probability of a high-value) may
be seen as more fair statistics in the sense that all observations
have the same weight on the summary statistic.
- \Warning :: Artificially reducing the outcome value (e.g. to be at
  most the mean plus 2 standard deviation) is generally a bad idea: it
  will induce a downward bias in the estimated mean and can lead to
  inflated type 1 error (if the probability of a large value is group
  dependent).

\clearpage



* References
#+LaTeX: \begingroup
#+LaTeX: \renewcommand{\section}[2]{}
bibliographystyle:apalike
[[bibliography:bibliography.bib]]
# help: https://gking.harvard.edu/files/natnotes2.pdf
#+LaTeX: \endgroup


* Reference :noexport:
# help: https://gking.harvard.edu/files/natnotes2.pdf

#+BEGIN_EXPORT latex
\begingroup
\renewcommand{\section}[2]{}
#+END_EXPORT
bibliographystyle:apalike
[[bibliography:bibliography.bib]] 
#+BEGIN_EXPORT latex
\endgroup
#+END_EXPORT

#+BEGIN_EXPORT LaTeX
\appendix
\titleformat{\section}
{\normalfont\Large\bfseries}{}{1em}{Appendix~\thesection:~}

\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\theequation}{\Alph{equation}}

\setcounter{figure}{0}    
\setcounter{table}{0}    
\setcounter{equation}{0}    

\setcounter{page}{1}
#+END_EXPORT

* CONFIG :noexport:
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+LaTeX_CLASS_OPTIONS: [12pt]
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Display of the document
# ## space between lines
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}

# ## margins
#+LATEX_HEADER:\geometry{top=1cm}

# ## personalize the prefix in the name of the sections
#+LaTeX_HEADER: \usepackage{titlesec}
# ## fix bug in titlesec version
# ##  https://tex.stackexchange.com/questions/299969/titlesec-loss-of-section-numbering-with-the-new-update-2016-03-15
#+LaTeX_HEADER: \usepackage{etoolbox}
#+LaTeX_HEADER: 
#+LaTeX_HEADER: \makeatletter
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
#+LaTeX_HEADER: \patchcmd{\ttlh@hang}{\noindent}{}{}{}
#+LaTeX_HEADER: \makeatother

** Color
# ## define new colors
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LaTeX_HEADER: \definecolor{myorange}{rgb}{1,0.2,0}
#+LaTeX_HEADER: \definecolor{mypurple}{rgb}{0.7,0,8}
#+LaTeX_HEADER: \definecolor{mycyan}{rgb}{0,0.6,0.6}
#+LaTeX_HEADER: \newcommand{\lightblue}{blue!50!white}
#+LaTeX_HEADER: \newcommand{\darkblue}{blue!80!black}
#+LaTeX_HEADER: \newcommand{\darkgreen}{green!50!black}
#+LaTeX_HEADER: \newcommand{\darkred}{red!50!black}
#+LaTeX_HEADER: \definecolor{gray}{gray}{0.5}

# ## change the color of the links
#+LaTeX_HEADER: \hypersetup{
#+LaTeX_HEADER:  citecolor=[rgb]{0,0.5,0},
#+LaTeX_HEADER:  urlcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER:  linkcolor=[rgb]{0,0,0.5},
#+LaTeX_HEADER: }

** Font
# https://tex.stackexchange.com/questions/25249/how-do-i-use-a-particular-font-for-a-small-section-of-text-in-my-document
#+LaTeX_HEADER: \newenvironment{note}{\small \color{gray}\fontfamily{lmtt}\selectfont}{\par}
#+LaTeX_HEADER: \newenvironment{activity}{\color{orange}\fontfamily{qzc}\selectfont}{\par}

** Symbols
# ## valid and cross symbols
#+LaTeX_HEADER: \RequirePackage{pifont}
#+LaTeX_HEADER: \RequirePackage{relsize}
#+LaTeX_HEADER: \newcommand{\Cross}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{56}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\Valid}{{\raisebox{-0.5ex}%
#+LaTeX_HEADER:		{\relsize{1.5}\ding{52}}}\hspace{1pt} }
#+LaTeX_HEADER: \newcommand{\CrossR}{ \textcolor{red}{\Cross} }
#+LaTeX_HEADER: \newcommand{\ValidV}{ \textcolor{green}{\Valid} }

# ## warning symbol
#+LaTeX_HEADER: \usepackage{stackengine}
#+LaTeX_HEADER: \usepackage{scalerel}
#+LaTeX_HEADER: \newcommand\Warning[1][3ex]{%
#+LaTeX_HEADER:   \renewcommand\stacktype{L}%
#+LaTeX_HEADER:   \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}%
#+LaTeX_HEADER:   \xspace
#+LaTeX_HEADER: }

# ## R logo
#+LATEX_HEADER:\definecolor{grayR}{HTML}{8A8990}
#+LATEX_HEADER:\definecolor{grayL}{HTML}{C4C7C9}
#+LATEX_HEADER:\definecolor{blueM}{HTML}{1F63B5}
#+LATEX_HEADER: \newcommand{\Rlogo}[1][0.07]{
#+LATEX_HEADER: \begin{tikzpicture}[scale=#1]
#+LATEX_HEADER: \shade [right color=grayR,left color=grayL,shading angle=60] 
#+LATEX_HEADER: (-3.55,0.3) .. controls (-3.55,1.75) 
#+LATEX_HEADER: and (-1.9,2.7) .. (0,2.7) .. controls (2.05,2.7)  
#+LATEX_HEADER: and (3.5,1.6) .. (3.5,0.3) .. controls (3.5,-1.2) 
#+LATEX_HEADER: and (1.55,-2) .. (0,-2) .. controls (-2.3,-2) 
#+LATEX_HEADER: and (-3.55,-0.75) .. cycle;
#+LATEX_HEADER: 
#+LATEX_HEADER: \fill[white] 
#+LATEX_HEADER: (-2.15,0.2) .. controls (-2.15,1.2) 
#+LATEX_HEADER: and (-0.7,1.8) .. (0.5,1.8) .. controls (2.2,1.8) 
#+LATEX_HEADER: and (3.1,1.2) .. (3.1,0.2) .. controls (3.1,-0.75) 
#+LATEX_HEADER: and (2.4,-1.45) .. (0.5,-1.45) .. controls (-1.1,-1.45) 
#+LATEX_HEADER: and (-2.15,-0.7) .. cycle;
#+LATEX_HEADER: 
#+LATEX_HEADER: \fill[blueM] 
#+LATEX_HEADER: (1.75,1.25) -- (-0.65,1.25) -- (-0.65,-2.75) -- (0.55,-2.75) -- (0.55,-1.15) -- 
#+LATEX_HEADER: (0.95,-1.15)  .. controls (1.15,-1.15) 
#+LATEX_HEADER: and (1.5,-1.9) .. (1.9,-2.75) -- (3.25,-2.75)  .. controls (2.2,-1) 
#+LATEX_HEADER: and (2.5,-1.2) .. (1.8,-0.95) .. controls (2.6,-0.9) 
#+LATEX_HEADER: and (2.85,-0.35) .. (2.85,0.2) .. controls (2.85,0.7) 
#+LATEX_HEADER: and (2.5,1.2) .. cycle;
#+LATEX_HEADER: 
#+LATEX_HEADER: \fill[white]  (1.4,0.4) -- (0.55,0.4) -- (0.55,-0.3) -- (1.4,-0.3).. controls (1.75,-0.3) 
#+LATEX_HEADER: and (1.75,0.4) .. cycle;
#+LATEX_HEADER: 
#+LATEX_HEADER: \end{tikzpicture}
#+LATEX_HEADER: }

** Code
# Documentation at https://org-babel.readthedocs.io/en/latest/header-args/#results
# :tangle (yes/no/filename) extract source code with org-babel-tangle-file, see http://orgmode.org/manual/Extracting-source-code.html 
# :cache (yes/no)
# :eval (yes/no/never)
# :results (value/output/silent/graphics/raw/latex)
# :export (code/results/none/both)
#+PROPERTY: header-args :session *R* :tangle yes :cache no ## extra argument need to be on the same line as :session *R*

# Code display:
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

# ## change font size input (global change)
# ## doc: https://ctan.math.illinois.edu/macros/latex/contrib/listings/listings.pdf
# #+LATEX_HEADER: \newskip\skipamount   \skipamount =6pt plus 0pt minus 6pt
# #+LATEX_HEADER: \lstdefinestyle{code-tiny}{basicstyle=\ttfamily\tiny, aboveskip =  kipamount, belowskip =  kipamount}
# #+LATEX_HEADER: \lstset{style=code-tiny}
# ## change font size input (local change, put just before BEGIN_SRC)
# ## #+ATTR_LATEX: :options basicstyle=\ttfamily\scriptsize
# ## change font size output (global change)
# ## \RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\tiny,formatcom = {\color[rgb]{0.5,0,0}}}

** Lists
#+LATEX_HEADER: \RequirePackage{enumitem} % better than enumerate

** Image and graphs
#+LATEX_HEADER: \usepackage{float} %figure inside minipage
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of} % 
#+LATEX_HEADER: \RequirePackage{caption} % newlines in graphics

#+LaTeX_HEADER: \RequirePackage{tikz-cd} % graph
# ## https://tools.ietf.org/doc/texlive-doc/latex/tikz-cd/tikz-cd-doc.pdf

** Table
#+LATEX_HEADER: \RequirePackage{booktabs} % for nice lines in table (e.g. toprule, bottomrule, midrule, cmidrule)

** Inline latex
# @@latex:any arbitrary LaTeX code@@


** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
# #+LaTeX_HEADER: \RequirePackage{amsthm}
# #+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
# #+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \usepackage{ifthen}
#+LATEX_HEADER: \usepackage{xifthen}
#+LATEX_HEADER: \usepackage{xargs}
#+LATEX_HEADER: \usepackage{xspace}

#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

**** Probability
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\logLikelihood[2][1=,2=]{\defOperator{#1}{#2}{\ell}{}{(}{)}{}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\partial #2^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}

#+LATEX_HEADER: \newcommand\Veta{\boldsymbol{\eta}}
#+LATEX_HEADER: \newcommand\VX{\mathbf{X}}
#+LATEX_HEADER: \newcommand\sample{\chi}
#+LATEX_HEADER: \newcommand\Hspace{\mathcal{H}}
#+LATEX_HEADER: \newcommand\Tspace{\mathcal{T}}


** Notations
