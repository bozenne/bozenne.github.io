#+TITLE: Estimating a relative change using a log-transformation of the outcome
#+Author: Brice Ozenne

* Result
Let's denote by \(Y\) the outcome and by \(G\) a group variable
\(G\) (binary variable). We are interested in the relative change in \(Y\) between the
groups. We decide to model the group effect on the log scale:
#+BEGIN_EXPORT latex
\begin{align*}
\log(Y) = Z = \alpha + \beta G + \varepsilon \text{ where } \varepsilon \sim \Gaus[0,\sigma^2]
\end{align*}
#+END_EXPORT
We claim that:
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]} = e^{\beta} - 1
\end{align*}
#+END_EXPORT

* Proof

** Re-writting the model as a multiplicative model
We can re-write the model as:
#+BEGIN_EXPORT latex
\begin{align*}
Y = e^{\alpha + \beta G}e^{\varepsilon} \text{ where } \varepsilon \sim \Gaus[0,\sigma^2]
\end{align*}
#+END_EXPORT
So for \(g\in\{1,2\}\):
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[Y|G=g] = e^{\alpha + \beta g} \Esp[e^{\varepsilon}]
\end{align*}
#+END_EXPORT
Then:
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]}
& = \frac{e^{\alpha + \beta} \Esp[e^{\varepsilon}]-e^{\alpha} \Esp[e^{\varepsilon}]}{e^{\alpha} \Esp[e^{\varepsilon}]} \\
& = \frac{e^{\alpha + \beta} -e^{\alpha}}{e^{\alpha}}  = e^{\beta} - 1 \\
\end{align*}
#+END_EXPORT

** Using a Taylor expansion


Using a second order Taylor expansion of \(\exp(Z)\) around
\(\mu(G)=\alpha + \beta G\) and assuming that the first moments of
\(Z\) are finite and the remaining moments are neglectable regarding
the factorial of the moment order (i.e. \(\forall i \geq 1 \),
\(\frac{1}{i!}\Esp[\varepsilon^i ]< +\infty\) and \(\sum_{i=1}^{\infty} \frac{1}{i!}\Esp[\varepsilon^i ]< +\infty\)), we get:
#+BEGIN_EXPORT latex
\begin{align*}
Y &= e^{Z} = e^{\mu} + \sum_{i=1}^{\infty} \frac{1}{i!} (Z - \mu)^i \frac{\partial^i e^{\mu}}{(\partial \mu)^i} \\
&= e^{\alpha + \beta G} + \sum_{i=1}^{\infty} \frac{1}{i!} (Z - \alpha - \beta G)^i e^{\alpha + \beta G} \\
\Esp[Y|G=g] &= e^{\alpha + \beta G} + \sum_{i=1}^{\infty} \frac{1}{i!} \Esp[(Z - \alpha - \beta g)^i] e^{\alpha + \beta G} \\
&= e^{\alpha + \beta G} \left(1 + \sum_{i=1}^{\infty} \frac{1}{i!} \Esp[\varepsilon^i] \right)
\end{align*}
#+END_EXPORT
where we used that the distribution of \(\varepsilon\) is independent
of \(g\). [Optional] \(\varepsilon\) follows a zero-mean normal distribution, so
the uneven moments are 0:
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[Y|G=g] &= e^{\alpha + \beta G} \left(1 + \sum_{i=1}^{\infty} \frac{1}{2i!} \Esp[\varepsilon^{2i}] \right)
\end{align*}
#+END_EXPORT
We can now express our parameter of interest:
#+BEGIN_EXPORT latex
\begin{align*}
\Delta_G &= \frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]} = \frac{\Esp[Y|G=1]}{\Esp[Y|G=0]} - 1 \\
&= \frac{e^{\alpha + \beta} \left(1 + \sum_{i=1}^{\infty} \frac{1}{2i!} \Esp[\varepsilon^{2i}] \right)}{e^{\alpha} \left(1 + \sum_{i=1}^{\infty} \frac{1}{2i!} \Esp[\varepsilon^{2i}] \right)} - 1 \\
&= e^{\beta} - 1
\end{align*}
#+END_EXPORT


# @@latex:any arbitrary LaTeX code@@
\clearpage

* Example :noexport:

Simulate data:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(lava)
m <- lvm(Y[5] ~ G)
categorical(m, K=2) <- ~G
transform(m, Z~Y) <- function(z){log(z)}

d <- lava::sim(m, n = 1e5)
head(d)
#+END_SRC

#+RESULTS:
:          Y G        Z
: 1 4.941076 1 1.597583
: 2 4.184619 0 1.431416
: 3 4.757324 0 1.559685
: 4 5.596557 1 1.722152
: 5 5.368230 0 1.680498
: 6 5.668698 0 1.734960

Fit models:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
coef.id <- coef(lm(Y ~ G, data = d))
coef.log <- coef(lm(Z ~ G, data = d))

list(id = coef.id,
     log = coef.log)
#+END_SRC

#+RESULTS:
: $id
: (Intercept)           G 
:   5.0035836   0.9923204 
: 
: $log
: (Intercept)           G 
:   1.5888092   0.1879317

Relative change estimated by several methods:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
c(id = as.double(coef.id["G"]/coef.id["(Intercept)"]), 
  log = as.double(exp(coef.log["G"])-1), 
  GS = as.double(mean(d[d$G==1,"Y"])/mean(d[d$G==0,"Y"]) - 1),
  true = 1/5)
#+END_SRC

#+RESULTS:
:        id       log        GS      true 
: 0.1983219 0.2067510 0.1983219 0.2000000

Performance in small samples:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
warper <- function(m, n){
    d <- lava::sim(m, n = n)
    coef.id <- coef(lm(Y ~ G, data = d))
    coef.log <- coef(lm(Z ~ G, data = d))
    out <- c(id = as.double(coef.id["G"]/coef.id["(Intercept)"]), 
             log = as.double(exp(coef.log["G"])-1))
    return(out)
}
M.res <- do.call(rbind,lapply(1:1000, function(i){warper(m, n = 12)}))
#+END_SRC

#+RESULTS:

Bias:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans(M.res-1/5)
#+END_SRC

#+RESULTS:
:         id        log 
: 0.01062298 0.01824621

Variance:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
apply(M.res,2,var)
#+END_SRC

#+RESULTS:
:         id        log 
: 0.01973720 0.02136166

Root mean squared error:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans((M.res-1/5)^2)
#+END_SRC

#+RESULTS:
:  change.id change.log 
: 0.01946796 0.02202432

In this simulation, the change computed with the log model has a
slightly larger bias and variance, with a quite similar root mean
squared error are quite similar. Here the true model was the additive
one (i.e. no tranformation) but we see that the multiplicative
one(i.e. log-transformation) gives valid results (even though the
distribution of the residuals is not normal on the log-scale). So the
model choice should be made on which of the two models: additive or
multiplicative is more likely to be correctly specified.

\clearpage

* Note for power calculation

** Recall: delta-method for normally distributed variables

\textbf{Theory}: we recall that for a random variable \(Y\) with finite first two
moments, the delta method applied around the mean for a transformation
\(f\) is:
#+BEGIN_EXPORT latex
\begin{align*}
f(Y) = f(\mu) + f'(\mu) (Y-\mu) + \frac{1}{2} f''(\mu) (Y-\mu)^2  + \frac{1}{6} f'''(\mu) (Y-\mu)^3 + o\left((Y-\mu)^2\right)
\end{align*}
#+END_EXPORT
where \(\mu=\Esp[Y]\). Introducing \(\sigma^2 = \Var[Y]\), we have:
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[f(Y)] =& f(\mu) + f'(\mu) (\Esp[Y]-\mu) + \frac{1}{2} f''(\mu) \Esp[(Y-\mu)^2] + \frac{1}{6} f'''(\mu) \Esp[(Y-\mu)^3] + o\left(\Esp[(Y-\mu)^3]\right) \\
=& f(\mu) + \frac{\sigma^2}{2} f''(\mu)  + o\left(\Esp[(Y-\mu)^3]\right)
\end{align*}
#+END_EXPORT
for a normal distribution since \(\Esp[(Y-\mu)^3]=0\). Also:
#+BEGIN_EXPORT latex
\begin{align*}
\Var[f(Y)] =& \left(f'(\mu)\right)^2 \Var\left[\Esp[Y]-\mu\right] + f'(\mu)f''(\mu) \Esp[(Y-\mu)^3] \\
& +\left(\frac{f'(\mu) f'''(\mu)}{3} + \frac{\left(f''(\mu)\right)^2}{4}\right) \Esp[(Y-\mu)^4] + o\left(\Esp[(Y-\mu)^4]\right) \\
=& \left(f'(\mu)\right)^2 \sigma^2 + 3 \sigma^4 \left(\frac{f'(\mu) f'''(\mu)}{3} + \frac{\left(f''(\mu)\right)^2}{4}\right) + o\left(\Esp[(Y-\mu)^4]\right) \\
\end{align*}
#+END_EXPORT

\bigskip

\textbf{Application}:  exponential transformation (\(f = \exp\))
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[\exp(Y)] &\approx \exp(\mu)\left(1 + \frac{\sigma^2}{2}\right) \\
\Var[\exp(Y)] &\approx \exp(2\mu)\left(\sigma^2 + \frac{7}{4} \sigma^4\right) \\
\end{align*}
#+END_EXPORT

\textbf{Illustration}: 
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
n <- 1e4
mu <- 0.1
sigma2 <- 0.1
X <- rnorm(n, mean = mu, sd = sqrt(sigma2))
fX <- exp(X)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
## first order
c(error_mean = mean(fX) - exp(mu), 
  errorPC_mean = 100*(mean(fX) - exp(mu))/mean(fX))
c(error_var = var(fX) - exp(2*mu)*sigma2, 
  errorPC_var = 100*(var(fX) - exp(2*mu)*sigma2)/var(fX))
#+END_SRC

#+RESULTS:
:   error_mean errorPC_mean 
:   0.05783048   4.97252058
:   error_var errorPC_var 
:  0.02343271 16.09687872

#+BEGIN_SRC R :exports both :results output :session *R* :cache no
## second order
c(mean = mean(fX),
  error_mean = mean(fX) - exp(mu)*(1+sigma2/2), 
  errorPC_mean = 100*(mean(fX) - exp(mu)*(1+sigma2/2))/mean(fX))
c(var = var(fX),
  error_var = var(fX) - exp(2*mu)*(sigma2 + (7/4)*sigma2^2), 
  errorPC_var = 100*(var(fX) - exp(2*mu)*(sigma2 + (7/4)*sigma2^2))/var(fX))
#+END_SRC

#+RESULTS:
:         mean   error_mean errorPC_mean 
:  1.163001402  0.002571938  0.221146614
:         var   error_var errorPC_var 
: 0.145572982 0.002058158 1.413832496

** Two independent groups

\textbf{Theory}: consider two groups \(G=0\) and \(G=1\) for which we want to compare
the percentage difference in outcome \(Y\). Our parameter of interest
is:
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]} = \gamma
\end{align*}
#+END_EXPORT
and we assume that on the original scale:
#+BEGIN_EXPORT latex
\begin{align*}
\Var[Y] = \Var[Y|G=1]  = \Var[Y|G=0] = \sigma_Y^2
\end{align*}
#+END_EXPORT
and
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[Y|G=0] = \alpha_Y
\end{align*}
#+END_EXPORT

\bigskip

We only assume that the outcome is normally distribution after log
transformation, i.e. \(\log(Y) \sim
\Gaus\left(a_0,s^2_0\right)\) in the first group
and \(\log(Y) \sim
\Gaus\left(a_1,s^2_1\right)\). We can use the
delta method to identify these parameters:
#+BEGIN_EXPORT latex
\begin{align*}
\alpha_Y &= \exp(a_0)\left(1 + \frac{s^2_0}{2}\right) \\
\sigma^2_Y &= \exp(2 a_0)\left(s^2_0 + \frac{7}{4}s^4_0\right) \\
\alpha_Y (\gamma+1) &= \exp(a_1)\left(1 + \frac{s^2_1}{2}\right) \\
\sigma^2_Y &= \exp(2 a_1)\left(s^2_1 + \frac{7}{4}s^4_1\right)
\end{align*}
#+END_EXPORT
i.e.
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\alpha^2_Y}{\sigma_Y^2} &= \frac{\left(1-\frac{s_0^2}{2}\right)^2}{s^2_0 + \frac{7}{4}s^4_0}  \\
a_0 &= \frac{1}{2}\log\left(\frac{\sigma^2_Y}{\left(s^2_0 + \frac{7}{4}s^4_0\right)}\right) \\
\frac{\alpha^2_Y(\gamma+1)^2}{\sigma_Y^2} &= \frac{\left(1-\frac{s_1^2}{2}\right)^2}{s^2_1 + \frac{7}{4}s^4_1}  \\
a_1 &= \frac{1}{2}\log\left(\frac{\sigma^2_Y}{\left(s^2_1 + \frac{7}{4}s^4_1\right)}\right) 
\end{align*}
#+END_EXPORT 
The first and third equation can be solved numerically.

\bigskip

\textbf{Illustration}:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
alpha_Y <- 1.15
sigma2_Y <- 0.15

s <- uniroot(function(x){alpha_Y^2/sigma2_Y - (1+x/2)^2/(x+x^2*7/4)},
               interval = c(0,1))$root
a <- log(sigma2_Y/(s+s^2*7/4))/2
c(a = a, s = s)
#+END_SRC

#+RESULTS:
:          a          s 
: 0.08802784 0.10608948



We can check that:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
c(exp(a)*(1+s/2), exp(2*a)*(s+s^2*7/4))
#+END_SRC

#+RESULTS:
: [1] 1.149944 0.150000

i.e.
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Z <- rnorm(1e4, mean=a, sd = sqrt(s))
mean(exp(Z))
var(exp(Z))
#+END_SRC

#+RESULTS:
: [1] 1.152237
: [1] 0.1496768

\textbf{Note}: an alternative approach is to use a log-normal distribution with
parameters:
#+BEGIN_EXPORT latex
\begin{align*}
s^2 =&\log\left(1+\frac{\sigma^2}{\alpha^2}\right) \\
a =&\log(\alpha) - \frac{s^2}{2} 
\end{align*}
#+END_EXPORT
Here it gives:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
s <- log(1+sigma2_Y/alpha_Y^2)
a <- log(alpha_Y) - s/2
c(a = a, s = s)
#+END_SRC

#+RESULTS:
:          a          s 
: 0.08604307 0.10743775

We can check that:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
exp(a + s/2) - alpha_Y
(exp(s)-1)*exp(2*a + s) - sigma2_Y
#+END_SRC

#+RESULTS:
: [1] 0
: [1] -5.551115e-17

and
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Y <- rlnorm(1e4, meanlog=a, sdlog = sqrt(s))
mean(Y)
var(Y)
#+END_SRC

#+RESULTS:
: [1] 1.146438
: [1] 0.1462818

* CONFIG :noexport:
# #+LaTeX_HEADER:\affil{Department of Biostatistics, University of Copenhagen, Copenhagen, Denmark}
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+LaTeX_CLASS_OPTIONS: [12pt]
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Latex command
#+LATEX_HEADER: \RequirePackage{ifthen}
#+LATEX_HEADER: \RequirePackage{xifthen}
#+LATEX_HEADER: \RequirePackage{xargs}
#+LATEX_HEADER: \RequirePackage{xspace}

#+LATEX_HEADER: \newcommand\Rlogo{\textbf{\textsf{R}}\xspace} % 

** Notations

** Code
# Documentation at https://org-babel.readthedocs.io/en/latest/header-args/#results
# :tangle (yes/no/filename) extract source code with org-babel-tangle-file, see http://orgmode.org/manual/Extracting-source-code.html 
# :cache (yes/no)
# :eval (yes/no/never)
# :results (value/output/silent/graphics/raw/latex)
# :export (code/results/none/both)
#+PROPERTY: header-args :session *R* :tangle yes :cache no ## extra argument need to be on the same line as :session *R*

# Code display:
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

# ## change font size input
# ## #+ATTR_LATEX: :options basicstyle=\ttfamily\scriptsize
# ## change font size output
# ## \RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\tiny,formatcom = {\color[rgb]{0.5,0,0}}}

** Display 
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\usepackage{authblk} % enable several affiliations (clash with beamer)
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}
#+LATEX_HEADER:\geometry{top=1cm}

** Image
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of} % 
#+LATEX_HEADER: \RequirePackage{caption} % newlines in graphics


** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
# #+LaTeX_HEADER: \RequirePackage{amsthm}
# #+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
# #+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

*** Shortcuts

**** Probability
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}
