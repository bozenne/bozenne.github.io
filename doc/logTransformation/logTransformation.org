#+TITLE: Estimating a relative change using a log-transformation of the outcome
#+Author: Brice Ozenne

* Interpretation of the regression coefficient after log-transformation
Let's denote by \(Y\) the outcome and by \(G\) a binary group
variable. We are interested in the relative change in \(Y\) between
the groups. We decide to model the group effect on the log scale:
#+BEGIN_EXPORT latex
\begin{align*}
\log(Y) = Z = \alpha + \beta G + \varepsilon \text{ where } \Esp[\varepsilon]=0 \text{ and } \Esp[\varepsilon]=\sigma^2
\end{align*}
#+END_EXPORT
We claim that:
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]} = e^{\beta} - 1
\end{align*}
#+END_EXPORT

** Proof: re-writting the model as a multiplicative model
We can re-write the model as:
#+BEGIN_EXPORT latex
\begin{align*}
Y = e^{\alpha + \beta G}e^{\varepsilon} \text{ where }
\end{align*}
#+END_EXPORT
So for \(g\in\{1,2\}\):
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[Y|G=g] = e^{\alpha + \beta g} \Esp[e^{\varepsilon}]
\end{align*}
#+END_EXPORT
Then:
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]}
& = \frac{e^{\alpha + \beta} \Esp[e^{\varepsilon}]-e^{\alpha} \Esp[e^{\varepsilon}]}{e^{\alpha} \Esp[e^{\varepsilon}]} \\
& = \frac{e^{\alpha + \beta} -e^{\alpha}}{e^{\alpha}}  = e^{\beta} - 1 \\
\end{align*}
#+END_EXPORT

** Proof: using a Taylor expansion

Using a second order Taylor expansion of \(\exp(Z)\) around
\(\mu(G)=\alpha + \beta G\) and assuming that the first moments of
\(Z\) are finite and the remaining moments are neglectable regarding
the factorial of the moment order (i.e. \(\forall i \geq 1 \),
\(\frac{1}{i!}\Esp[\varepsilon^i ]< +\infty\) and \(\sum_{i=1}^{\infty} \frac{1}{i!}\Esp[\varepsilon^i ]< +\infty\)), we get:
#+BEGIN_EXPORT latex
\begin{align*}
Y &= e^{Z} = e^{\mu} + \sum_{i=1}^{\infty} \frac{1}{i!} (Z - \mu)^i \frac{\partial^i e^{\mu}}{(\partial \mu)^i} \\
&= e^{\alpha + \beta G} + \sum_{i=1}^{\infty} \frac{1}{i!} (Z - \alpha - \beta G)^i e^{\alpha + \beta G} \\
\Esp[Y|G=g] &= e^{\alpha + \beta G} + \sum_{i=1}^{\infty} \frac{1}{i!} \Esp[(Z - \alpha - \beta g)^i] e^{\alpha + \beta G} \\
&= e^{\alpha + \beta G} \left(1 + \sum_{i=1}^{\infty} \frac{1}{i!} \Esp[\varepsilon^i] \right)
\end{align*}
#+END_EXPORT
where we used that the distribution of \(\varepsilon\) is independent
of \(g\). We can now express our parameter of interest:
#+BEGIN_EXPORT latex
\begin{align*}
\Delta_G &= \frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]} = \frac{\Esp[Y|G=1]}{\Esp[Y|G=0]} - 1 \\
&= \frac{e^{\alpha + \beta} \left(1 + \sum_{i=1}^{\infty} \frac{1}{i!} \Esp[\varepsilon^{i}] \right)}{e^{\alpha} \left(1 + \sum_{i=1}^{\infty} \frac{1}{i!} \Esp[\varepsilon^{i}] \right)} - 1 \\
&= e^{\beta} - 1
\end{align*}
#+END_EXPORT


# @@latex:any arbitrary LaTeX code@@
\clearpage

* Example :noexport:

Simulate data:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
library(lava)
m <- lvm(Y[5] ~ G)
categorical(m, K=2) <- ~G
transform(m, Z~Y) <- function(z){log(z)}

d <- lava::sim(m, n = 1e5)
head(d)
#+END_SRC

#+RESULTS:
:          Y G        Z
: 1 4.941076 1 1.597583
: 2 4.184619 0 1.431416
: 3 4.757324 0 1.559685
: 4 5.596557 1 1.722152
: 5 5.368230 0 1.680498
: 6 5.668698 0 1.734960

Fit models:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
coef.id <- coef(lm(Y ~ G, data = d))
coef.log <- coef(lm(Z ~ G, data = d))

list(id = coef.id,
     log = coef.log)
#+END_SRC

#+RESULTS:
: $id
: (Intercept)           G 
:   5.0035836   0.9923204 
: 
: $log
: (Intercept)           G 
:   1.5888092   0.1879317

Relative change estimated by several methods:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
c(id = as.double(coef.id["G"]/coef.id["(Intercept)"]), 
  log = as.double(exp(coef.log["G"])-1), 
  GS = as.double(mean(d[d$G==1,"Y"])/mean(d[d$G==0,"Y"]) - 1),
  true = 1/5)
#+END_SRC

#+RESULTS:
:        id       log        GS      true 
: 0.1983219 0.2067510 0.1983219 0.2000000

Performance in small samples:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
warper <- function(m, n){
    d <- lava::sim(m, n = n)
    coef.id <- coef(lm(Y ~ G, data = d))
    coef.log <- coef(lm(Z ~ G, data = d))
    out <- c(id = as.double(coef.id["G"]/coef.id["(Intercept)"]), 
             log = as.double(exp(coef.log["G"])-1))
    return(out)
}
M.res <- do.call(rbind,lapply(1:1000, function(i){warper(m, n = 12)}))
#+END_SRC

#+RESULTS:

Bias:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans(M.res-1/5)
#+END_SRC

#+RESULTS:
:         id        log 
: 0.01062298 0.01824621

Variance:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
apply(M.res,2,var)
#+END_SRC

#+RESULTS:
:         id        log 
: 0.01973720 0.02136166

Root mean squared error:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
colMeans((M.res-1/5)^2)
#+END_SRC

#+RESULTS:
:  change.id change.log 
: 0.01946796 0.02202432

In this simulation, the change computed with the log model has a
slightly larger bias and variance, with a quite similar root mean
squared error are quite similar. Here the true model was the additive
one (i.e. no tranformation) but we see that the multiplicative
one(i.e. log-transformation) gives valid results (even though the
distribution of the residuals is not normal on the log-scale). So the
model choice should be made on which of the two models: additive or
multiplicative is more likely to be correctly specified.

\clearpage

* Note for power calculation

** Recall: delta-method for normally distributed variables

\textbf{Theory}: we recall that for a random variable \(Y\) with finite first two
moments, the delta method applied around the mean for a transformation
\(f\) is:
#+BEGIN_EXPORT latex
\begin{align*}
f(Y) = f(\mu_Y) + f'(\mu_Y) (Y-\mu_Y) + \frac{1}{2} f''(\mu_Y) (Y-\mu_Y)^2  + \frac{1}{6} f'''(\mu_Y) (Y-\mu_Y)^3 + o\left((Y-\mu_Y)^2\right)
\end{align*}
#+END_EXPORT
where \(\mu_Y=\Esp[Y]\). Introducing \(\sigma_Y^2 = \Var[Y]\) and using
that for a normal distribution \(\Esp[(Y-\mu_Y)^3]=0\), we have:
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[f(Y)] =& f(\mu_Y) + f'(\mu_Y) (\Esp[Y]-\mu_Y) + \frac{1}{2} f''(\mu_Y) \Esp[(Y-\mu_Y)^2] \\ &+ \frac{1}{6} f'''(\mu_Y) \Esp[(Y-\mu_Y)^3] + o\left(\Esp[(Y-\mu_Y)^3]\right) \\
=& f(\mu_Y) + \frac{\sigma_Y^2}{2} f''(\mu_Y)  + o\left(\Esp[(Y-\mu_Y)^3]\right)
\end{align*}
#+END_EXPORT
Similarly using that for a normal distribution \(\Esp[(Y-\mu_Y)^4]=3\sigma_Y^4\):
#+BEGIN_EXPORT latex
\begin{align*}
\Var[f(Y)] =& \left(f'(\mu_Y)\right)^2 \Var\left[\Esp[Y]-\mu_Y\right] + f'(\mu_Y)f''(\mu_Y) \Esp[(Y-\mu_Y)^3] \\
& +\left(\frac{f'(\mu_Y) f'''(\mu_Y)}{3} + \frac{\left(f''(\mu_Y)\right)^2}{4}\right) \Esp[(Y-\mu_Y)^4] + o\left(\Esp[(Y-\mu_Y)^4]\right) \\
=& \left(f'(\mu_Y)\right)^2 \sigma_Y^2 + 3 \sigma_Y^4 \left(\frac{f'(\mu_Y) f'''(\mu_Y)}{3} + \frac{\left(f''(\mu_Y)\right)^2}{4}\right) + o\left(\Esp[(Y-\mu_Y)^4]\right) 
\end{align*}
#+END_EXPORT
and introducing \(X\) with mean \(\mu_X\), variance \(\sigma_X^2\), and correlation \(\rho\) with \(Y\):
#+BEGIN_EXPORT latex
\begin{align*}
\Cov[f(X),f(Y)] =& f'(\mu_X) f'(\mu_Y) \Cov[X-\mu_X,Y-\mu_Y] \\ &+ \frac{1}{4} f''(\mu_X) f''(\mu_Y) \Cov[(X-\mu_X)^2,(Y-\mu_Y)^2] + o\left(\Cov[(X-\mu_X)^2,(Y-\mu_Y)^2]\right)
\end{align*}
#+END_EXPORT

\bigskip

\textbf{Application}:  exponential transformation (\(f = \exp\))

\bigskip

Using that \(\Cov[(X-\mu_X)^2,(Y-\mu_Y)^2]\approx2 \rho^2 \sigma_X^2 \sigma_Y^2\):
#+BEGIN_EXPORT latex
\begin{align*}
\Esp[\exp(Y)] &\approx \exp(\mu_Y)\left(1 + \frac{\sigma_Y^2}{2}\right) \\
\Var[\exp(Y)] &\approx \exp(2\mu_Y)\left(\sigma_Y^2 + \frac{7}{4} \sigma_Y^4\right)\\
\Cov[\exp(X),\exp(Y)] &\approx \exp(\mu_X+\mu_Y)\left(\rho \sigma_X \sigma_Y + \frac{1}{2} \rho^2 \sigma_X^2 \sigma_Y^2\right) 
%\Cor[\exp(X),\exp(Y)] &\approx \rho + \frac{1}{2} \rho^2 \sigma_X \sigma_Y
\end{align*}
#+END_EXPORT
\Warning these approximations are precise when the mean and variance are small

\bigskip


\textbf{Illustration}: We consider a normally distributed outcome with
expectation 0.1 and variance 0.1. What is its expectation and variance
after exp-transformation?
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10); n <- 1e4
mu <- 0.1; sigma2 <- 0.1

## first order method
mu.exp1 <- exp(mu)
var.exp1 <- exp(2*mu)*sigma2

## second order method
mu.exp2 <- exp(mu)*(1+sigma2/2)
var.exp2 <- exp(2*mu)*(sigma2 + (7/4)*sigma2^2)

## empirical value
X.exp <- exp(rnorm(n, mean = mu, sd = sqrt(sigma2)))
mu.expGS <- mean(X.exp)
var.expGS <-  var(X.exp)
#+END_SRC

#+RESULTS:

Comparison mean:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
rbind(value = c(first.order = mu.exp1, 
                second.order = mu.exp2, 
                truth = mu.expGS),
      bias = c(mu.exp1,mu.exp2,mu.expGS)-mu.expGS,
      relative.bias = (c(mu.exp1,mu.exp2,mu.expGS)-mu.expGS)/mu.expGS)
#+END_SRC

#+RESULTS:
:               first.order second.order    truth
: value           1.1051709   1.38146365 1.425308
: bias           -0.3201366  -0.04384390 0.000000
: relative.bias  -0.2246088  -0.03076101 0.000000

Comparison variance:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
rbind(value = c(first.order = var.exp1, 
                second.order = var.exp2, 
                truth = var.expGS),
      bias = c(var.exp1,var.exp2,var.expGS)-var.expGS,
      relative.bias = (c(var.exp1,var.exp2,var.expGS)-var.expGS)/var.expGS)
#+END_SRC

#+RESULTS:
:               first.order second.order   truth
: value           0.6107014    1.1450651 1.35949
: bias           -0.7487890   -0.2144253 0.00000
: relative.bias  -0.5507865   -0.1577248 0.00000

The second order estimate is much more accurate, especially for the
variance.

\clearpage

We now consider a bivariate normally distributed outcome with
expectation 0.1, variance 0.1, and correlation 0.5. What is the
correlation after exp-transformation?
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
set.seed(10); n <- 1e4
mu <- c(0.1,0.1); sigma2 <- c(0.1,0.1); rho <- 0.5
Sigma <- matrix(c(sigma2[1],
                  rho*sqrt(prod(sigma2)),
                  rho*sqrt(prod(sigma2)),
                  sigma2[2]),
                2,2)
XY <- mvtnorm::rmvnorm(n, mean = mu, sigma = Sigma)
X <- XY[,1] ; Y <- XY[,2]

cov(exp(X),exp(Y))
exp(mean(X)+2*mean(Y)) * (cor(X,Y)*sd(Y)*sd(X) + 0.5*cor(X,Y)^2*var(Y)*var(X))
#+END_SRC

#+RESULTS:
: [1] 0.06839007
: [1] 0.06846545


\clearpage

** Two independent groups - normal distribution

\textbf{Theory}: consider two groups \(G=0\) and \(G=1\) for which we
want to compare the percentage difference in outcome \(Y\). We are
willing to assume that on the log-scale \(Y\) is normally
distributed. Our parameter of interest is:
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\Esp[Y|G=1]-\Esp[Y|G=0]}{\Esp[Y|G=0]} = \gamma
\end{align*}
#+END_EXPORT
we denote \(\alpha = \Esp[Y|G=0]\) and we assume that on the original scale:
#+BEGIN_EXPORT latex
\begin{align*}
\Var[Y|G=1]  = \Var[Y|G=0] = \sigma^2
\end{align*}
#+END_EXPORT
How should be parametrized the gaussian distribution of
\(\log(Y)|G=0\) and \(\log(Y)|G=1\) to satisfy
\((\alpha,\gamma,\sigma^2)\)? In other words we want to find
\(m_0,m_1,s_0,s_1\) such that:
#+BEGIN_EXPORT latex
\begin{align*}
Z_0 = \log(Y)|G=0 &\sim \Gaus\left(m_0,s^2_0\right) \\
Z_1 = \log(Y)|G=1 &\sim \Gaus\left(m_1,s^2_1\right) 
\end{align*}
#+END_EXPORT
We can use the delta method to identify these parameters since:
#+BEGIN_EXPORT latex
\begin{align*}
\alpha &= \Esp[\exp(Z_0)] = \exp(a_0)\left(1 + \frac{s^2_0}{2}\right) \\
\sigma^2 &= \Var[\exp(Z_0)] = \exp(2 a_0)\left(s^2_0 + \frac{7}{4}s^4_0\right) \\
\alpha (\gamma + 1) &= \Esp[\exp(Z_1)] = \exp(a_1)\left(1 + \frac{s^2_1}{2}\right)\\
\sigma^2 &= \Var[\exp(Z_1)] = \exp(2 a_1)\left(s^2_1 + \frac{7}{4}s^4_1\right)
\end{align*}
#+END_EXPORT
i.e.
#+BEGIN_EXPORT latex
\begin{align*}
\frac{\alpha^2}{\sigma^2} = \frac{\left(1+\frac{s_0^2}{2}\right)^2}{s^2_0 + \frac{7}{4}s^4_0}            \qquad &\rightarrow \text{gives } s_0 \\
a_0 = \frac{1}{2}\log\left(\frac{\sigma^2}{\left(s^2_0 + \frac{7}{4}s^4_0\right)}\right)                \qquad &\rightarrow \text{gives } a_0 \\
\frac{\alpha^2(\gamma+1)^2}{\sigma^2} = \frac{\left(1+\frac{s_1^2}{2}\right)^2}{s^2_1 + \frac{7}{4}s^4_1}\qquad &\rightarrow \text{gives } s_1 \\
a_1 = \frac{1}{2}\log\left(\frac{\sigma^2}{\left(s^2_1 + \frac{7}{4}s^4_1\right)}\right)                \qquad &\rightarrow \text{gives } a_1 
\end{align*}
#+END_EXPORT 
The first and third equation can be solved numerically.

\clearpage

\textbf{Illustration}: We consider two groups having a 10% difference
in their baseline value (\(\alpha=1.15\)) and a variance of \(\sigma^2
= 0.15\). What are the parameters of the corresponding normal
distribution on the log-scale and the standardized effect size?
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
alpha <- 1.15
sigma2 <- 0.15
gamma <- 0.1
#+END_SRC

#+RESULTS:

Solve the equations:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
s0 <- uniroot(function(x){alpha^2/sigma2 - (1+x/2)^2/(x+x^2*7/4)},
              interval = c(0,1))$root
a0 <- log(sigma2/(s0+s0^2*7/4))/2
s1 <- uniroot(function(x){alpha^2*(gamma+1)^2/sigma2 - (1+x/2)^2/(x+x^2*7/4)},
              interval = c(0,1))$root
a1 <- log(sigma2/(s1+s1^2*7/4))/2
c(a0 = a0, s0 = s0, a1 = a1, s1 = s1)
#+END_SRC

#+RESULTS:
:         a0         s0         a1         s1 
: 0.08802784 0.10608948 0.19175319 0.08851048

We can check that =uniroot= converged correctly:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
c(exp(a0)*(1+s0/2) - alpha, 
  exp(2*a0)*(s0+s0^2*7/4) - sigma2, 
  exp(a1)*(1+s1/2) - alpha*(1+gamma), 
  exp(2*a1)*(s1+s1^2*7/4) - sigma2)
#+END_SRC

#+RESULTS:
: [1] -5.563198e-05  0.000000e+00 -1.895835e-05  0.000000e+00

and the variables have the appropriate distribution:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Z0 <- exp(rnorm(1e4, mean=a0, sd = sqrt(s0)))
Z1 <- exp(rnorm(1e4, mean=a1, sd = sqrt(s1)))
c(alpha = mean(Z0), 
  gamma = (mean(Z1)-mean(Z0))/mean(Z0), 
  sigma2 = var(Z0), 
  sigma2 = var(Z1))
#+END_SRC

#+RESULTS:
:     alpha     gamma    sigma2    sigma2 
: 1.1435272 0.1090391 0.1473705 0.1507638

For a power calculation we would use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
pwr.t.test(d = (a1-a0)/sqrt(s0/2+s1/2), sig.level = 0.05, power = 0.8)
## dvmisc::power_2t_unequal(n = 143, d = a1-a0, sigsq1 = s0, sigsq2 = s1, alpha = 0.05)
#+END_SRC

#+RESULTS:
#+begin_example

     Two-sample t test power calculation 

              n = 142.9312
              d = 0.3325282
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

NOTE: n is number in *each* group
#+end_example

# Check:
# #+BEGIN_SRC R :exports both :results output :session *R* :cache no
# out <- sapply(1:10000,function(x){t.test(rnorm(143, mean = alpha, sd = sqrt(sigma2)),rnorm(143, mean = alpha*(1+gamma), sd = sqrt(sigma2)))$p.value})
# mean(out<=0.05)
# #+END_SRC

# #+RESULTS:
# : [1] 0.7084

# #+BEGIN_SRC R :exports both :results output :session *R* :cache no
# out <- sapply(1:10000,function(x){t.test(rnorm(143, mean = a0, sd = sqrt(s0)),rnorm(143, mean = a1, sd = sqrt(s1)))$p.value})
# mean(out<=0.05)
# #+END_SRC

# #+RESULTS:
# : [1] 0.8003

** Two independent groups - log-normal distribution

An alternative approach is to use a log-normal distribution. Random
variables with log normal distribution have their logarithm equal to a
specific value \(a\) and their standard deviation equal to a specific
value \(s\). So we want to get:
#+BEGIN_EXPORT latex
\begin{align*}
\alpha &= \exp(a_0 + \frac{1}{2} s_0^2) \\
\sigma^2 &= \exp(2*a_0 + s_0^2)*(\exp(s_0^2)-1) \\
\alpha (1+\gamma) &= \exp(a_1 + \frac{1}{2} s_1^2) \\
\sigma^2 &= \exp(2*a_1 + s_1^2)*(\exp(s_1^2)-1)
\end{align*}
#+END_EXPORT
So
#+BEGIN_EXPORT latex
\begin{align*}
s_0 &= \log\left(1+\frac{\sigma^2}{\alpha^2}\right)\\
a_0 &= \log(\alpha)-\frac{s_0^2}{2}\\
s_1 &= \log\left(1+\frac{\sigma^2}{\alpha*(1+\gamma)^2}\right)\\
a_1 &= \log(\alpha*(1+\gamma))-\frac{s_1^2}{2}
\end{align*}
#+END_EXPORT

\clearpage

\textbf{Illustration}: We still consider two groups having a 10%
difference in their baseline value (\(\alpha=1.15\)) and a variance of
\(\sigma^2 = 0.15\). What are the parameters of the corresponding
normal distribution on the log-scale and the standardized effect size?
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
alpha <- 1.15
sigma2 <- 0.15
gamma <- 0.1
#+END_SRC

#+RESULTS:

We identify the parameters of the log-normal distributions:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
s0 <- log(1+sigma2/alpha^2)
a0 <- log(alpha) - s0/2 
s1 <- log(1+sigma2/(alpha*(1+gamma))^2)
a1 <- log(alpha*(1+gamma)) - s1/2 
c(a0 = a0, s0 = s0, a1 = a1, s1 = s1)
#+END_SRC

#+RESULTS:
:         a0         s0         a1         s1 
: 0.08604307 0.10743775 0.19027207 0.08960011

We can check that the variables have the appropriate distribution:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
Z0 <- rlnorm(1e4, mean=a0, sd = sqrt(s0))
Z1 <- rlnorm(1e4, mean=a1, sd = sqrt(s1))
c(alpha = mean(Z0), 
  gamma = (mean(Z1)-mean(Z0))/mean(Z0), 
  sigma2 = var(Z0), 
  sigma2 = var(Z1))
#+END_SRC

#+RESULTS:
:     alpha     gamma    sigma2    sigma2 
: 1.1480725 0.1019535 0.1455856 0.1510286

For a power calculation we would use:
#+BEGIN_SRC R :exports both :results output :session *R* :cache no
pwr.t.test(d = (a1-a0)/sqrt(s0/2+s1/2), sig.level = 0.05, power = 0.8)
## dvmisc::power_2t_unequal(n = 143, d = a1-a0, sigsq1 = s0, sigsq2 = s1, alpha = 0.05)
#+END_SRC

#+RESULTS:
#+begin_example

     Two-sample t test power calculation 

              n = 143.3238
              d = 0.3320693
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

NOTE: n is number in *each* group
#+end_example

\clearpage

* Moments of the normal distribution

Denote \(X\) and \(Y\) two normally distributed variables, with mean
\(\mu_X\),\(\mu_Y\) and variance \(\sigma^2_X\),\(\sigma^2_Y\). Then:
- \(\Esp[X^3]=3 \mu_X \sigma^2_X + \mu_X^3\)
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
X <- rnorm(1e6, mean = 10.1, sd = 2.1)
mean(X^3)
mean((X-mean(X)+mean(X))^3)
mean(((X-mean(X))^2+2*mean(X)*(X-mean(X))+mean(X)^2)*(X-mean(X)+mean(X)))
mean((2*mean(X)*(X-mean(X)))*(X-mean(X))) + mean(((X-mean(X))^2+mean(X)^2)*(mean(X)))
2*mean(X)*var(X) + var(X)*mean(X)+mean(X)^3
3*mean(X)*var(X) + mean(X)^3
#+END_SRC

#+RESULTS:
: [1] 1164.078
: [1] 1164.078
: [1] 1164.078
: [1] 1164.051
: [1] 1164.051
: [1] 1164.051

- \(\Esp[X^4]=3\left(\sigma^2_X\right)^2 + 6 \sigma^2_X \mu_X^2 + \mu_X^4\)
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
X <- rnorm(1e6, mean = 10.1, sd = 2.1)
mean(X^4)
mean((X-mean(X)+mean(X))^4)
mean(((X-mean(X))^2+2*(X-mean(X))*mean(X)+mean(X)^2)^2)
mean((X-mean(X))^2 * ((X-mean(X))^2+2*(X-mean(X))*mean(X)+mean(X)^2) + 2*(X-mean(X))*mean(X) * ((X-mean(X))^2+2*(X-mean(X))*mean(X)+mean(X)^2) + mean(X)^2 * ((X-mean(X))^2+2*(X-mean(X))*mean(X)+mean(X)^2) )
cat("\n")
mean((X-mean(X))^4  + (X-mean(X))^2* mean(X)^2                       + 2*(X-mean(X))*mean(X) * 2*(X-mean(X))*mean(X) + mean(X)^2 * ((X-mean(X))^2+mean(X)^2) )
mean((X-mean(X))^4) + mean((X-mean(X))^2* mean(X)^2) + mean(2*(X-mean(X))*mean(X) * 2*(X-mean(X))*mean(X)) + mean(mean(X)^2 * (X-mean(X))^2) + mean(X)^4
mean((X-mean(X))^4) + var(X)* mean(X)^2              + 4*var(X)*mean(X)^2                                  + var(X)*mean(X)^2              + mean(X)^4
mean((X-mean(X))^4) + 6*var(X)*mean(X)^2 + mean(X)^4
3*var(X)^2 + 6*var(X)*mean(X)^2 + mean(X)^4
#+END_SRC

#+RESULTS:
#+begin_example
[1] 13154.17
[1] 13154.17
[1] 13154.17
[1] 13154.17

[1] 13153.69
[1] 13153.69
[1] 13153.7
[1] 13153.7
[1] 13153.58
#+end_example

- \(\Cov[X^2,X]=2 \mu_X \sigma^2_X\)
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
X <- rnorm(1e6, mean = 10.1, sd = 2.1)
cov(X^2,X)
mean((X^2-mean(X^2))*(X-mean(X)))
mean(X^3-X*mean(X^2)-X^2*mean(X)+mean(X^2)*mean(X))
mean(X^3)-mean(X)*mean(X^2)-mean(X^2)*mean(X)+mean(X^2)*mean(X)
mean(X^3) - mean(X)*mean(X^2) - mean(X^2)*mean(X) + mean(X^2)*mean(X)
mean(X^3) - mean(X)*(var(X)+mean(X)^2)
3*mean(X)*var(X) + mean(X)^3 - mean(X) * (var(X)+mean(X)^2)
2*mean(X)*var(X)
#+END_SRC

#+RESULTS:
: [1] 89.04187
: [1] 89.04178
: [1] 89.04178
: [1] 89.04178
: [1] 89.04178
: [1] 89.04173
: [1] 89.08142
: [1] 89.08142

- \(\Cov[X^2,Y]=2 \mu_X \rho \sigma_X \sigma_Y \)
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
m1 <- 10
m2 <- 10.2
s1 <- 10.5
s2 <- 10.5
rho <- 0.5
Sigma <- matrix(c(s1,rho*sqrt(s1)*sqrt(s2),rho*sqrt(s1)*sqrt(s2),s2),2,2)

set.seed(10)
XY <- mvtnorm::rmvnorm(1e4, mean=c(m1,m1), sigma =  Sigma)
X <- XY[,2]
Y <- XY[,1]

cov(X^2,Y)
mean((X^2-mean(X^2))*(Y-mean(Y)))
mean(X^2*Y)-mean(Y)*mean(X^2)-mean(X^2)*mean(Y)+mean(Y)*mean(X^2)
mean(X^2*Y)-mean(Y)*(var(X)+mean(X)^2)
mean(X^2*(mean(Y)+(X-mean(X))*cor(X,Y)*sd(Y)/sd(X)))-mean(Y)*(var(X)+mean(X)^2)
mean((X^2*mean(Y)+X^2*(X-mean(X))*cor(X,Y)*sd(Y)/sd(X)))-mean(Y)*(var(X)+mean(X)^2)
(mean(X^3)-mean(X^2)*mean(X))*cor(X,Y)*sd(Y)/sd(X)
(3*mean(X)*var(X) + mean(X)^3-mean(X^2)*mean(X))*cor(X,Y)*sd(Y)/sd(X)
(3*mean(X)*var(X) + mean(X)^3-var(X)*mean(X)-mean(X)^3)*cor(X,Y)*sd(Y)/sd(X)
2*mean(X)*sd(X)*cor(X,Y)*sd(Y)
#+END_SRC

#+RESULTS:
#+begin_example
[1] 105.1012
[1] 105.0907
[1] 105.0907
[1] 105.0802
[1] 104.8445
[1] 104.8445
[1] 104.855
[1] 104.7566
[1] 104.7513
[1] 104.7513
#+end_example

- \(\Esp[X^2*Y^2] = (\sigma^2_X+\mu_X^2)(\sigma^2_Y+\mu_Y^2) + 2 \rho^2 \sigma^2_X \sigma^2_Y + 4 \rho \sigma_Y \sigma_X \mu_X \mu_Y\)
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
m1 <- 0
m2 <- 0
s1 <- 10.5
s2 <- 10.5
rho <- 0.5
Sigma <- matrix(c(s1,rho*sqrt(s1)*sqrt(s2),rho*sqrt(s1)*sqrt(s2),s2),2,2)

set.seed(10)
XY <- mvtnorm::rmvnorm(1e4, mean=c(m1,m1), sigma =  Sigma)
X <- XY[,2]
Y <- XY[,1]

fit <- mean(Y)+(X-mean(X))*cor(X,Y)*sd(Y)/sd(X) 
epsilon <- Y-fit
mean(X^2*Y^2)
mean(X^2*(fit+epsilon)^2)
mean(X^2*fit^2) + mean(X^2*epsilon^2) + 2 * mean(X^2*epsilon*fit)
mean(X^2*fit^2) + mean(X^2)*mean(epsilon^2) + 2 * mean(X^2*fit)*mean(epsilon)
mean(X^2*fit^2) + mean(X^2)*mean(epsilon^2)
mean(X^2*fit^2) + mean(X^2)*(1-cor(X,Y)^2)*var(Y)
mean(X^2*(mean(Y)^2+(X-mean(X))^2*cor(X,Y)^2*var(Y)/var(X)+2*mean(Y)*(X-mean(X))*cor(X,Y)*sd(Y)/sd(X)) ) + mean(X^2)*(1-cor(X,Y)^2)*var(Y)
mean(Y)^2*mean(X^2) + mean(X^2*(X-mean(X))^2*cor(X,Y)^2*var(Y)/var(X)) + mean(X^2*2*mean(Y)*(X-mean(X))*cor(X,Y)*sd(Y)/sd(X)) + mean(X^2)*(1-cor(X,Y)^2)*var(Y)
mean(Y)^2*mean(X^2) + (mean(X^4)-2*mean(X^3)*mean(X)+mean(X^2)*mean(X)^2)*cor(X,Y)^2*var(Y)/var(X) + 2*mean(Y)*cor(X,Y)*sd(Y)/sd(X)*mean(X^3) - 2*mean(Y)*mean(X)*cor(X,Y)*sd(Y)/sd(X)*mean(X^2) + mean(X^2)*(1-cor(X,Y)^2)*var(Y)
mean(Y)^2*(var(X)+mean(X)^2) + (3*var(X)^2 + 6*var(X)*mean(X)^2 + mean(X)^4-2*(3*mean(X)*var(X) + mean(X)^3)*mean(X)+(var(X)+mean(X)^2)*mean(X)^2)*cor(X,Y)^2*var(Y)/var(X) + 2*mean(Y)*cor(X,Y)*sd(Y)/sd(X)*(3*mean(X)*var(X) + mean(X)^3) - 2*mean(Y)*mean(X)*cor(X,Y)*sd(Y)/sd(X)*(var(X)+mean(X)^2) + (var(X)+mean(X)^2)*(1-cor(X,Y)^2)*var(Y)
mean(Y)^2*(var(X)+mean(X)^2) + (3*var(X)^2 + var(X)*mean(X)^2)*cor(X,Y)^2*var(Y)/var(X) + 2*mean(Y)*cor(X,Y)*sd(Y)/sd(X)*(3*mean(X)*var(X) + mean(X)^3) - 2*mean(Y)*mean(X)*cor(X,Y)*sd(Y)/sd(X)*(var(X)+mean(X)^2) + (var(X)+mean(X)^2)*(1-cor(X,Y)^2)*var(Y)
(var(X)+mean(X)^2)*(var(Y)+mean(Y)^2) + 2*cor(X,Y)^2*var(Y)*var(X) + 4*cor(X,Y)*sd(Y)*sd(X)*mean(Y)*mean(X) 
#+END_SRC

#+RESULTS:
#+begin_example
[1] 163.934
[1] 163.934
[1] 163.934
[1] 165.3792
[1] 165.3792
[1] 165.3876
[1] 165.3876
[1] 165.3876
[1] 165.3876
[1] 167.0229
[1] 167.0229
[1] 167.0229
#+end_example

- \(\Cov[\left(X-\mu_X\right)^2,\left(Y-\mu_Y\right)^2] = 2 \rho^2 \sigma^2_X \sigma^2_Y\)
#+BEGIN_SRC R :exports none :results output :session *R* :cache no
m1 <- 0
m2 <- 0
s1 <- 10.5
s2 <- 10.5
rho <- 0.5
Sigma <- matrix(c(s1,rho*sqrt(s1)*sqrt(s2),rho*sqrt(s1)*sqrt(s2),s2),2,2)

set.seed(10)
XY <- mvtnorm::rmvnorm(1e4, mean=c(m1,m1), sigma =  Sigma)
X <- XY[,2]
Y <- XY[,1]

cov((X-mean(X))^2,(Y-mean(Y))^2)
cov(X^2-2*X*mean(X),Y^2-2*Y*mean(Y))
cov(X^2,Y^2) + cov(X^2,-2*Y*mean(Y)) + cov(-2*X*mean(X),Y^2) + cov(2*X*mean(X),2*Y*mean(Y))
mean(X^2*Y^2) - mean(X^2)*mean(Y^2) - 2*mean(Y)*cov(X^2,Y) - 2*mean(X)*cov(X,Y^2) + 4*mean(X)*mean(Y)*cov(X,Y)
cat("\n")
(var(X)+mean(X)^2)*(var(Y)+mean(Y)^2) + 2*cor(X,Y)^2*var(Y)*var(X) + 4*cor(X,Y)*sd(Y)*sd(X)*mean(Y)*mean(X)  -mean(X^2)*mean(Y^2) - 2*mean(Y)*cov(X^2,Y) - 2*mean(X)*cov(X,Y^2) + 4*mean(X)*mean(Y)*cov(X,Y)
2*cor(X,Y)^2*var(Y)*var(X) + 8*cor(X,Y)*sd(Y)*sd(X)*mean(Y)*mean(X) - 2*mean(Y)*2*mean(X)*sd(X)*cor(X,Y)*sd(Y) - 2*mean(X)*2*mean(Y)*sd(Y)*cor(X,Y)*sd(X)
2*cor(X,Y)^2*var(Y)*var(X)
#+END_SRC

#+RESULTS:
: [1] 1.895759
: [1] 1.895759
: [1] 1.895759
: [1] 1.895753
: 
: [1] 1.8893
: [1] 1.880605
: [1] 1.880605

* CONFIG :noexport:
# #+LaTeX_HEADER:\affil{Department of Biostatistics, University of Copenhagen, Copenhagen, Denmark}
#+LANGUAGE:  en
#+LaTeX_CLASS: org-article
#+LaTeX_CLASS_OPTIONS: [12pt]
#+OPTIONS:   title:t author:t toc:nil todo:nil
#+OPTIONS:   H:3 num:t 
#+OPTIONS:   TeX:t LaTeX:t

** Latex command
#+LATEX_HEADER: \RequirePackage{ifthen}
#+LATEX_HEADER: \RequirePackage{xifthen}
#+LATEX_HEADER: \RequirePackage{xargs}
#+LATEX_HEADER: \RequirePackage{xspace}

#+LATEX_HEADER: \newcommand\Rlogo{\textbf{\textsf{R}}\xspace} % 

** Notations

** Code
# Documentation at https://org-babel.readthedocs.io/en/latest/header-args/#results
# :tangle (yes/no/filename) extract source code with org-babel-tangle-file, see http://orgmode.org/manual/Extracting-source-code.html 
# :cache (yes/no)
# :eval (yes/no/never)
# :results (value/output/silent/graphics/raw/latex)
# :export (code/results/none/both)
#+PROPERTY: header-args :session *R* :tangle yes :cache no ## extra argument need to be on the same line as :session *R*

# Code display:
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}

# ## change font size input
# ## #+ATTR_LATEX: :options basicstyle=\ttfamily\scriptsize
# ## change font size output
# ## \RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\tiny,formatcom = {\color[rgb]{0.5,0,0}}}

** Display 
#+LATEX_HEADER: \RequirePackage{colortbl} % arrayrulecolor to mix colors
#+LATEX_HEADER: \RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
#+LaTeX_HEADER:\usepackage{authblk} % enable several affiliations (clash with beamer)
#+LaTeX_HEADER:\renewcommand{\baselinestretch}{1.1}
#+LATEX_HEADER:\geometry{top=1cm}

** Image
#+LATEX_HEADER: \RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
#+LATEX_HEADER: \RequirePackage{capt-of} % 
#+LATEX_HEADER: \RequirePackage{caption} % newlines in graphics


# ## warning symbol
#+LaTeX_HEADER: \usepackage{stackengine}
#+LaTeX_HEADER: \usepackage{scalerel}
#+LaTeX_HEADER: \newcommand\Warning[1][3ex]{%
#+LaTeX_HEADER:   \renewcommand\stacktype{L}%
#+LaTeX_HEADER:   \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny\bfseries !}}{#1}%
#+LaTeX_HEADER:   \xspace
#+LaTeX_HEADER: }

** Algorithm
#+LATEX_HEADER: \RequirePackage{amsmath}
#+LATEX_HEADER: \RequirePackage{algorithm}
#+LATEX_HEADER: \RequirePackage[noend]{algpseudocode}

** Math
#+LATEX_HEADER: \RequirePackage{dsfont}
#+LATEX_HEADER: \RequirePackage{amsmath,stmaryrd,graphicx}
#+LATEX_HEADER: \RequirePackage{prodint} % product integral symbol (\PRODI)

# ## lemma
# #+LaTeX_HEADER: \RequirePackage{amsthm}
# #+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
# #+LaTeX_HEADER: \newtheorem{lemma}[theorem]{Lemma}

*** Template for shortcut
#+LATEX_HEADER: \newcommand\defOperator[7]{%
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER:		\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
#+LATEX_HEADER:	}{
#+LATEX_HEADER:	\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand\defUOperator[5]{%
#+LATEX_HEADER: \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:		#5\left#3 #2 \right#4
#+LATEX_HEADER: }{
#+LATEX_HEADER:	\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
#+LATEX_HEADER:		\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommand{\defBoldVar}[2]{	
#+LATEX_HEADER:	\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
#+LATEX_HEADER: }

*** Shortcuts

**** Probability
#+LATEX_HEADER: \newcommandx\Cor[2][1=,2=]{\defOperator{#1}{#2}{C}{or}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
#+LATEX_HEADER: \newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}

#+LATEX_HEADER: \newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}

#+LATEX_HEADER: \newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}

**** Operators
#+LATEX_HEADER: \newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
#+LATEX_HEADER: \newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}

#+LATEX_HEADER: \newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
#+LATEX_HEADER: \newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
#+LATEX_HEADER: \newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
#+LATEX_HEADER: \newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
#+LATEX_HEADER: \newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}

#+LATEX_HEADER: \newcommandx\Hypothesis[2][1=,2=]{
#+LATEX_HEADER:         \ifthenelse{\isempty{#1}}{
#+LATEX_HEADER:         \mathcal{H}
#+LATEX_HEADER:         }{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#2}}{
#+LATEX_HEADER: 		\mathcal{H}_{#1}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\mathcal{H}^{(#2)}_{#1}
#+LATEX_HEADER:         }
#+LATEX_HEADER:         }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{#4 #1}{#4 #2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
#+LATEX_HEADER: }
#+LATEX_HEADER: }

#+LATEX_HEADER: \newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}

#+LATEX_HEADER: \newcommandx\ddpartial[3][1=,2=,3=]{
#+LATEX_HEADER: 	\ifthenelse{\isempty{#3}}{
#+LATEX_HEADER: 		\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
#+LATEX_HEADER: 	}{
#+LATEX_HEADER: 	\frac{\partial^2 #1}{\partial #2\partial #3}
#+LATEX_HEADER: }
#+LATEX_HEADER: } 

**** General math
#+LATEX_HEADER: \newcommand\Real{\mathbb{R}}
#+LATEX_HEADER: \newcommand\Rational{\mathbb{Q}}
#+LATEX_HEADER: \newcommand\Natural{\mathbb{N}}
#+LATEX_HEADER: \newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
#+LATEX_HEADER: \newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \newcommand\half{\frac{1}{2}}
#+LaTeX_HEADER: \newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
#+LaTeX_HEADER: \newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}
#+TITLE:
#+Author: Brice Ozenne



# @@latex:any arbitrary LaTeX code@@






