---
layout: post
title:  "Reporting a statistical analysis"
categories: jekyll update
usemathjax: true
---

Here are some personal thoughts about how to report the statistical
analysis in an applied paper (e.g. medical study).

## Structure of the 'statistical analysis' paragraph

In the method section, the statistical paragraph should relate research questions,
generally phrased in plain English at the end of the introduction, e.g.:
> investigate the effect of psilocibin intake on the change in depressive syndroms.

to a statistical procedure producing, in fine, numerical values
reported in the result section. The precise definition of the exposure
(e.g. dosage in mg), of the outcome (e.g. clinical scale used), and
the study population (e.g. patients with MDD between 18 and 50 years),
should be defined before in the method section and are thus not
mentioned further here. A comprehensive description of the statistical
procedure should explicit:
- **the parameter of interest**, typically the mean of a random variable, or difference or ratio of means.
- **the statistical model** relating the parameter of interest to observable data.
- **an estimation procedure** used to estimate the model parameters and the parameter of interest and **uncertainty quantification procedure** used to compute confidence intervals.
- **the testing procedure** used to compute p-values. This includes null hypothesis, type of test (Wald test, score test, likelihood ratio test), and possible adjustment for multiple comparisons.

For instance:
> - To assess the difference between the psilocibin and placebo group in average 1 month change in depression score,
> - we considered a linear regression modeling the change in depression scores as a function of psilocibin intake, age, sex, ... where the regression coefficient for psilocibin intake was our parameter of interest.
> - Model parameters were estimated by Maximum Likelihood Estimation (MLE) and uncertainty was derived from asymptotic likelihood theory.
> - a Wald test was used to test whether the difference in change was 0.

The definition of the parameter of interest is the most important
aspect, followed by the modeling procedure since it is where
assumptions are being made. In most studies, the estimation and
testing procedure is more a technicality (i.e. does not matter when
interpreting the results) and can thus be omitted. An exception is
when performing multiple tests where it is important for the
interpretation to report whether any adjustment was performed.

Unfortunately, statistical paragraphs are often more a list of statistical tools, e.g.:
> a logistic model was used to compare the groups in term of HAMD score

making it hard for the reader to decipher what the authors are
doing. The parameter of interest is not explicitly defined and
readers not versed in statistics will find it hard to
follow. Statisticians will also be in doubt about what the authors are
actually doing, since a statistical model can be used to target
different parameters (e.g. odds ratio or marginal difference in
outcome probability between treated and untreated).

## Software and reproducibility

It common to see in applied papers sentences like:
> All statistical analyses were performed using R (version 4.0.2) and the lava package (version 1.7.0)

While citing the software used is a good idea as it gives credit to
the authors, just mentioning the version of the software/packages is
often useless as it does not help with reproducibility. Instead one
should share the code and report the version of the operating system,
software, all related packages (output of ```r sessionInfo() ```), e.g.:
> The R software (R Core Team (2022) was use to implement the statistical analysis. The source code, the version of R and of related software package, can be found at https://github.com/bozenne/article-template/tree/main."

Note that you can run ```r citation() ``` and ```r citation("lava")
``` to obtain how to cite the R software and related packages (here the lava package).

## Common mistakes

- **reporting p-values in a descriptive table** (typically table 1),
    or, more generally, systematic comparison of patient
    characteristics like "we compared the clinical characteristics
    between treatment groups using t-test and chi-squared test". The
    output of such analyses is often difficult to interpret (e.g. due
    to multiple testing) and not relevant (e.g. whether the mean age
    is precisely the same between the two groups is beside the
    point). Current guidelines recommand also to avoid them
    (e.g. [STROBE](https://doi.org/10.1371/journal.pmed.0040297),
    section 14.a).

- **claiming that all model assumptions were checked and/or are met**. This is
    simply impossible as some of the model assumptions are untestable,
    e.g. missing data at random, no unobserved confounding,
    independent and identically distributed observations.

- **confusing R Studio and the R software**. R studio is a possible
    user interface to the R software whereas the R software is where
    the actual calculations happen. I usually do not find it relevant
    to cite R studio in a scientific article.

- **concluding about the null hypothesis (i.e. no effect) based on a
  large p-value**. P-values quantify evidence against the alternative
  hypothesis (existence of an effect), not evidence in favor of the
  null hypothesis. Consider doing equivalence testing or (a priori)
  define what is a neglectable effect and show that the confidence
  interval only contains neglectable effects.

- **concluding about treatment effect in a single arm
    study**. Consider a study including unmedicated individuals
    (baseline) receiving a single treatment and followed over
    time. One cannot disentangle the treatment effect from time
    effects (regression to the mean, seasonal effect, natural disease
    evolution,...).

- **using power calculation to interpret the results**. Statistical
    power is a great tool to plan a study but not to analyze the
    results ([Hoenig, 2001](http://www.jstor.org/stable/2685525)). 

- **unadjusted per protocol analysis**. In an experimental study,
    patients may deviate from the protocol and for instance stop
    taking the treatment. It can be tempting to exclude those patients
    from the analysis. However stopping treatment is seldom random and
    often related to the outcome (e.g. full recovery, no improvement,
    or rapid deterioration) so not accounting for time-dependent
    confounders will likely lead to substantial bias. In most cases, a
    proper analysis requires specialized statistical tools ([Hern√°n,
    2017](https://doi.org//10.1056/NEJMsm1605385)).

- **comparing p-values across studies**, e.g. whether one study found
    a significant effect and not the other. P-values are not meant to
    be reproducible across studies as, among other things, they depend
    on the sample size (which typically greatly differ between
    studies). Comparing estimates and confidence intervals is often
    more reasonable.

- **present multiple adjusted effect estimates from a single model in
    a single table**. This has been referred to as "the table 2
    fallacy" ([Westreich, 2013](https://doi.org/10.1093/aje/kws412)) as
    the interpretation of these estimates is far from obvious. Indeed
    the confounders, mediators, colliders of an exposure (say
    treatment) will typically differ from those of another exposure
    (say age) and 2 separate models are needed to obtain reasonnable
    estimates.

- **define groups based on post-baseline information**. This is
    sometimes refered to as conditioning on the future and has led to
    severely biased estimated in epidemiological studies ([Lange,
    2014](https://doi.org/10.1093/ije/dyu100), [Christensen,
    2022](https://doi.org/10.1038/s41746-021-00522-4)). Instead of
    assessing whether baseline characteristics differ between
    post-baseline groups, an analysis assessing whether the
    probability of belonging to a post-baseline group depends on
    baseline characteristics is often more easier to interpret.

## Backtransformation

Consider a study where two groups are compared (say active and
placebo) and the log-transformed outcome is analyzed using a linear
regression. On the log-scale, we obtain an intercept of 0.5 (placebo
group) and a treatment effect of 0.25. So a expected value in the
active group of 0.75.

Estimate and confidence intervals (but not p-values!) should typically
be back-transformed to ease interpretation. An estimate of 0.5 would
lead to back-transformed value of exp(0.5), approximatively 1.65. The
other group would get a value of 2.12. The backtransformed treatment
effect would be approximatively 1.28.

Assumming homoschedastic residuals between the two groups (on the log
scale), 1.28 is the ratio between the expected outcomes in the two
groups. Said otherwise, the mean outcome in the active group is 28%
higher compared to the placebo group. WARNING: 1.54 and 2.12 are NOT
the expected mean outcome value in each group (one would need to
multiply by the exponential of half the residuals variance). Assuming
a log-normal distribution, 1.54 and 2.12 can be interpreted as the
median value in each group and 1.28 as the ratio between the median
values.

## Partial residuals

Providing a graphical representation associated with the results of a
statistical analysis is a common request. Displaying the raw data with
the results of a statistical test (on the same graph or in the title
or caption) can be misleading when adjusting for covariates. Indeed
the raw data would correspond to an unadjusted analysis which may
greatly differ from the adjusted analysis. The possible mismatch
between the graphical representation and the estimated summary
statistic(s) may confuse the reader.

When working with a continuous outcome, one can instead display
partial residuals. Consider the following two-arm trial:
```r
> data(ckdW, package = "LMMstar")
> ggplot(ckdW, aes(x = allocation, y = pwv0)) + geom_boxplot()
```
<br>
![](https://bozenne.github.io/img/PRES-boxplot_raw.png)
<br>

where the following statistical model was used to test the group
difference (allocation variable):
```r
> e.lmm <- lmm(pwv0 ~ allocation + sex + age, data = ckdW)
> model.tables(e.lmm)
```
                  estimate         se      df      lower    upper      p.value
    (Intercept) -0.6701892 1.98683016 47.0094 -4.6671548 3.326776 7.373804e-01
    allocationB  0.3269616 0.82004599 47.0094 -1.3227494 1.976673 6.919113e-01
    sexmale      0.8474481 0.95876696 47.0094 -1.0813321 2.776228 3.812519e-01
    age          0.1682865 0.03246632 47.0094  0.1029731 0.233600 4.510612e-06


We obtain a mean group difference of 0.327 [-1.323;1.977] instead of
0.266 [-1.806;2.339] had we not have adjusted for sex and age. Partial
residuals can be extracted with the residuals method:
```r
> ckdW$pres <- residuals(e.lmm, type = "partial", var = "allocation")
> head(ckdW$pres)
```

    [1] -2.7098727 -2.3524574 -1.6281556 -0.1672843 -0.5647286 -2.4024574

One counter-intuitive feature of partial residuals is that there may
not follow the original scale (they are typically centered around 0
wherease the outcome value was strictly positive). But they do exactly
reflect the estimated difference:
```r
> tapply(ckdW$pres,ckdW$allocation,mean)
```
               A            B 
    1.064773e-16 3.269616e-01

So the graphical display based on the partial residuals will match the
result of the statistical test:
```r
> plot(e.lmm, type = "partial", var = "allocation")
```

<br>
![](https://bozenne.github.io/img/PRES-boxplot_partial0.png)
<br>

Assuming that the model holds, the display is to be understood as the
outcome we would have observed had all individuals have had the same
sex (here female) and same age (here age 0). Another counter-intuitive
feature is that continuous variables have by default reference level 0
which in the case of age is un-realistic. The first issue can be fixed
by keeping the intercept (argument var) and the second by specifying
the reference levels:
```r
> mytype <- "partial"
> attr(mytype, "reference") <- data.frame(sex = factor("female", levels(ckdW$sex)),
                                          age = 58)
> plot(e.lmm, type = mytype, var = c("(Intercept)","allocation"))
```
<br>
![](https://bozenne.github.io/img/PRES-boxplot_partial.png)
<br>

## Greek letters and notations

There is no intrinsic meanning to \(\beta\), \(\rho\), ... One should
define what each refers to in the statistical analysis
paragraph. Avoid to use the same letter to refer to two different
things. It can be a good idea to use \(p\) to refer to p-values
relative to a single test and \(p_{adj}\) to FWER adjusted
p-values. In the latter case, it should be made clear over which/how
many tests the FWER is evaluated.
